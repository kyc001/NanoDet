#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
è·Ÿè¸ªæƒé‡åŠ è½½è¿‡ç¨‹
æ‰¾å‡ºæƒé‡å·®å¼‚çš„å…·ä½“åŸå› 
"""

import os
import sys
import torch
import jittor as jt
import numpy as np

# æ·»åŠ è·¯å¾„
sys.path.append('/home/kyc/project/nanodet/nanodet-jittor')
from nanodet.model.arch.nanodet_plus import NanoDetPlus


def trace_weight_loading():
    """è·Ÿè¸ªæƒé‡åŠ è½½çš„è¯¦ç»†è¿‡ç¨‹"""
    print("ğŸ” è·Ÿè¸ªæƒé‡åŠ è½½è¯¦ç»†è¿‡ç¨‹")
    print("=" * 60)
    
    # åˆ›å»ºæ¨¡å‹é…ç½®
    backbone_cfg = {
        'name': 'ShuffleNetV2',
        'model_size': '1.0x',
        'out_stages': [2, 3, 4],
        'activation': 'LeakyReLU',
        'pretrain': True  # è¿™é‡Œä¼šåŠ è½½é¢„è®­ç»ƒæƒé‡
    }
    
    fpn_cfg = {
        'name': 'GhostPAN',
        'in_channels': [116, 232, 464],
        'out_channels': 96,
        'kernel_size': 5,
        'num_extra_level': 1,
        'use_depthwise': True,
        'activation': 'LeakyReLU'
    }
    
    head_cfg = {
        'name': 'NanoDetPlusHead',
        'num_classes': 20,
        'input_channel': 96,
        'feat_channels': 96,
        'stacked_convs': 2,
        'kernel_size': 5,
        'strides': [8, 16, 32, 64],
        'activation': 'LeakyReLU',
        'reg_max': 7,
        'norm_cfg': {'type': 'BN'},
        'loss': {
            'loss_qfl': {
                'name': 'QualityFocalLoss',
                'use_sigmoid': True,
                'beta': 2.0,
                'loss_weight': 1.0
            },
            'loss_dfl': {
                'name': 'DistributionFocalLoss',
                'loss_weight': 0.25
            },
            'loss_bbox': {
                'name': 'GIoULoss',
                'loss_weight': 2.0
            }
        }
    }
    
    aux_head_cfg = {
        'name': 'SimpleConvHead',
        'num_classes': 20,
        'input_channel': 192,
        'feat_channels': 192,
        'stacked_convs': 4,
        'strides': [8, 16, 32, 64],
        'activation': 'LeakyReLU',
        'reg_max': 7
    }
    
    print("1ï¸âƒ£ åˆ›å»ºæ¨¡å‹ï¼ˆåŒ…å«é¢„è®­ç»ƒæƒé‡åŠ è½½ï¼‰...")
    model = NanoDetPlus(backbone_cfg, fpn_cfg, aux_head_cfg, head_cfg)
    
    # è®°å½•é¢„è®­ç»ƒæƒé‡åŠ è½½åçš„çŠ¶æ€
    print("\n2ï¸âƒ£ è®°å½•é¢„è®­ç»ƒæƒé‡åŠ è½½åçš„çŠ¶æ€...")
    pretrained_weights = {}
    for name, param in model.named_parameters():
        if name.startswith('backbone.'):
            pretrained_weights[name] = param.numpy().copy()
    
    print(f"è®°å½•äº† {len(pretrained_weights)} ä¸ªbackboneå‚æ•°")
    
    # é€‰æ‹©å‡ ä¸ªå…³é”®å‚æ•°è¿›è¡Œè·Ÿè¸ª
    trace_params = [
        'backbone.conv1.0.weight',
        'backbone.conv1.1.weight',
        'backbone.conv1.1.bias',
        'backbone.conv1.1.running_mean',
        'backbone.conv1.1.running_var',
    ]
    
    print("\né¢„è®­ç»ƒæƒé‡åŠ è½½åçš„å…³é”®å‚æ•°:")
    for param_name in trace_params:
        if param_name in pretrained_weights:
            param_data = pretrained_weights[param_name]
            print(f"  {param_name}: èŒƒå›´[{param_data.min():.6f}, {param_data.max():.6f}], å‡å€¼{param_data.mean():.6f}")
    
    print("\n3ï¸âƒ£ åŠ è½½NanoDetè®­ç»ƒæƒé‡...")
    
    # åŠ è½½PyTorchæƒé‡
    checkpoint_path = "/home/kyc/project/nanodet/nanodet-pytorch/workspace/nanodet-plus-m_320_voc_bs64/model_best/model_best.ckpt"
    ckpt = torch.load(checkpoint_path, map_location='cpu')
    state_dict = ckpt.get('state_dict', ckpt)
    
    print(f"PyTorch checkpointåŒ…å« {len(state_dict)} ä¸ªå‚æ•°")
    
    # æ£€æŸ¥è¦è¦†ç›–çš„backboneå‚æ•°
    print("\nPyTorchæƒé‡ä¸­çš„å…³é”®backboneå‚æ•°:")
    for param_name in trace_params:
        pytorch_name = f"model.{param_name}"
        if pytorch_name in state_dict:
            pytorch_param = state_dict[pytorch_name]
            pytorch_data = pytorch_param.detach().numpy()
            print(f"  {pytorch_name}: èŒƒå›´[{pytorch_data.min():.6f}, {pytorch_data.max():.6f}], å‡å€¼{pytorch_data.mean():.6f}")
            
            # å¯¹æ¯”é¢„è®­ç»ƒæƒé‡
            if param_name in pretrained_weights:
                pretrained_data = pretrained_weights[param_name]
                diff = np.abs(pytorch_data - pretrained_data).max()
                print(f"    ä¸é¢„è®­ç»ƒæƒé‡å·®å¼‚: {diff:.6f}")
    
    # è·å–Jittoræ¨¡å‹çš„å‚æ•°å­—å…¸
    jittor_state_dict = {}
    for name, param in model.named_parameters():
        jittor_state_dict[name] = param
    
    print("\n4ï¸âƒ£ æ‰§è¡Œæƒé‡è¦†ç›–...")
    
    # è®°å½•è¦†ç›–è¿‡ç¨‹
    overwritten_count = 0
    for pytorch_name, pytorch_param in state_dict.items():
        jittor_name = pytorch_name
        if jittor_name.startswith("model."):
            jittor_name = jittor_name[6:]
        
        if "num_batches_tracked" in jittor_name or jittor_name.startswith("avg_"):
            continue
        
        if "distribution_project.project" in jittor_name:
            continue
        
        if jittor_name in jittor_state_dict:
            jittor_param = jittor_state_dict[jittor_name]
            
            # è®°å½•è¦†ç›–å‰çš„å€¼
            if jittor_name in trace_params:
                before_data = jittor_param.numpy().copy()
                print(f"\nè¦†ç›– {jittor_name}:")
                print(f"  è¦†ç›–å‰: èŒƒå›´[{before_data.min():.6f}, {before_data.max():.6f}], å‡å€¼{before_data.mean():.6f}")
            
            if list(pytorch_param.shape) == list(jittor_param.shape):
                jittor_param.assign(jt.array(pytorch_param.detach().numpy()))
                overwritten_count += 1
                
                # è®°å½•è¦†ç›–åçš„å€¼
                if jittor_name in trace_params:
                    after_data = jittor_param.numpy().copy()
                    pytorch_data = pytorch_param.detach().numpy()
                    print(f"  è¦†ç›–å: èŒƒå›´[{after_data.min():.6f}, {after_data.max():.6f}], å‡å€¼{after_data.mean():.6f}")
                    print(f"  PyTorch: èŒƒå›´[{pytorch_data.min():.6f}, {pytorch_data.max():.6f}], å‡å€¼{pytorch_data.mean():.6f}")
                    
                    # éªŒè¯è¦†ç›–æ˜¯å¦æˆåŠŸ
                    diff = np.abs(after_data - pytorch_data).max()
                    print(f"  è¦†ç›–å·®å¼‚: {diff:.10f}")
                    
                    if diff < 1e-6:
                        print(f"  âœ… è¦†ç›–æˆåŠŸ")
                    else:
                        print(f"  âŒ è¦†ç›–å¤±è´¥")
                        
            elif "scale" in jittor_name and len(pytorch_param.shape) == 0 and list(jittor_param.shape) == [1]:
                jittor_param.assign(jt.array([pytorch_param.detach().numpy()]))
                overwritten_count += 1
    
    print(f"\nâœ“ è¦†ç›–äº† {overwritten_count} ä¸ªå‚æ•°")
    
    print("\n5ï¸âƒ£ éªŒè¯æœ€ç»ˆæƒé‡çŠ¶æ€...")
    
    # éªŒè¯æœ€ç»ˆçŠ¶æ€
    for param_name in trace_params:
        if param_name in jittor_state_dict:
            final_data = jittor_state_dict[param_name].numpy()
            print(f"\næœ€ç»ˆ {param_name}:")
            print(f"  æœ€ç»ˆå€¼: èŒƒå›´[{final_data.min():.6f}, {final_data.max():.6f}], å‡å€¼{final_data.mean():.6f}")
            
            # ä¸PyTorchæƒé‡å¯¹æ¯”
            pytorch_name = f"model.{param_name}"
            if pytorch_name in state_dict:
                pytorch_data = state_dict[pytorch_name].detach().numpy()
                diff = np.abs(final_data - pytorch_data).max()
                print(f"  ä¸PyTorchå·®å¼‚: {diff:.10f}")
                
                if diff < 1e-6:
                    print(f"  âœ… ä¸PyTorchä¸€è‡´")
                else:
                    print(f"  âŒ ä¸PyTorchä¸ä¸€è‡´")
                    
                    # è¯¦ç»†åˆ†æä¸ä¸€è‡´çš„åŸå› 
                    print(f"    æœ€ç»ˆç»Ÿè®¡: å‡å€¼={final_data.mean():.6f}, æ ‡å‡†å·®={final_data.std():.6f}")
                    print(f"    PyTorchç»Ÿè®¡: å‡å€¼={pytorch_data.mean():.6f}, æ ‡å‡†å·®={pytorch_data.std():.6f}")
    
    print(f"\nâœ… æƒé‡åŠ è½½è·Ÿè¸ªå®Œæˆ")
    return model


def compare_with_fresh_model():
    """ä¸æ–°åˆ›å»ºçš„æ¨¡å‹å¯¹æ¯”"""
    print("\nğŸ” ä¸æ–°åˆ›å»ºçš„æ¨¡å‹å¯¹æ¯”")
    print("=" * 60)
    
    # åˆ›å»ºä¸€ä¸ªæ–°çš„æ¨¡å‹ï¼ˆä¸åŠ è½½ä»»ä½•æƒé‡ï¼‰
    backbone_cfg = {
        'name': 'ShuffleNetV2',
        'model_size': '1.0x',
        'out_stages': [2, 3, 4],
        'activation': 'LeakyReLU',
        'pretrain': False  # ä¸åŠ è½½é¢„è®­ç»ƒæƒé‡
    }
    
    fpn_cfg = {
        'name': 'GhostPAN',
        'in_channels': [116, 232, 464],
        'out_channels': 96,
        'kernel_size': 5,
        'num_extra_level': 1,
        'use_depthwise': True,
        'activation': 'LeakyReLU'
    }
    
    head_cfg = {
        'name': 'NanoDetPlusHead',
        'num_classes': 20,
        'input_channel': 96,
        'feat_channels': 96,
        'stacked_convs': 2,
        'kernel_size': 5,
        'strides': [8, 16, 32, 64],
        'activation': 'LeakyReLU',
        'reg_max': 7,
        'norm_cfg': {'type': 'BN'},
        'loss': {
            'loss_qfl': {
                'name': 'QualityFocalLoss',
                'use_sigmoid': True,
                'beta': 2.0,
                'loss_weight': 1.0
            },
            'loss_dfl': {
                'name': 'DistributionFocalLoss',
                'loss_weight': 0.25
            },
            'loss_bbox': {
                'name': 'GIoULoss',
                'loss_weight': 2.0
            }
        }
    }
    
    aux_head_cfg = {
        'name': 'SimpleConvHead',
        'num_classes': 20,
        'input_channel': 192,
        'feat_channels': 192,
        'stacked_convs': 4,
        'strides': [8, 16, 32, 64],
        'activation': 'LeakyReLU',
        'reg_max': 7
    }
    
    print("åˆ›å»ºæ–°æ¨¡å‹ï¼ˆæ— é¢„è®­ç»ƒæƒé‡ï¼‰...")
    fresh_model = NanoDetPlus(backbone_cfg, fpn_cfg, aux_head_cfg, head_cfg)
    
    # æ‰‹åŠ¨åŠ è½½PyTorchæƒé‡
    print("æ‰‹åŠ¨åŠ è½½PyTorchæƒé‡...")
    checkpoint_path = "/home/kyc/project/nanodet/nanodet-pytorch/workspace/nanodet-plus-m_320_voc_bs64/model_best/model_best.ckpt"
    ckpt = torch.load(checkpoint_path, map_location='cpu')
    state_dict = ckpt.get('state_dict', ckpt)
    
    # è·å–æ–°æ¨¡å‹çš„å‚æ•°å­—å…¸
    fresh_state_dict = {}
    for name, param in fresh_model.named_parameters():
        fresh_state_dict[name] = param
    
    # æ‰‹åŠ¨åŠ è½½æƒé‡
    loaded_count = 0
    for pytorch_name, pytorch_param in state_dict.items():
        jittor_name = pytorch_name
        if jittor_name.startswith("model."):
            jittor_name = jittor_name[6:]
        
        if "num_batches_tracked" in jittor_name or jittor_name.startswith("avg_"):
            continue
        
        if "distribution_project.project" in jittor_name:
            continue
        
        if jittor_name in fresh_state_dict:
            jittor_param = fresh_state_dict[jittor_name]
            
            if list(pytorch_param.shape) == list(jittor_param.shape):
                jittor_param.assign(jt.array(pytorch_param.detach().numpy()))
                loaded_count += 1
            elif "scale" in jittor_name and len(pytorch_param.shape) == 0 and list(jittor_param.shape) == [1]:
                jittor_param.assign(jt.array([pytorch_param.detach().numpy()]))
                loaded_count += 1
    
    print(f"âœ“ æ‰‹åŠ¨åŠ è½½äº† {loaded_count} ä¸ªå‚æ•°")
    
    # å¯¹æ¯”ä¸¤ä¸ªæ¨¡å‹çš„æƒé‡
    print("\nå¯¹æ¯”ä¸¤ä¸ªæ¨¡å‹çš„æƒé‡:")
    
    trace_params = [
        'backbone.conv1.0.weight',
        'backbone.conv1.1.weight',
        'backbone.conv1.1.bias',
        'backbone.conv1.1.running_mean',
        'backbone.conv1.1.running_var',
    ]
    
    # å…ˆè·Ÿè¸ªåŠ è½½æƒé‡çš„æ¨¡å‹
    traced_model = trace_weight_loading()
    
    for param_name in trace_params:
        if param_name in fresh_state_dict and param_name in traced_model.state_dict():
            fresh_data = fresh_state_dict[param_name].numpy()
            traced_data = traced_model.state_dict()[param_name].numpy()
            
            diff = np.abs(fresh_data - traced_data).max()
            
            print(f"\n{param_name}:")
            print(f"  æ–°æ¨¡å‹: èŒƒå›´[{fresh_data.min():.6f}, {fresh_data.max():.6f}]")
            print(f"  è·Ÿè¸ªæ¨¡å‹: èŒƒå›´[{traced_data.min():.6f}, {traced_data.max():.6f}]")
            print(f"  å·®å¼‚: {diff:.10f}")
            
            if diff < 1e-6:
                print(f"  âœ… ä¸¤ä¸ªæ¨¡å‹ä¸€è‡´")
            else:
                print(f"  âŒ ä¸¤ä¸ªæ¨¡å‹ä¸ä¸€è‡´")


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ å¼€å§‹è·Ÿè¸ªæƒé‡åŠ è½½è¿‡ç¨‹")
    
    # è·Ÿè¸ªæƒé‡åŠ è½½
    traced_model = trace_weight_loading()
    
    # ä¸æ–°æ¨¡å‹å¯¹æ¯”
    # compare_with_fresh_model()
    
    print(f"\nâœ… è·Ÿè¸ªå®Œæˆ")


if __name__ == '__main__':
    main()
