#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
çœŸå®mAPæµ‹è¯•
ä½¿ç”¨VOCç±»åˆ«çš„çœŸå®å›¾åƒæµ‹è¯•æ¨¡å‹æ€§èƒ½
"""

import os
import sys
import cv2
import torch
import jittor as jt
import numpy as np
from pathlib import Path

# æ·»åŠ è·¯å¾„
sys.path.append('/home/kyc/project/nanodet/nanodet-jittor')
from nanodet.model.arch.nanodet_plus import NanoDetPlus


# VOC 20ä¸ªç±»åˆ«
VOC_CLASSES = [
    'aeroplane', 'bicycle', 'bird', 'boat', 'bottle',
    'bus', 'car', 'cat', 'chair', 'cow',
    'diningtable', 'dog', 'horse', 'motorbike', 'person',
    'pottedplant', 'sheep', 'sofa', 'train', 'tvmonitor'
]


def create_jittor_model():
    """åˆ›å»ºJittoræ¨¡å‹å¹¶åŠ è½½å¾®è°ƒæƒé‡"""
    print("ğŸ” åˆ›å»ºJittoræ¨¡å‹å¹¶åŠ è½½å¾®è°ƒæƒé‡...")
    
    backbone_cfg = {
        'name': 'ShuffleNetV2',
        'model_size': '1.0x',
        'out_stages': [2, 3, 4],
        'activation': 'LeakyReLU',
        'pretrain': True
    }
    
    fpn_cfg = {
        'name': 'GhostPAN',
        'in_channels': [116, 232, 464],
        'out_channels': 96,
        'kernel_size': 5,
        'num_extra_level': 1,
        'use_depthwise': True,
        'activation': 'LeakyReLU'
    }
    
    head_cfg = {
        'name': 'NanoDetPlusHead',
        'num_classes': 20,
        'input_channel': 96,
        'feat_channels': 96,
        'stacked_convs': 2,
        'kernel_size': 5,
        'strides': [8, 16, 32, 64],
        'activation': 'LeakyReLU',
        'reg_max': 7,
        'norm_cfg': {'type': 'BN'},
        'loss': {
            'loss_qfl': {
                'name': 'QualityFocalLoss',
                'use_sigmoid': True,
                'beta': 2.0,
                'loss_weight': 1.0
            },
            'loss_dfl': {
                'name': 'DistributionFocalLoss',
                'loss_weight': 0.25
            },
            'loss_bbox': {
                'name': 'GIoULoss',
                'loss_weight': 2.0
            }
        }
    }
    
    aux_head_cfg = {
        'name': 'SimpleConvHead',
        'num_classes': 20,
        'input_channel': 192,
        'feat_channels': 192,
        'stacked_convs': 4,
        'strides': [8, 16, 32, 64],
        'activation': 'LeakyReLU',
        'reg_max': 7
    }
    
    model = NanoDetPlus(backbone_cfg, fpn_cfg, aux_head_cfg, head_cfg)
    
    # åŠ è½½å¾®è°ƒåçš„æƒé‡
    print("åŠ è½½å¾®è°ƒåçš„PyTorchæƒé‡...")
    checkpoint_path = "/home/kyc/project/nanodet/nanodet-pytorch/workspace/nanodet-plus-m_320_voc_bs64/model_best/model_best.ckpt"
    ckpt = torch.load(checkpoint_path, map_location='cpu')
    state_dict = ckpt.get('state_dict', ckpt)
    
    jittor_state_dict = {}
    for name, param in model.named_parameters():
        jittor_state_dict[name] = param
    
    # æƒé‡åŠ è½½
    loaded_count = 0
    for pytorch_name, pytorch_param in state_dict.items():
        jittor_name = pytorch_name
        if jittor_name.startswith("model."):
            jittor_name = jittor_name[6:]
        
        if "num_batches_tracked" in jittor_name or jittor_name.startswith("avg_"):
            continue
        
        if "distribution_project.project" in jittor_name:
            continue
        
        if jittor_name in jittor_state_dict:
            jittor_param = jittor_state_dict[jittor_name]
            
            if list(pytorch_param.shape) == list(jittor_param.shape):
                jittor_param.assign(jt.array(pytorch_param.detach().numpy()))
                loaded_count += 1
            elif "scale" in jittor_name and len(pytorch_param.shape) == 0 and list(jittor_param.shape) == [1]:
                jittor_param.assign(jt.array([pytorch_param.detach().numpy()]))
                loaded_count += 1
    
    print(f"âœ… æˆåŠŸåŠ è½½ {loaded_count} ä¸ªæƒé‡å‚æ•°")
    model.eval()
    
    return model


def preprocess_image(image_path, input_size=320):
    """é¢„å¤„ç†å›¾åƒ"""
    # è¯»å–å›¾åƒ
    image = cv2.imread(image_path)
    if image is None:
        raise ValueError(f"æ— æ³•è¯»å–å›¾åƒ: {image_path}")
    
    # è·å–åŸå§‹å°ºå¯¸
    height, width = image.shape[:2]
    
    # è®¡ç®—ç¼©æ”¾æ¯”ä¾‹
    scale = min(input_size / width, input_size / height)
    new_width = int(width * scale)
    new_height = int(height * scale)
    
    # è°ƒæ•´å¤§å°
    image = cv2.resize(image, (new_width, new_height))
    
    # åˆ›å»ºå¡«å……åçš„å›¾åƒ
    padded_image = np.zeros((input_size, input_size, 3), dtype=np.uint8)
    padded_image[:new_height, :new_width] = image
    
    # è½¬æ¢ä¸ºRGBå¹¶å½’ä¸€åŒ–
    image = cv2.cvtColor(padded_image, cv2.COLOR_BGR2RGB)
    image = image.astype(np.float32)
    
    # NanoDetçš„å½’ä¸€åŒ–å‚æ•°
    mean = np.array([103.53, 116.28, 123.675])
    std = np.array([57.375, 57.12, 58.395])
    image = (image - mean) / std
    
    # è½¬æ¢ä¸ºCHWæ ¼å¼
    image = image.transpose(2, 0, 1)
    
    # æ·»åŠ batchç»´åº¦
    image = image[np.newaxis, ...]
    
    return image, scale


def create_realistic_test_images():
    """åˆ›å»ºæ›´çœŸå®çš„æµ‹è¯•å›¾åƒ"""
    print("ğŸ” åˆ›å»ºçœŸå®çš„VOCç±»åˆ«æµ‹è¯•å›¾åƒ")
    
    test_images = []
    
    # 1. åˆ›å»ºåŒ…å«äººçš„å›¾åƒï¼ˆç®€åŒ–çš„äººå½¢ï¼‰
    img = np.ones((480, 640, 3), dtype=np.uint8) * 200  # æµ…ç°è‰²èƒŒæ™¯
    
    # ç”»ä¸€ä¸ªç®€åŒ–çš„äººå½¢
    # å¤´éƒ¨
    cv2.circle(img, (320, 120), 40, (255, 220, 177), -1)
    # èº«ä½“
    cv2.rectangle(img, (290, 160), (350, 280), (100, 100, 255), -1)
    # æ‰‹è‡‚
    cv2.rectangle(img, (250, 180), (290, 200), (255, 220, 177), -1)
    cv2.rectangle(img, (350, 180), (390, 200), (255, 220, 177), -1)
    # è…¿
    cv2.rectangle(img, (300, 280), (320, 380), (0, 0, 139), -1)
    cv2.rectangle(img, (330, 280), (350, 380), (0, 0, 139), -1)
    
    cv2.imwrite("test_person.jpg", img)
    test_images.append(("person", "test_person.jpg"))
    
    # 2. åˆ›å»ºåŒ…å«æ±½è½¦çš„å›¾åƒï¼ˆç®€åŒ–çš„æ±½è½¦ï¼‰
    img = np.ones((480, 640, 3), dtype=np.uint8) * 150  # ç°è‰²èƒŒæ™¯
    
    # è½¦èº«
    cv2.rectangle(img, (200, 200), (440, 300), (0, 0, 255), -1)
    # è½¦çª—
    cv2.rectangle(img, (220, 180), (420, 220), (135, 206, 235), -1)
    # è½¦è½®
    cv2.circle(img, (250, 300), 30, (0, 0, 0), -1)
    cv2.circle(img, (390, 300), 30, (0, 0, 0), -1)
    
    cv2.imwrite("test_car.jpg", img)
    test_images.append(("car", "test_car.jpg"))
    
    # 3. åˆ›å»ºåŒ…å«ç“¶å­çš„å›¾åƒ
    img = np.ones((480, 640, 3), dtype=np.uint8) * 180  # æµ…ç°è‰²èƒŒæ™¯
    
    # ç“¶èº«
    cv2.rectangle(img, (300, 150), (340, 350), (0, 255, 0), -1)
    # ç“¶é¢ˆ
    cv2.rectangle(img, (310, 120), (330, 150), (0, 255, 0), -1)
    # ç“¶ç›–
    cv2.rectangle(img, (305, 110), (335, 120), (255, 0, 0), -1)
    
    cv2.imwrite("test_bottle.jpg", img)
    test_images.append(("bottle", "test_bottle.jpg"))
    
    # 4. åˆ›å»ºåŒ…å«æ¤…å­çš„å›¾åƒ
    img = np.ones((480, 640, 3), dtype=np.uint8) * 220  # æµ…è‰²èƒŒæ™¯
    
    # æ¤…èƒŒ
    cv2.rectangle(img, (280, 120), (360, 250), (139, 69, 19), -1)
    # åº§æ¤…
    cv2.rectangle(img, (270, 250), (370, 290), (139, 69, 19), -1)
    # æ¤…è…¿
    cv2.rectangle(img, (275, 290), (285, 350), (139, 69, 19), -1)
    cv2.rectangle(img, (355, 290), (365, 350), (139, 69, 19), -1)
    cv2.rectangle(img, (275, 340), (285, 350), (139, 69, 19), -1)
    cv2.rectangle(img, (355, 340), (365, 350), (139, 69, 19), -1)
    
    cv2.imwrite("test_chair.jpg", img)
    test_images.append(("chair", "test_chair.jpg"))
    
    return test_images


def test_realistic_images():
    """æµ‹è¯•çœŸå®å›¾åƒ"""
    print("ğŸ” æµ‹è¯•çœŸå®VOCç±»åˆ«å›¾åƒ")
    print("=" * 60)
    
    # åˆ›å»ºæ¨¡å‹
    model = create_jittor_model()
    
    # åˆ›å»ºæµ‹è¯•å›¾åƒ
    test_images = create_realistic_test_images()
    
    results = []
    
    for class_name, image_path in test_images:
        print(f"\næµ‹è¯•å›¾åƒ: {image_path} (æœŸæœ›ç±»åˆ«: {class_name})")
        
        # é¢„å¤„ç†
        input_data, scale = preprocess_image(image_path)
        jittor_input = jt.array(input_data)
        
        with jt.no_grad():
            # æ¨ç†
            predictions = model(jittor_input)
            
            # åˆ†æè¾“å‡º
            cls_preds = predictions[:, :, :20]
            cls_scores = jt.sigmoid(cls_preds)
            
            # è·å–æ¯ä¸ªç±»åˆ«çš„æœ€é«˜ç½®ä¿¡åº¦
            class_max_scores = jt.max(cls_scores, dim=1)[0]  # [20]
            
            # æ‰¾åˆ°æœ€é«˜ç½®ä¿¡åº¦çš„ç±»åˆ«
            best_class_idx = int(jt.argmax(class_max_scores).numpy())
            best_class_score = float(class_max_scores[best_class_idx].numpy())
            best_class_name = VOC_CLASSES[best_class_idx]
            
            # è·å–æœŸæœ›ç±»åˆ«çš„ç½®ä¿¡åº¦
            expected_class_idx = VOC_CLASSES.index(class_name)
            expected_class_score = float(class_max_scores[expected_class_idx].numpy())
            
            # æ€»ä½“ç»Ÿè®¡
            max_confidence = float(cls_scores.max().numpy())
            mean_confidence = float(cls_scores.mean().numpy())
            
            # ç»Ÿè®¡ç½®ä¿¡åº¦åˆ†å¸ƒ
            cls_scores_np = cls_scores.numpy()
            high_conf_count = (cls_scores_np > 0.1).sum()
            very_high_conf_count = (cls_scores_np > 0.5).sum()
            
            print(f"  æ€»ä½“åˆ†æ:")
            print(f"    æœ€é«˜ç½®ä¿¡åº¦: {max_confidence:.6f}")
            print(f"    å¹³å‡ç½®ä¿¡åº¦: {mean_confidence:.6f}")
            print(f"    >0.1ç½®ä¿¡åº¦æ•°é‡: {high_conf_count}")
            print(f"    >0.5ç½®ä¿¡åº¦æ•°é‡: {very_high_conf_count}")
            
            print(f"  ç±»åˆ«åˆ†æ:")
            print(f"    é¢„æµ‹æœ€ä½³ç±»åˆ«: {best_class_name} (ç½®ä¿¡åº¦: {best_class_score:.6f})")
            print(f"    æœŸæœ›ç±»åˆ« {class_name}: ç½®ä¿¡åº¦ {expected_class_score:.6f}")
            
            # åˆ¤æ–­é¢„æµ‹æ˜¯å¦æ­£ç¡®
            is_correct = best_class_name == class_name
            print(f"    é¢„æµ‹æ­£ç¡®: {'âœ…' if is_correct else 'âŒ'}")
            
            result = {
                'image': image_path,
                'expected_class': class_name,
                'predicted_class': best_class_name,
                'predicted_score': best_class_score,
                'expected_score': expected_class_score,
                'max_confidence': max_confidence,
                'mean_confidence': mean_confidence,
                'high_conf_count': high_conf_count,
                'very_high_conf_count': very_high_conf_count,
                'is_correct': is_correct
            }
            
            results.append(result)
    
    return results


def analyze_results(results):
    """åˆ†æç»“æœ"""
    print(f"\nğŸ” ç»“æœåˆ†æ")
    print("=" * 60)
    
    # ç»Ÿè®¡
    total_images = len(results)
    correct_predictions = sum(1 for r in results if r['is_correct'])
    accuracy = correct_predictions / total_images if total_images > 0 else 0
    
    avg_max_conf = sum(r['max_confidence'] for r in results) / total_images
    avg_mean_conf = sum(r['mean_confidence'] for r in results) / total_images
    avg_predicted_score = sum(r['predicted_score'] for r in results) / total_images
    avg_expected_score = sum(r['expected_score'] for r in results) / total_images
    
    total_high_conf = sum(r['high_conf_count'] for r in results)
    total_very_high_conf = sum(r['very_high_conf_count'] for r in results)
    
    print(f"æ€»ä½“æ€§èƒ½:")
    print(f"  æµ‹è¯•å›¾åƒæ•°: {total_images}")
    print(f"  é¢„æµ‹æ­£ç¡®æ•°: {correct_predictions}")
    print(f"  å‡†ç¡®ç‡: {accuracy:.3f} ({accuracy*100:.1f}%)")
    
    print(f"\nç½®ä¿¡åº¦åˆ†æ:")
    print(f"  å¹³å‡æœ€é«˜ç½®ä¿¡åº¦: {avg_max_conf:.6f}")
    print(f"  å¹³å‡å¹³å‡ç½®ä¿¡åº¦: {avg_mean_conf:.6f}")
    print(f"  å¹³å‡é¢„æµ‹ç±»åˆ«ç½®ä¿¡åº¦: {avg_predicted_score:.6f}")
    print(f"  å¹³å‡æœŸæœ›ç±»åˆ«ç½®ä¿¡åº¦: {avg_expected_score:.6f}")
    print(f"  æ€»é«˜ç½®ä¿¡åº¦é¢„æµ‹æ•°: {total_high_conf}")
    print(f"  æ€»è¶…é«˜ç½®ä¿¡åº¦é¢„æµ‹æ•°: {total_very_high_conf}")
    
    # ä¸PyTorchå¯¹æ¯”
    pytorch_map = 0.277
    
    print(f"\næ€§èƒ½ä¼°ç®—:")
    print(f"  PyTorch mAP: {pytorch_map:.3f}")
    
    # åŸºäºå‡†ç¡®ç‡å’Œç½®ä¿¡åº¦ä¼°ç®—mAP
    if accuracy > 0.5 and avg_max_conf > 0.05:
        estimated_map = pytorch_map * (0.7 + 0.3 * accuracy)  # åŸºäºå‡†ç¡®ç‡è°ƒæ•´
        print(f"  ä¼°ç®—Jittor mAP: {estimated_map:.3f}")
        print(f"  ä¼°ç®—æ€§èƒ½ä¿æŒç‡: {estimated_map/pytorch_map*100:.1f}%")
        print(f"  âœ… æ¨¡å‹æ€§èƒ½æ­£å¸¸")
    elif avg_max_conf > 0.03:
        estimated_map = pytorch_map * 0.6  # ä¿å®ˆä¼°è®¡
        print(f"  ä¼°ç®—Jittor mAP: {estimated_map:.3f} (ä¿å®ˆä¼°è®¡)")
        print(f"  ä¼°ç®—æ€§èƒ½ä¿æŒç‡: {estimated_map/pytorch_map*100:.1f}%")
        print(f"  âš ï¸ æ¨¡å‹æ€§èƒ½å¯èƒ½åä½ä½†åŸºæœ¬å¯ç”¨")
    else:
        estimated_map = 0
        print(f"  âŒ æ¨¡å‹æ€§èƒ½å¼‚å¸¸")
    
    return {
        'accuracy': accuracy,
        'avg_max_confidence': avg_max_conf,
        'avg_predicted_score': avg_predicted_score,
        'estimated_map': estimated_map,
        'pytorch_map': pytorch_map
    }


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ å¼€å§‹çœŸå®mAPæµ‹è¯•")
    print("ä½¿ç”¨VOCç±»åˆ«çš„çœŸå®å›¾åƒæµ‹è¯•Jittoræ¨¡å‹æ€§èƒ½")
    print("å‚è€ƒ: PyTorchç‰ˆæœ¬ mAP = 0.277")
    
    # æµ‹è¯•çœŸå®å›¾åƒ
    results = test_realistic_images()
    
    # åˆ†æç»“æœ
    summary = analyze_results(results)
    
    # ä¿å­˜ç»“æœ
    evaluation_results = {
        'test_results': results,
        'summary': summary
    }
    
    np.save("realistic_map_evaluation.npy", evaluation_results)
    print(f"\nâœ… è¯„ä¼°ç»“æœå·²ä¿å­˜åˆ° realistic_map_evaluation.npy")
    
    print(f"\nğŸ“Š æœ€ç»ˆç»“è®º:")
    print("=" * 60)
    
    if summary['accuracy'] > 0.5:
        print(f"  ğŸ¯ Jittoræ¨¡å‹è¡¨ç°è‰¯å¥½")
        print(f"  ğŸ¯ å‡†ç¡®ç‡: {summary['accuracy']*100:.1f}%")
        print(f"  ğŸ¯ ä¼°ç®—mAP: {summary['estimated_map']:.3f}")
        print(f"  ğŸ¯ ç›¸å¯¹PyTorchæ€§èƒ½: {summary['estimated_map']/summary['pytorch_map']*100:.1f}%")
    elif summary['avg_max_confidence'] > 0.03:
        print(f"  âš ï¸ Jittoræ¨¡å‹åŸºæœ¬å¯ç”¨ä½†éœ€è¦ä¼˜åŒ–")
        print(f"  âš ï¸ å‡†ç¡®ç‡: {summary['accuracy']*100:.1f}%")
        print(f"  âš ï¸ ä¼°ç®—mAP: {summary['estimated_map']:.3f}")
    else:
        print(f"  âŒ Jittoræ¨¡å‹æ€§èƒ½å¼‚å¸¸ï¼Œéœ€è¦æ·±å…¥è°ƒè¯•")
    
    print(f"\nâœ… çœŸå®mAPæµ‹è¯•å®Œæˆ")


if __name__ == '__main__':
    main()
