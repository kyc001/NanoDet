#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
PyTorchç‰ˆæœ¬å‚è€ƒæµ‹è¯•
è®°å½•æ‰€æœ‰ç»†èŠ‚ï¼šå‚æ•°æ•°é‡ã€å‡½æ•°è°ƒç”¨ã€mAPç»“æœ
ä½œä¸ºJittorç‰ˆæœ¬å¯¹é½çš„æ ‡å‡†å‚è€ƒ
"""

import os
import sys
import json
import cv2
import torch
import numpy as np
from collections import defaultdict

# æ·»åŠ PyTorchç‰ˆæœ¬è·¯å¾„
sys.path.insert(0, '/home/kyc/project/nanodet/nanodet-pytorch')

# å¯¼å…¥PyTorchç‰ˆæœ¬çš„æ¨¡å—
from nanodet.model.arch import build_model
from nanodet.util import cfg, load_config
from nanodet.data.transform import Pipeline


def load_pytorch_config():
    """åŠ è½½PyTorché…ç½®"""
    config_path = "/home/kyc/project/nanodet/nanodet-pytorch/config/nanodet-plus-m_320_voc.yml"
    
    print(f"åŠ è½½PyTorché…ç½®: {config_path}")
    
    # åŠ è½½é…ç½®
    load_config(cfg, config_path)
    
    print("âœ“ PyTorché…ç½®åŠ è½½æˆåŠŸ")
    print(f"  æ¨¡å‹ç±»å‹: {cfg.model.arch.name}")
    print(f"  è¾“å…¥å°ºå¯¸: {cfg.data.train.input_size}")
    print(f"  ç±»åˆ«æ•°é‡: {cfg.model.arch.head.num_classes}")
    
    return cfg


def create_pytorch_model(cfg):
    """åˆ›å»ºPyTorchæ¨¡å‹"""
    print("åˆ›å»ºPyTorchæ¨¡å‹...")
    
    # åˆ›å»ºæ¨¡å‹
    model = build_model(cfg.model)
    
    print("âœ“ PyTorchæ¨¡å‹åˆ›å»ºæˆåŠŸ")
    
    # ç»Ÿè®¡å‚æ•°æ•°é‡
    total_params = 0
    trainable_params = 0
    
    param_details = {}
    
    for name, param in model.named_parameters():
        param_count = param.numel()
        total_params += param_count
        
        if param.requires_grad:
            trainable_params += param_count
        
        # è®°å½•å‚æ•°è¯¦æƒ…
        param_details[name] = {
            'shape': list(param.shape),
            'count': param_count,
            'requires_grad': param.requires_grad,
            'dtype': str(param.dtype)
        }
    
    print(f"ğŸ“Š PyTorchæ¨¡å‹å‚æ•°ç»Ÿè®¡:")
    print(f"  æ€»å‚æ•°æ•°é‡: {total_params:,}")
    print(f"  å¯è®­ç»ƒå‚æ•°: {trainable_params:,}")
    print(f"  å‚æ•°é¡¹æ•°é‡: {len(param_details)}")
    
    # æŒ‰æ¨¡å—åˆ†ç»„ç»Ÿè®¡
    module_stats = defaultdict(int)
    for name, details in param_details.items():
        module_name = name.split('.')[0]
        module_stats[module_name] += details['count']
    
    print(f"\nğŸ“Š æŒ‰æ¨¡å—ç»Ÿè®¡:")
    for module, count in sorted(module_stats.items()):
        print(f"  {module}: {count:,} å‚æ•°")
    
    return model, param_details


def load_pytorch_checkpoint(model, checkpoint_path):
    """åŠ è½½PyTorch checkpoint"""
    print(f"åŠ è½½PyTorch checkpoint: {checkpoint_path}")
    
    if not os.path.exists(checkpoint_path):
        print(f"âŒ checkpointæ–‡ä»¶ä¸å­˜åœ¨: {checkpoint_path}")
        return False
    
    # åŠ è½½checkpoint
    checkpoint = torch.load(checkpoint_path, map_location='cpu')
    state_dict = checkpoint.get('state_dict', checkpoint)
    
    print(f"âœ“ checkpointåŒ…å« {len(state_dict)} ä¸ªå‚æ•°")
    
    # åˆ†æcheckpointå‚æ•°
    checkpoint_details = {}
    for name, param in state_dict.items():
        checkpoint_details[name] = {
            'shape': list(param.shape),
            'count': param.numel(),
            'dtype': str(param.dtype)
        }
    
    # åŠ è½½åˆ°æ¨¡å‹
    try:
        # ç§»é™¤å¯èƒ½çš„å‰ç¼€
        new_state_dict = {}
        for key, value in state_dict.items():
            new_key = key.replace('model.', '') if key.startswith('model.') else key
            new_state_dict[new_key] = value
        
        # åŠ è½½å‚æ•°
        missing_keys, unexpected_keys = model.load_state_dict(new_state_dict, strict=False)
        
        print(f"âœ“ PyTorchæ¨¡å‹æƒé‡åŠ è½½å®Œæˆ")
        print(f"  ç¼ºå¤±å‚æ•°: {len(missing_keys)}")
        print(f"  å¤šä½™å‚æ•°: {len(unexpected_keys)}")
        
        if len(missing_keys) > 0:
            print(f"  ç¼ºå¤±å‚æ•°ç¤ºä¾‹: {missing_keys[:5]}")
        if len(unexpected_keys) > 0:
            print(f"  å¤šä½™å‚æ•°ç¤ºä¾‹: {unexpected_keys[:5]}")
        
        return True, checkpoint_details
        
    except Exception as e:
        print(f"âŒ æƒé‡åŠ è½½å¤±è´¥: {e}")
        return False, {}


def test_pytorch_inference(model, cfg):
    """æµ‹è¯•PyTorchæ¨ç†"""
    print("\nğŸ” æµ‹è¯•PyTorchæ¨ç†...")
    
    # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
    model.eval()
    
    # åˆ›å»ºæµ‹è¯•è¾“å…¥
    input_size = cfg.data.train.input_size  # [w, h]
    test_input = torch.randn(1, 3, input_size[1], input_size[0])  # [B, C, H, W]
    
    print(f"  è¾“å…¥å½¢çŠ¶: {test_input.shape}")
    print(f"  è¾“å…¥æ•°å€¼èŒƒå›´: [{test_input.min():.6f}, {test_input.max():.6f}]")
    
    # æ¨ç†
    with torch.no_grad():
        output = model(test_input)
    
    print(f"âœ… PyTorchæ¨ç†æˆåŠŸ!")
    print(f"  è¾“å‡ºå½¢çŠ¶: {output.shape}")
    print(f"  è¾“å‡ºæ•°å€¼èŒƒå›´: [{output.min():.6f}, {output.max():.6f}]")
    
    # åˆ†æè¾“å‡ºé€šé“
    if len(output.shape) == 3:  # [B, N, C]
        batch_size, num_anchors, num_channels = output.shape
        print(f"  æ‰¹æ¬¡å¤§å°: {batch_size}")
        print(f"  é”šç‚¹æ•°é‡: {num_anchors}")
        print(f"  è¾“å‡ºé€šé“: {num_channels}")
        
        # åˆ†æé€šé“åˆ†é…
        num_classes = cfg.model.arch.head.num_classes
        reg_max = cfg.model.arch.head.reg_max
        expected_cls_channels = num_classes
        expected_reg_channels = 4 * (reg_max + 1)
        expected_total = expected_cls_channels + expected_reg_channels
        
        print(f"\nğŸ”¹ é€šé“åˆ†æ:")
        print(f"  ç±»åˆ«æ•°: {num_classes}")
        print(f"  reg_max: {reg_max}")
        print(f"  æœŸæœ›åˆ†ç±»é€šé“: {expected_cls_channels}")
        print(f"  æœŸæœ›å›å½’é€šé“: {expected_reg_channels}")
        print(f"  æœŸæœ›æ€»é€šé“: {expected_total}")
        print(f"  å®é™…æ€»é€šé“: {num_channels}")
        
        if num_channels == expected_total:
            print("âœ… è¾“å‡ºé€šé“æ•°æ­£ç¡®")
        else:
            print("âŒ è¾“å‡ºé€šé“æ•°ä¸æ­£ç¡®")
    
    return output


def test_pytorch_postprocess(model, cfg):
    """æµ‹è¯•PyTorchåå¤„ç†"""
    print("\nğŸ” æµ‹è¯•PyTorchåå¤„ç†...")
    
    # åŠ è½½ä¸€å¼ çœŸå®å›¾åƒ
    test_img_path = "data/VOCdevkit/VOC2007/JPEGImages/000001.jpg"
    
    if not os.path.exists(test_img_path):
        print(f"âŒ æµ‹è¯•å›¾åƒä¸å­˜åœ¨: {test_img_path}")
        return None
    
    # è¯»å–å›¾åƒ
    img = cv2.imread(test_img_path)
    original_shape = img.shape[:2]  # (H, W)
    
    print(f"  åŸå§‹å›¾åƒå½¢çŠ¶: {original_shape}")
    
    # é¢„å¤„ç†
    input_size = cfg.data.train.input_size  # [w, h]
    img_resized = cv2.resize(img, tuple(input_size))
    
    # è½¬æ¢ä¸ºtensor
    img_tensor = torch.from_numpy(img_resized.transpose(2, 0, 1)).unsqueeze(0).float()
    
    # æµ‹è¯•ä¸åŒçš„å½’ä¸€åŒ–æ–¹å¼
    print(f"\nğŸ“Š æµ‹è¯•ä¸åŒé¢„å¤„ç†æ–¹å¼:")
    
    # æ–¹å¼1: æ— å½’ä¸€åŒ–
    print(f"\n1ï¸âƒ£ æ— å½’ä¸€åŒ–:")
    with torch.no_grad():
        output1 = model(img_tensor)
    
    cls_preds1 = output1[:, :, :cfg.model.arch.head.num_classes]
    cls_scores1 = torch.sigmoid(cls_preds1)
    max_score1 = cls_scores1.max().item()
    print(f"  æœ€é«˜ç½®ä¿¡åº¦: {max_score1:.6f}")
    
    # æ–¹å¼2: ImageNetå½’ä¸€åŒ–
    print(f"\n2ï¸âƒ£ ImageNetå½’ä¸€åŒ–:")
    mean = torch.tensor([123.675, 116.28, 103.53]).reshape(1, 3, 1, 1)
    std = torch.tensor([58.395, 57.12, 57.375]).reshape(1, 3, 1, 1)
    img_normalized = (img_tensor - mean) / std
    
    with torch.no_grad():
        output2 = model(img_normalized)
    
    cls_preds2 = output2[:, :, :cfg.model.arch.head.num_classes]
    cls_scores2 = torch.sigmoid(cls_preds2)
    max_score2 = cls_scores2.max().item()
    print(f"  æœ€é«˜ç½®ä¿¡åº¦: {max_score2:.6f}")
    
    # æ–¹å¼3: 0-1å½’ä¸€åŒ–
    print(f"\n3ï¸âƒ£ 0-1å½’ä¸€åŒ–:")
    img_01 = img_tensor / 255.0
    
    with torch.no_grad():
        output3 = model(img_01)
    
    cls_preds3 = output3[:, :, :cfg.model.arch.head.num_classes]
    cls_scores3 = torch.sigmoid(cls_preds3)
    max_score3 = cls_scores3.max().item()
    print(f"  æœ€é«˜ç½®ä¿¡åº¦: {max_score3:.6f}")
    
    # é€‰æ‹©æœ€ä½³æ–¹å¼
    max_scores = [max_score1, max_score2, max_score3]
    best_method = np.argmax(max_scores)
    method_names = ["æ— å½’ä¸€åŒ–", "ImageNetå½’ä¸€åŒ–", "0-1å½’ä¸€åŒ–"]
    
    print(f"\nğŸ† æœ€ä½³é¢„å¤„ç†æ–¹å¼: {method_names[best_method]} (ç½®ä¿¡åº¦: {max_scores[best_method]:.6f})")
    
    # ä½¿ç”¨æœ€ä½³æ–¹å¼çš„è¾“å‡º
    if best_method == 0:
        best_output = output1
    elif best_method == 1:
        best_output = output2
    else:
        best_output = output3
    
    return best_output, method_names[best_method], max_scores[best_method]


def analyze_pytorch_postprocess_functions():
    """åˆ†æPyTorchç‰ˆæœ¬ä½¿ç”¨çš„åå¤„ç†å‡½æ•°"""
    print("\nğŸ” åˆ†æPyTorchåå¤„ç†å‡½æ•°...")
    
    # æ£€æŸ¥PyTorchç‰ˆæœ¬çš„åå¤„ç†æ¨¡å—
    try:
        from nanodet.util.postprocess import postprocess
        print("âœ“ æ‰¾åˆ°PyTorchåå¤„ç†å‡½æ•°: nanodet.util.postprocess.postprocess")
    except ImportError:
        print("âŒ æœªæ‰¾åˆ°æ ‡å‡†åå¤„ç†å‡½æ•°")
    
    # æ£€æŸ¥å…¶ä»–å¯èƒ½çš„åå¤„ç†æ¨¡å—
    postprocess_modules = [
        'nanodet.util.postprocess',
        'nanodet.model.head.nanodet_plus_head',
        'nanodet.util.nms',
        'nanodet.util.bbox_util'
    ]
    
    for module_name in postprocess_modules:
        try:
            module = __import__(module_name, fromlist=[''])
            functions = [name for name in dir(module) if not name.startswith('_')]
            print(f"âœ“ æ¨¡å— {module_name} åŒ…å«å‡½æ•°: {functions[:10]}")
        except ImportError:
            print(f"âŒ æ¨¡å— {module_name} ä¸å­˜åœ¨")


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ å¼€å§‹PyTorchç‰ˆæœ¬å‚è€ƒæµ‹è¯•")
    print("=" * 80)
    
    # 1. åŠ è½½é…ç½®
    cfg = load_pytorch_config()
    
    # 2. åˆ›å»ºæ¨¡å‹
    model, param_details = create_pytorch_model(cfg)
    
    # 3. åŠ è½½æƒé‡
    checkpoint_path = "/home/kyc/project/nanodet/nanodet-pytorch/workspace/nanodet-plus-m_320_voc_bs64/model_best/model_best.ckpt"
    success, checkpoint_details = load_pytorch_checkpoint(model, checkpoint_path)
    
    if not success:
        print("âŒ PyTorchæ¨¡å‹åŠ è½½å¤±è´¥")
        return False
    
    # 4. æµ‹è¯•æ¨ç†
    output = test_pytorch_inference(model, cfg)
    
    # 5. æµ‹è¯•åå¤„ç†
    postprocess_result = test_pytorch_postprocess(model, cfg)
    
    # 6. åˆ†æåå¤„ç†å‡½æ•°
    analyze_pytorch_postprocess_functions()
    
    # 7. ä¿å­˜è¯¦ç»†è®°å½•
    record = {
        'config': {
            'model_name': cfg.model.arch.name,
            'input_size': cfg.data.train.input_size,
            'num_classes': cfg.model.arch.head.num_classes,
            'reg_max': cfg.model.arch.head.reg_max
        },
        'model_params': param_details,
        'checkpoint_params': checkpoint_details,
        'inference_result': {
            'output_shape': list(output.shape),
            'output_range': [float(output.min()), float(output.max())]
        }
    }
    
    if postprocess_result:
        best_output, best_method, best_score = postprocess_result
        record['postprocess_result'] = {
            'best_method': best_method,
            'best_score': best_score,
            'output_shape': list(best_output.shape)
        }
    
    # ä¿å­˜è®°å½•
    with open('pytorch_reference_record.json', 'w') as f:
        json.dump(record, f, indent=2)
    
    print("\nâœ… PyTorchå‚è€ƒæµ‹è¯•å®Œæˆ!")
    print("  è¯¦ç»†è®°å½•å·²ä¿å­˜åˆ°: pytorch_reference_record.json")
    
    return True


if __name__ == '__main__':
    success = main()
    sys.exit(0 if success else 1)
