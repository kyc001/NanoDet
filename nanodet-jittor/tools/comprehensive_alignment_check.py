#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
å…¨é¢å¯¹é½æ£€æŸ¥å·¥å…·
ç³»ç»Ÿæ€§è§£å†³æ‰€æœ‰PyTorchåˆ°Jittorè¿ç§»é—®é¢˜
"""

import os
import sys
import torch
import jittor as jt
import numpy as np
from collections import OrderedDict

# æ·»åŠ è·¯å¾„
sys.path.append('/home/kyc/project/nanodet/nanodet-jittor')
from nanodet.model.arch.nanodet_plus import NanoDetPlus


def check_model_architecture_alignment():
    """æ£€æŸ¥æ¨¡å‹æ¶æ„å¯¹é½"""
    print("ğŸ” æ£€æŸ¥æ¨¡å‹æ¶æ„å¯¹é½")
    print("=" * 60)
    
    # åˆ›å»ºJittoræ¨¡å‹
    backbone_cfg = {
        'name': 'ShuffleNetV2',
        'model_size': '1.0x',
        'out_stages': [2, 3, 4],
        'activation': 'LeakyReLU',
        'pretrain': False  # ä¸åŠ è½½é¢„è®­ç»ƒæƒé‡ï¼Œä¸“æ³¨äºæ¶æ„æ£€æŸ¥
    }
    
    fpn_cfg = {
        'name': 'GhostPAN',
        'in_channels': [116, 232, 464],
        'out_channels': 96,
        'kernel_size': 5,
        'num_extra_level': 1,
        'use_depthwise': True,
        'activation': 'LeakyReLU'
    }
    
    head_cfg = {
        'name': 'NanoDetPlusHead',
        'num_classes': 20,
        'input_channel': 96,
        'feat_channels': 96,
        'stacked_convs': 2,
        'kernel_size': 5,
        'strides': [8, 16, 32, 64],
        'activation': 'LeakyReLU',
        'reg_max': 7,
        'norm_cfg': {'type': 'BN'},
        'loss': {
            'loss_qfl': {
                'name': 'QualityFocalLoss',
                'use_sigmoid': True,
                'beta': 2.0,
                'loss_weight': 1.0
            },
            'loss_dfl': {
                'name': 'DistributionFocalLoss',
                'loss_weight': 0.25
            },
            'loss_bbox': {
                'name': 'GIoULoss',
                'loss_weight': 2.0
            }
        }
    }
    
    aux_head_cfg = {
        'name': 'SimpleConvHead',
        'num_classes': 20,
        'input_channel': 192,
        'feat_channels': 192,
        'stacked_convs': 4,
        'strides': [8, 16, 32, 64],
        'activation': 'LeakyReLU',
        'reg_max': 7
    }
    
    model = NanoDetPlus(backbone_cfg, fpn_cfg, aux_head_cfg, head_cfg)
    
    # åˆ†ææ¨¡å‹ç»“æ„
    print(f"Jittoræ¨¡å‹ç»“æ„åˆ†æ:")
    
    # ç»Ÿè®¡å‚æ•°æ•°é‡
    total_params = 0
    trainable_params = 0
    
    param_groups = {
        'backbone': 0,
        'fpn': 0,
        'aux_fpn': 0,
        'head': 0,
        'aux_head': 0,
        'other': 0
    }
    
    for name, param in model.named_parameters():
        param_count = param.numel()
        total_params += param_count
        
        if param.requires_grad:
            trainable_params += param_count
        
        # åˆ†ç»„ç»Ÿè®¡
        if name.startswith('backbone.'):
            param_groups['backbone'] += param_count
        elif name.startswith('fpn.'):
            param_groups['fpn'] += param_count
        elif name.startswith('aux_fpn.'):
            param_groups['aux_fpn'] += param_count
        elif name.startswith('head.'):
            param_groups['head'] += param_count
        elif name.startswith('aux_head.'):
            param_groups['aux_head'] += param_count
        else:
            param_groups['other'] += param_count
    
    print(f"  æ€»å‚æ•°æ•°: {total_params:,}")
    print(f"  å¯è®­ç»ƒå‚æ•°: {trainable_params:,}")
    print(f"  å‚æ•°åˆ†å¸ƒ:")
    for group, count in param_groups.items():
        print(f"    {group}: {count:,} ({count/total_params*100:.1f}%)")
    
    # æ£€æŸ¥æ¨¡å‹å±‚æ•°
    total_modules = 0
    module_types = {}
    
    for name, module in model.named_modules():
        if name:  # è·³è¿‡æ ¹æ¨¡å—
            total_modules += 1
            module_type = type(module).__name__
            module_types[module_type] = module_types.get(module_type, 0) + 1
    
    print(f"\n  æ€»æ¨¡å—æ•°: {total_modules}")
    print(f"  æ¨¡å—ç±»å‹åˆ†å¸ƒ:")
    for module_type, count in sorted(module_types.items()):
        print(f"    {module_type}: {count}")
    
    return model


def check_batchnorm_parameters():
    """æ£€æŸ¥BatchNormå‚æ•°é—®é¢˜"""
    print(f"\nğŸ” æ£€æŸ¥BatchNormå‚æ•°é—®é¢˜")
    print("=" * 60)
    
    # åˆ›å»ºæ¨¡å‹
    model = check_model_architecture_alignment()
    
    # ç»Ÿè®¡BatchNormç›¸å…³å‚æ•°
    bn_params = []
    bn_buffers = []
    scale_params = []
    
    for name, param in model.named_parameters():
        if 'running_mean' in name or 'running_var' in name:
            bn_buffers.append((name, param.shape))
        elif ('weight' in name or 'bias' in name) and ('bn' in name.lower() or 'norm' in name.lower() or '.1.' in name):
            bn_params.append((name, param.shape))
        elif 'scale' in name:
            scale_params.append((name, param.shape))
    
    print(f"BatchNormå‚æ•°ç»Ÿè®¡:")
    print(f"  BNæƒé‡/åç½®å‚æ•°: {len(bn_params)}")
    print(f"  BNç»Ÿè®¡å‚æ•°(running_mean/var): {len(bn_buffers)}")
    print(f"  Scaleå‚æ•°: {len(scale_params)}")
    
    if scale_params:
        print(f"\nScaleå‚æ•°è¯¦æƒ…:")
        for name, shape in scale_params[:5]:  # åªæ˜¾ç¤ºå‰5ä¸ª
            print(f"    {name}: {shape}")
    
    # æ£€æŸ¥PyTorchæƒé‡ä¸­çš„å¯¹åº”å‚æ•°
    print(f"\næ£€æŸ¥PyTorchæƒé‡ä¸­çš„å¯¹åº”å‚æ•°:")
    checkpoint_path = "/home/kyc/project/nanodet/nanodet-pytorch/workspace/nanodet-plus-m_320_voc_bs64/model_best/model_best.ckpt"
    
    try:
        ckpt = torch.load(checkpoint_path, map_location='cpu')
        state_dict = ckpt.get('state_dict', ckpt)
        
        pytorch_bn_params = []
        pytorch_scale_params = []
        
        for name, param in state_dict.items():
            if 'scale' in name:
                pytorch_scale_params.append((name, param.shape))
            elif ('weight' in name or 'bias' in name) and ('bn' in name.lower() or 'norm' in name.lower() or '.1.' in name):
                pytorch_bn_params.append((name, param.shape))
        
        print(f"  PyTorch BNå‚æ•°: {len(pytorch_bn_params)}")
        print(f"  PyTorch Scaleå‚æ•°: {len(pytorch_scale_params)}")
        
        if pytorch_scale_params:
            print(f"\n  PyTorch Scaleå‚æ•°è¯¦æƒ…:")
            for name, shape in pytorch_scale_params[:5]:
                print(f"    {name}: {shape}")
                
                # æ£€æŸ¥å¯¹åº”çš„Jittorå‚æ•°
                jittor_name = name
                if jittor_name.startswith("model."):
                    jittor_name = jittor_name[6:]
                
                jittor_param = None
                for jname, jparam in model.named_parameters():
                    if jname == jittor_name:
                        jittor_param = jparam
                        break
                
                if jittor_param is not None:
                    print(f"      å¯¹åº”Jittor: {jittor_name}: {jittor_param.shape}")
                    if list(param.shape) != list(jittor_param.shape):
                        print(f"      âŒ å½¢çŠ¶ä¸åŒ¹é…: PyTorch{param.shape} vs Jittor{jittor_param.shape}")
                else:
                    print(f"      âŒ åœ¨Jittorä¸­æœªæ‰¾åˆ°å¯¹åº”å‚æ•°")
        
    except Exception as e:
        print(f"âŒ åŠ è½½PyTorchæƒé‡å¤±è´¥: {e}")


def check_distribution_project_issue():
    """æ£€æŸ¥distribution_projecté—®é¢˜"""
    print(f"\nğŸ” æ£€æŸ¥distribution_projecté—®é¢˜")
    print("=" * 60)
    
    # åˆ›å»ºæ¨¡å‹
    model = check_model_architecture_alignment()
    
    # æ£€æŸ¥headä¸­çš„distribution_project
    if hasattr(model.head, 'distribution_project'):
        dist_proj = model.head.distribution_project
        print(f"âœ… distribution_projectå­˜åœ¨")
        print(f"  ç±»å‹: {type(dist_proj)}")
        
        if hasattr(dist_proj, 'project'):
            project = dist_proj.project
            print(f"  projectå±æ€§: {type(project)}")
            print(f"  projectå½¢çŠ¶: {project.shape if hasattr(project, 'shape') else 'N/A'}")
            
            # æ£€æŸ¥æ˜¯å¦åœ¨named_parametersä¸­
            found_in_params = False
            for name, param in model.named_parameters():
                if 'distribution_project.project' in name:
                    found_in_params = True
                    print(f"  âœ… åœ¨named_parametersä¸­æ‰¾åˆ°: {name}")
                    break
            
            if not found_in_params:
                print(f"  âœ… ä¸åœ¨named_parametersä¸­ (æ­£ç¡®)")
        else:
            print(f"  âŒ æ²¡æœ‰projectå±æ€§")
    else:
        print(f"âŒ distribution_projectä¸å­˜åœ¨")


def check_weight_loading_compatibility():
    """æ£€æŸ¥æƒé‡åŠ è½½å…¼å®¹æ€§"""
    print(f"\nğŸ” æ£€æŸ¥æƒé‡åŠ è½½å…¼å®¹æ€§")
    print("=" * 60)
    
    # åˆ›å»ºæ¨¡å‹
    model = check_model_architecture_alignment()
    
    # åŠ è½½PyTorchæƒé‡
    checkpoint_path = "/home/kyc/project/nanodet/nanodet-pytorch/workspace/nanodet-plus-m_320_voc_bs64/model_best/model_best.ckpt"
    
    try:
        ckpt = torch.load(checkpoint_path, map_location='cpu')
        state_dict = ckpt.get('state_dict', ckpt)
        
        print(f"PyTorchæƒé‡æ–‡ä»¶:")
        print(f"  æ€»å‚æ•°æ•°: {len(state_dict)}")
        
        # è·å–Jittoræ¨¡å‹å‚æ•°
        jittor_params = {}
        for name, param in model.named_parameters():
            jittor_params[name] = param
        
        print(f"Jittoræ¨¡å‹å‚æ•°:")
        print(f"  æ€»å‚æ•°æ•°: {len(jittor_params)}")
        
        # åˆ†æåŒ¹é…æƒ…å†µ
        matched = 0
        shape_mismatch = 0
        missing_in_jittor = 0
        missing_in_pytorch = 0
        
        # PyTorch -> JittoråŒ¹é…
        for pytorch_name, pytorch_param in state_dict.items():
            jittor_name = pytorch_name
            if jittor_name.startswith("model."):
                jittor_name = jittor_name[6:]
            
            if "num_batches_tracked" in jittor_name or jittor_name.startswith("avg_"):
                continue
            
            if "distribution_project.project" in jittor_name:
                continue
            
            if jittor_name in jittor_params:
                jittor_param = jittor_params[jittor_name]
                
                if list(pytorch_param.shape) == list(jittor_param.shape):
                    matched += 1
                elif "scale" in jittor_name and len(pytorch_param.shape) == 0 and list(jittor_param.shape) == [1]:
                    matched += 1  # Scaleå‚æ•°ç‰¹æ®Šå¤„ç†
                else:
                    shape_mismatch += 1
                    print(f"    å½¢çŠ¶ä¸åŒ¹é…: {jittor_name} PyTorch{pytorch_param.shape} vs Jittor{jittor_param.shape}")
            else:
                missing_in_jittor += 1
        
        # Jittor -> PyTorchåŒ¹é…
        for jittor_name in jittor_params.keys():
            pytorch_name = f"model.{jittor_name}"
            if pytorch_name not in state_dict:
                missing_in_pytorch += 1
        
        print(f"\næƒé‡åŒ¹é…åˆ†æ:")
        print(f"  âœ… æˆåŠŸåŒ¹é…: {matched}")
        print(f"  âŒ å½¢çŠ¶ä¸åŒ¹é…: {shape_mismatch}")
        print(f"  âŒ Jittorä¸­ç¼ºå¤±: {missing_in_jittor}")
        print(f"  âŒ PyTorchä¸­ç¼ºå¤±: {missing_in_pytorch}")
        
        success_rate = matched / (matched + shape_mismatch + missing_in_jittor) * 100 if (matched + shape_mismatch + missing_in_jittor) > 0 else 0
        print(f"  ğŸ“Š åŒ¹é…æˆåŠŸç‡: {success_rate:.1f}%")
        
        return success_rate > 95
        
    except Exception as e:
        print(f"âŒ æƒé‡åŠ è½½æ£€æŸ¥å¤±è´¥: {e}")
        return False


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ å¼€å§‹å…¨é¢å¯¹é½æ£€æŸ¥")
    
    # 1. æ¨¡å‹æ¶æ„å¯¹é½æ£€æŸ¥
    model = check_model_architecture_alignment()
    
    # 2. BatchNormå‚æ•°æ£€æŸ¥
    check_batchnorm_parameters()
    
    # 3. distribution_projecté—®é¢˜æ£€æŸ¥
    check_distribution_project_issue()
    
    # 4. æƒé‡åŠ è½½å…¼å®¹æ€§æ£€æŸ¥
    weight_compatible = check_weight_loading_compatibility()
    
    print(f"\nğŸ“Š å…¨é¢æ£€æŸ¥æ€»ç»“:")
    print(f"  æ¨¡å‹æ¶æ„: âœ…")
    print(f"  æƒé‡å…¼å®¹æ€§: {'âœ…' if weight_compatible else 'âŒ'}")
    
    print(f"\nâœ… å…¨é¢å¯¹é½æ£€æŸ¥å®Œæˆ")


if __name__ == '__main__':
    main()
