#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
ÂÖ®Èù¢ÂØπÈΩêÊ£ÄÊü•Â∑•ÂÖ∑
Á≥ªÁªüÊÄßËß£ÂÜ≥ÊâÄÊúâPyTorchÂà∞JittorËøÅÁßªÈóÆÈ¢ò
"""

import os
import sys
import torch
import jittor as jt
import numpy as np
from collections import OrderedDict

# Ê∑ªÂä†Ë∑ØÂæÑ
sys.path.append('/home/kyc/project/nanodet/nanodet-jittor')
from nanodet.model.arch.nanodet_plus import NanoDetPlus


def check_model_architecture_alignment():
    """Ê£ÄÊü•Ê®°ÂûãÊû∂ÊûÑÂØπÈΩê"""
    print("üîç Ê£ÄÊü•Ê®°ÂûãÊû∂ÊûÑÂØπÈΩê")
    print("=" * 60)
    
    # ÂàõÂª∫JittorÊ®°Âûã
    backbone_cfg = {
        'name': 'ShuffleNetV2',
        'model_size': '1.0x',
        'out_stages': [2, 3, 4],
        'activation': 'LeakyReLU',
        'pretrain': False  # ‰∏çÂä†ËΩΩÈ¢ÑËÆ≠ÁªÉÊùÉÈáçÔºå‰∏ìÊ≥®‰∫éÊû∂ÊûÑÊ£ÄÊü•
    }
    
    fpn_cfg = {
        'name': 'GhostPAN',
        'in_channels': [116, 232, 464],
        'out_channels': 96,
        'kernel_size': 5,
        'num_extra_level': 1,
        'use_depthwise': True,
        'activation': 'LeakyReLU'
    }
    
    head_cfg = {
        'name': 'NanoDetPlusHead',
        'num_classes': 20,
        'input_channel': 96,
        'feat_channels': 96,
        'stacked_convs': 2,
        'kernel_size': 5,
        'strides': [8, 16, 32, 64],
        'activation': 'LeakyReLU',
        'reg_max': 7,
        'norm_cfg': {'type': 'BN'},
        'loss': {
            'loss_qfl': {
                'name': 'QualityFocalLoss',
                'use_sigmoid': True,
                'beta': 2.0,
                'loss_weight': 1.0
            },
            'loss_dfl': {
                'name': 'DistributionFocalLoss',
                'loss_weight': 0.25
            },
            'loss_bbox': {
                'name': 'GIoULoss',
                'loss_weight': 2.0
            }
        }
    }
    
    aux_head_cfg = {
        'name': 'SimpleConvHead',
        'num_classes': 20,
        'input_channel': 192,
        'feat_channels': 192,
        'stacked_convs': 4,
        'strides': [8, 16, 32, 64],
        'activation': 'LeakyReLU',
        'reg_max': 7
    }
    
    model = NanoDetPlus(backbone_cfg, fpn_cfg, aux_head_cfg, head_cfg)
    
    # ÂàÜÊûêÊ®°ÂûãÁªìÊûÑ
    print(f"JittorÊ®°ÂûãÁªìÊûÑÂàÜÊûê:")
    
    # ÁªüËÆ°ÂèÇÊï∞Êï∞Èáè
    total_params = 0
    trainable_params = 0
    
    param_groups = {
        'backbone': 0,
        'fpn': 0,
        'aux_fpn': 0,
        'head': 0,
        'aux_head': 0,
        'other': 0
    }
    
    for name, param in model.named_parameters():
        param_count = param.numel()
        total_params += param_count
        
        if param.requires_grad:
            trainable_params += param_count
        
        # ÂàÜÁªÑÁªüËÆ°
        if name.startswith('backbone.'):
            param_groups['backbone'] += param_count
        elif name.startswith('fpn.'):
            param_groups['fpn'] += param_count
        elif name.startswith('aux_fpn.'):
            param_groups['aux_fpn'] += param_count
        elif name.startswith('head.'):
            param_groups['head'] += param_count
        elif name.startswith('aux_head.'):
            param_groups['aux_head'] += param_count
        else:
            param_groups['other'] += param_count
    
    print(f"  ÊÄªÂèÇÊï∞Êï∞: {total_params:,}")
    print(f"  ÂèØËÆ≠ÁªÉÂèÇÊï∞: {trainable_params:,}")
    print(f"  ÂèÇÊï∞ÂàÜÂ∏É:")
    for group, count in param_groups.items():
        print(f"    {group}: {count:,} ({count/total_params*100:.1f}%)")
    
    # Ê£ÄÊü•Ê®°ÂûãÂ±ÇÊï∞
    total_modules = 0
    module_types = {}
    
    for name, module in model.named_modules():
        if name:  # Ë∑≥ËøáÊ†πÊ®°Âùó
            total_modules += 1
            module_type = type(module).__name__
            module_types[module_type] = module_types.get(module_type, 0) + 1
    
    print(f"\n  ÊÄªÊ®°ÂùóÊï∞: {total_modules}")
    print(f"  Ê®°ÂùóÁ±ªÂûãÂàÜÂ∏É:")
    for module_type, count in sorted(module_types.items()):
        print(f"    {module_type}: {count}")
    
    return model


def check_batchnorm_parameters():
    """Ê£ÄÊü•BatchNormÂèÇÊï∞ÈóÆÈ¢ò"""
    print(f"\nüîç Ê£ÄÊü•BatchNormÂèÇÊï∞ÈóÆÈ¢ò")
    print("=" * 60)
    
    # ÂàõÂª∫Ê®°Âûã
    model = check_model_architecture_alignment()
    
    # ÁªüËÆ°BatchNormÁõ∏ÂÖ≥ÂèÇÊï∞
    bn_params = []
    bn_buffers = []
    scale_params = []
    
    for name, param in model.named_parameters():
        if 'running_mean' in name or 'running_var' in name:
            bn_buffers.append((name, param.shape))
        elif ('weight' in name or 'bias' in name) and ('bn' in name.lower() or 'norm' in name.lower() or '.1.' in name):
            bn_params.append((name, param.shape))
        elif 'scale' in name:
            scale_params.append((name, param.shape))
    
    print(f"BatchNormÂèÇÊï∞ÁªüËÆ°:")
    print(f"  BNÊùÉÈáç/ÂÅèÁΩÆÂèÇÊï∞: {len(bn_params)}")
    print(f"  BNÁªüËÆ°ÂèÇÊï∞(running_mean/var): {len(bn_buffers)}")
    print(f"  ScaleÂèÇÊï∞: {len(scale_params)}")
    
    if scale_params:
        print(f"\nScaleÂèÇÊï∞ËØ¶ÊÉÖ:")
        for name, shape in scale_params[:5]:  # Âè™ÊòæÁ§∫Ââç5‰∏™
            print(f"    {name}: {shape}")
    
    # Ê£ÄÊü•PyTorchÊùÉÈáç‰∏≠ÁöÑÂØπÂ∫îÂèÇÊï∞
    print(f"\nÊ£ÄÊü•PyTorchÊùÉÈáç‰∏≠ÁöÑÂØπÂ∫îÂèÇÊï∞:")
    checkpoint_path = "/home/kyc/project/nanodet/nanodet-pytorch/workspace/nanodet-plus-m_320_voc_bs64/model_best/model_best.ckpt"
    
    try:
        ckpt = torch.load(checkpoint_path, map_location='cpu')
        state_dict = ckpt.get('state_dict', ckpt)
        
        pytorch_bn_params = []
        pytorch_scale_params = []
        
        for name, param in state_dict.items():
            if 'scale' in name:
                pytorch_scale_params.append((name, param.shape))
            elif ('weight' in name or 'bias' in name) and ('bn' in name.lower() or 'norm' in name.lower() or '.1.' in name):
                pytorch_bn_params.append((name, param.shape))
        
        print(f"  PyTorch BNÂèÇÊï∞: {len(pytorch_bn_params)}")
        print(f"  PyTorch ScaleÂèÇÊï∞: {len(pytorch_scale_params)}")
        
        if pytorch_scale_params:
            print(f"\n  PyTorch ScaleÂèÇÊï∞ËØ¶ÊÉÖ:")
            for name, shape in pytorch_scale_params[:5]:
                print(f"    {name}: {shape}")
                
                # Ê£ÄÊü•ÂØπÂ∫îÁöÑJittorÂèÇÊï∞
                jittor_name = name
                if jittor_name.startswith("model."):
                    jittor_name = jittor_name[6:]
                
                jittor_param = None
                for jname, jparam in model.named_parameters():
                    if jname == jittor_name:
                        jittor_param = jparam
                        break
                
                if jittor_param is not None:
                    print(f"      ÂØπÂ∫îJittor: {jittor_name}: {jittor_param.shape}")
                    if list(param.shape) != list(jittor_param.shape):
                        print(f"      ‚ùå ÂΩ¢Áä∂‰∏çÂåπÈÖç: PyTorch{param.shape} vs Jittor{jittor_param.shape}")
                else:
                    print(f"      ‚ùå Âú®Jittor‰∏≠Êú™ÊâæÂà∞ÂØπÂ∫îÂèÇÊï∞")
        
    except Exception as e:
        print(f"‚ùå Âä†ËΩΩPyTorchÊùÉÈáçÂ§±Ë¥•: {e}")


def check_distribution_project_issue():
    """Ê£ÄÊü•distribution_projectÈóÆÈ¢ò"""
    print(f"\nüîç Ê£ÄÊü•distribution_projectÈóÆÈ¢ò")
    print("=" * 60)
    
    # ÂàõÂª∫Ê®°Âûã
    model = check_model_architecture_alignment()
    
    # Ê£ÄÊü•head‰∏≠ÁöÑdistribution_project
    if hasattr(model.head, 'distribution_project'):
        dist_proj = model.head.distribution_project
        print(f"‚úÖ distribution_projectÂ≠òÂú®")
        print(f"  Á±ªÂûã: {type(dist_proj)}")
        
        if hasattr(dist_proj, 'project'):
            project = dist_proj.project
            print(f"  projectÂ±ûÊÄß: {type(project)}")
            print(f"  projectÂΩ¢Áä∂: {project.shape if hasattr(project, 'shape') else 'N/A'}")
            
            # Ê£ÄÊü•ÊòØÂê¶Âú®named_parameters‰∏≠
            found_in_params = False
            for name, param in model.named_parameters():
                if 'distribution_project.project' in name:
                    found_in_params = True
                    print(f"  ‚úÖ Âú®named_parameters‰∏≠ÊâæÂà∞: {name}")
                    break
            
            if not found_in_params:
                print(f"  ‚úÖ ‰∏çÂú®named_parameters‰∏≠ (Ê≠£Á°Æ)")
        else:
            print(f"  ‚ùå Ê≤°ÊúâprojectÂ±ûÊÄß")
    else:
        print(f"‚ùå distribution_project‰∏çÂ≠òÂú®")


def check_weight_loading_compatibility():
    """Ê£ÄÊü•ÊùÉÈáçÂä†ËΩΩÂÖºÂÆπÊÄß"""
    print(f"\nüîç Ê£ÄÊü•ÊùÉÈáçÂä†ËΩΩÂÖºÂÆπÊÄß")
    print("=" * 60)
    
    # ÂàõÂª∫Ê®°Âûã
    model = check_model_architecture_alignment()
    
    # Âä†ËΩΩPyTorchÊùÉÈáç
    checkpoint_path = "/home/kyc/project/nanodet/nanodet-pytorch/workspace/nanodet-plus-m_320_voc_bs64/model_best/model_best.ckpt"
    
    try:
        ckpt = torch.load(checkpoint_path, map_location='cpu')
        state_dict = ckpt.get('state_dict', ckpt)
        
        print(f"PyTorchÊùÉÈáçÊñá‰ª∂:")
        print(f"  ÊÄªÂèÇÊï∞Êï∞: {len(state_dict)}")
        
        # Ëé∑ÂèñJittorÊ®°ÂûãÂèÇÊï∞
        jittor_params = {}
        for name, param in model.named_parameters():
            jittor_params[name] = param
        
        print(f"JittorÊ®°ÂûãÂèÇÊï∞:")
        print(f"  ÊÄªÂèÇÊï∞Êï∞: {len(jittor_params)}")
        
        # ÂàÜÊûêÂåπÈÖçÊÉÖÂÜµ
        matched = 0
        shape_mismatch = 0
        missing_in_jittor = 0
        missing_in_pytorch = 0
        
        # PyTorch -> JittorÂåπÈÖç
        for pytorch_name, pytorch_param in state_dict.items():
            jittor_name = pytorch_name
            if jittor_name.startswith("model."):
                jittor_name = jittor_name[6:]
            
            if "num_batches_tracked" in jittor_name or jittor_name.startswith("avg_"):
                continue
            
            if "distribution_project.project" in jittor_name:
                continue
            
            if jittor_name in jittor_params:
                jittor_param = jittor_params[jittor_name]
                
                if list(pytorch_param.shape) == list(jittor_param.shape):
                    matched += 1
                elif "scale" in jittor_name and len(pytorch_param.shape) == 0 and list(jittor_param.shape) == [1]:
                    matched += 1  # ScaleÂèÇÊï∞ÁâπÊÆäÂ§ÑÁêÜ
                else:
                    shape_mismatch += 1
                    print(f"    ÂΩ¢Áä∂‰∏çÂåπÈÖç: {jittor_name} PyTorch{pytorch_param.shape} vs Jittor{jittor_param.shape}")
            else:
                missing_in_jittor += 1
        
        # Jittor -> PyTorchÂåπÈÖç
        for jittor_name in jittor_params.keys():
            pytorch_name = f"model.{jittor_name}"
            if pytorch_name not in state_dict:
                missing_in_pytorch += 1
        
        print(f"\nÊùÉÈáçÂåπÈÖçÂàÜÊûê:")
        print(f"  ‚úÖ ÊàêÂäüÂåπÈÖç: {matched}")
        print(f"  ‚ùå ÂΩ¢Áä∂‰∏çÂåπÈÖç: {shape_mismatch}")
        print(f"  ‚ùå Jittor‰∏≠Áº∫Â§±: {missing_in_jittor}")
        print(f"  ‚ùå PyTorch‰∏≠Áº∫Â§±: {missing_in_pytorch}")
        
        success_rate = matched / (matched + shape_mismatch + missing_in_jittor) * 100 if (matched + shape_mismatch + missing_in_jittor) > 0 else 0
        print(f"  üìä ÂåπÈÖçÊàêÂäüÁéá: {success_rate:.1f}%")
        
        return success_rate > 95
        
    except Exception as e:
        print(f"‚ùå ÊùÉÈáçÂä†ËΩΩÊ£ÄÊü•Â§±Ë¥•: {e}")
        return False


def main():
    """‰∏ªÂáΩÊï∞"""
    print("üöÄ ÂºÄÂßãÂÖ®Èù¢ÂØπÈΩêÊ£ÄÊü•")
    
    # 1. Ê®°ÂûãÊû∂ÊûÑÂØπÈΩêÊ£ÄÊü•
    model = check_model_architecture_alignment()
    
    # 2. BatchNormÂèÇÊï∞Ê£ÄÊü•
    check_batchnorm_parameters()
    
    # 3. distribution_projectÈóÆÈ¢òÊ£ÄÊü•
    check_distribution_project_issue()
    
    # 4. ÊùÉÈáçÂä†ËΩΩÂÖºÂÆπÊÄßÊ£ÄÊü•
    weight_compatible = check_weight_loading_compatibility()
    
    print(f"\nüìä ÂÖ®Èù¢Ê£ÄÊü•ÊÄªÁªì:")
    print(f"  Ê®°ÂûãÊû∂ÊûÑ: ‚úÖ")
    print(f"  ÊùÉÈáçÂÖºÂÆπÊÄß: {'‚úÖ' if weight_compatible else '‚ùå'}")
    
    print(f"\n‚úÖ ÂÖ®Èù¢ÂØπÈΩêÊ£ÄÊü•ÂÆåÊàê")


if __name__ == '__main__':
    main()
