#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
Headæ·±åº¦è°ƒè¯•å·¥å…·
ä¸“é—¨æ£€æŸ¥Headçš„æƒé‡åŠ è½½å’Œbiasè®¾ç½®
"""

import os
import sys
import torch
import jittor as jt
import numpy as np

# æ·»åŠ è·¯å¾„
sys.path.append('/home/kyc/project/nanodet/nanodet-jittor')
from nanodet.model.arch.nanodet_plus import NanoDetPlus


def create_test_input():
    """åˆ›å»ºå›ºå®šçš„æµ‹è¯•è¾“å…¥"""
    np.random.seed(42)
    torch.manual_seed(42)
    jt.set_global_seed(42)
    
    # ä½¿ç”¨å›ºå®šçš„æµ‹è¯•æ•°æ®
    if os.path.exists("fixed_input_data.npy"):
        input_data = np.load("fixed_input_data.npy")
    else:
        input_data = np.random.randn(1, 3, 320, 320).astype(np.float32)
        np.save("fixed_input_data.npy", input_data)
    
    return input_data


def create_jittor_model():
    """åˆ›å»ºJittoræ¨¡å‹"""
    print("ğŸ” åˆ›å»ºJittoræ¨¡å‹...")
    
    backbone_cfg = {
        'name': 'ShuffleNetV2',
        'model_size': '1.0x',
        'out_stages': [2, 3, 4],
        'activation': 'LeakyReLU',
        'pretrain': True
    }
    
    fpn_cfg = {
        'name': 'GhostPAN',
        'in_channels': [116, 232, 464],
        'out_channels': 96,
        'kernel_size': 5,
        'num_extra_level': 1,
        'use_depthwise': True,
        'activation': 'LeakyReLU'
    }
    
    head_cfg = {
        'name': 'NanoDetPlusHead',
        'num_classes': 20,
        'input_channel': 96,
        'feat_channels': 96,
        'stacked_convs': 2,
        'kernel_size': 5,
        'strides': [8, 16, 32, 64],
        'activation': 'LeakyReLU',
        'reg_max': 7,
        'norm_cfg': {'type': 'BN'},
        'loss': {
            'loss_qfl': {
                'name': 'QualityFocalLoss',
                'use_sigmoid': True,
                'beta': 2.0,
                'loss_weight': 1.0
            },
            'loss_dfl': {
                'name': 'DistributionFocalLoss',
                'loss_weight': 0.25
            },
            'loss_bbox': {
                'name': 'GIoULoss',
                'loss_weight': 2.0
            }
        }
    }
    
    aux_head_cfg = {
        'name': 'SimpleConvHead',
        'num_classes': 20,
        'input_channel': 192,
        'feat_channels': 192,
        'stacked_convs': 4,
        'strides': [8, 16, 32, 64],
        'activation': 'LeakyReLU',
        'reg_max': 7
    }
    
    model = NanoDetPlus(backbone_cfg, fpn_cfg, aux_head_cfg, head_cfg)
    
    # åŠ è½½æƒé‡
    print("åŠ è½½PyTorchæƒé‡...")
    checkpoint_path = "/home/kyc/project/nanodet/nanodet-pytorch/workspace/nanodet-plus-m_320_voc_bs64/model_best/model_best.ckpt"
    ckpt = torch.load(checkpoint_path, map_location='cpu')
    state_dict = ckpt.get('state_dict', ckpt)
    
    jittor_state_dict = {}
    for name, param in model.named_parameters():
        jittor_state_dict[name] = param
    
    # æƒé‡åŠ è½½
    for pytorch_name, pytorch_param in state_dict.items():
        jittor_name = pytorch_name
        if jittor_name.startswith("model."):
            jittor_name = jittor_name[6:]
        
        if "num_batches_tracked" in jittor_name or jittor_name.startswith("avg_"):
            continue
        
        if "distribution_project.project" in jittor_name:
            continue
        
        if jittor_name in jittor_state_dict:
            jittor_param = jittor_state_dict[jittor_name]
            
            if list(pytorch_param.shape) == list(jittor_param.shape):
                jittor_param.assign(jt.array(pytorch_param.detach().numpy()))
            elif "scale" in jittor_name and len(pytorch_param.shape) == 0 and list(jittor_param.shape) == [1]:
                jittor_param.assign(jt.array([pytorch_param.detach().numpy()]))
    
    model.eval()
    return model


def check_head_weights():
    """æ£€æŸ¥Headæƒé‡"""
    print("ğŸ” æ£€æŸ¥Headæƒé‡")
    print("=" * 60)
    
    model = create_jittor_model()
    head = model.head
    
    # åŠ è½½PyTorchæƒé‡
    checkpoint_path = "/home/kyc/project/nanodet/nanodet-pytorch/workspace/nanodet-plus-m_320_voc_bs64/model_best/model_best.ckpt"
    ckpt = torch.load(checkpoint_path, map_location='cpu')
    state_dict = ckpt.get('state_dict', ckpt)
    
    print(f"Headæƒé‡æ£€æŸ¥:")
    
    # æ£€æŸ¥gfl_clsæƒé‡
    for i in range(len(head.gfl_cls)):
        jittor_layer = head.gfl_cls[i]
        
        # æ£€æŸ¥æƒé‡
        weight_name = f"head.gfl_cls.{i}.weight"
        bias_name = f"head.gfl_cls.{i}.bias"
        
        pytorch_weight_name = f"model.{weight_name}"
        pytorch_bias_name = f"model.{bias_name}"
        
        if pytorch_weight_name in state_dict:
            pytorch_weight = state_dict[pytorch_weight_name].detach().numpy()
            jittor_weight = jittor_layer.weight.numpy()
            
            weight_diff = np.abs(pytorch_weight - jittor_weight).max()
            print(f"  gfl_cls.{i}.weight: å·®å¼‚{weight_diff:.10f}")
            
            if weight_diff < 1e-6:
                print(f"    âœ… æƒé‡ä¸€è‡´")
            else:
                print(f"    âŒ æƒé‡ä¸ä¸€è‡´")
                print(f"      PyTorch: èŒƒå›´[{pytorch_weight.min():.6f}, {pytorch_weight.max():.6f}]")
                print(f"      Jittor: èŒƒå›´[{jittor_weight.min():.6f}, {jittor_weight.max():.6f}]")
        
        if pytorch_bias_name in state_dict:
            pytorch_bias = state_dict[pytorch_bias_name].detach().numpy()
            jittor_bias = jittor_layer.bias.numpy()
            
            bias_diff = np.abs(pytorch_bias - jittor_bias).max()
            print(f"  gfl_cls.{i}.bias: å·®å¼‚{bias_diff:.10f}")
            
            if bias_diff < 1e-6:
                print(f"    âœ… biasä¸€è‡´")
            else:
                print(f"    âŒ biasä¸ä¸€è‡´")
                print(f"      PyTorch: èŒƒå›´[{pytorch_bias.min():.6f}, {pytorch_bias.max():.6f}]")
                print(f"      Jittor: èŒƒå›´[{jittor_bias.min():.6f}, {jittor_bias.max():.6f}]")
            
            # ç‰¹åˆ«æ£€æŸ¥åˆ†ç±»bias
            cls_bias = pytorch_bias[:20]  # å‰20ä¸ªæ˜¯åˆ†ç±»bias
            print(f"      åˆ†ç±»bias: èŒƒå›´[{cls_bias.min():.6f}, {cls_bias.max():.6f}]")
            print(f"      åˆ†ç±»biaså‡å€¼: {cls_bias.mean():.6f}")


def check_head_initialization():
    """æ£€æŸ¥Headåˆå§‹åŒ–"""
    print(f"\nğŸ” æ£€æŸ¥Headåˆå§‹åŒ–")
    print("=" * 60)
    
    model = create_jittor_model()
    head = model.head
    
    print(f"Headé…ç½®:")
    print(f"  num_classes: {head.num_classes}")
    print(f"  feat_channels: {head.feat_channels}")
    print(f"  stacked_convs: {head.stacked_convs}")
    print(f"  strides: {head.strides}")
    print(f"  reg_max: {head.reg_max}")
    
    # æ£€æŸ¥gfl_clså±‚çš„é…ç½®
    for i, layer in enumerate(head.gfl_cls):
        print(f"  gfl_cls.{i}: in_channels={layer.in_channels}, out_channels={layer.out_channels}")
        
        # æ£€æŸ¥biasåˆå§‹åŒ–
        bias = layer.bias.numpy()
        cls_bias = bias[:20]  # åˆ†ç±»bias
        reg_bias = bias[20:]  # å›å½’bias
        
        print(f"    åˆ†ç±»bias: èŒƒå›´[{cls_bias.min():.6f}, {cls_bias.max():.6f}], å‡å€¼{cls_bias.mean():.6f}")
        print(f"    å›å½’bias: èŒƒå›´[{reg_bias.min():.6f}, {reg_bias.max():.6f}], å‡å€¼{reg_bias.mean():.6f}")
        
        # æ£€æŸ¥æ˜¯å¦ç¬¦åˆé¢„æœŸçš„åˆå§‹åŒ–
        expected_cls_bias = -4.595  # æ¥è‡ªinit_weights
        if abs(cls_bias.mean() - expected_cls_bias) < 0.1:
            print(f"    âœ… åˆ†ç±»biasåˆå§‹åŒ–æ­£ç¡®")
        else:
            print(f"    âŒ åˆ†ç±»biasåˆå§‹åŒ–å¯èƒ½æœ‰é—®é¢˜ï¼Œé¢„æœŸçº¦{expected_cls_bias}")


def test_head_forward_step_by_step():
    """é€æ­¥æµ‹è¯•Headå‰å‘ä¼ æ’­"""
    print(f"\nğŸ” é€æ­¥æµ‹è¯•Headå‰å‘ä¼ æ’­")
    print("=" * 60)
    
    model = create_jittor_model()
    head = model.head
    
    # åˆ›å»ºæµ‹è¯•è¾“å…¥
    input_data = create_test_input()
    jittor_input = jt.array(input_data)
    
    with jt.no_grad():
        # è·å–FPNç‰¹å¾
        backbone_features = model.backbone(jittor_input)
        fpn_features = model.fpn(backbone_features)
        
        print(f"FPNç‰¹å¾:")
        for i, feat in enumerate(fpn_features):
            print(f"  ç‰¹å¾{i}: {feat.shape}, èŒƒå›´[{feat.min():.6f}, {feat.max():.6f}]")
        
        # æ‰‹åŠ¨æ‰§è¡ŒHeadçš„å‰å‘ä¼ æ’­
        outputs = []
        for level, (feat, cls_convs, gfl_cls) in enumerate(zip(fpn_features, head.cls_convs, head.gfl_cls)):
            print(f"\n  å¤„ç†level {level}:")
            print(f"    è¾“å…¥ç‰¹å¾: {feat.shape}, èŒƒå›´[{feat.min():.6f}, {feat.max():.6f}]")
            
            # é€šè¿‡cls_convs
            current_feat = feat
            for conv_idx, conv in enumerate(cls_convs):
                current_feat = conv(current_feat)
                print(f"    cls_conv{conv_idx}: {current_feat.shape}, èŒƒå›´[{current_feat.min():.6f}, {current_feat.max():.6f}]")
            
            # é€šè¿‡gfl_cls
            output = gfl_cls(current_feat)
            print(f"    gfl_clsè¾“å‡º: {output.shape}, èŒƒå›´[{output.min():.6f}, {output.max():.6f}]")
            
            # åˆ†æè¾“å‡º
            cls_pred = output[:, :20, :, :]
            reg_pred = output[:, 20:, :, :]
            
            print(f"    åˆ†ç±»é¢„æµ‹: èŒƒå›´[{cls_pred.min():.6f}, {cls_pred.max():.6f}]")
            print(f"    å›å½’é¢„æµ‹: èŒƒå›´[{reg_pred.min():.6f}, {reg_pred.max():.6f}]")
            
            # è®¡ç®—ç½®ä¿¡åº¦
            cls_scores = jt.sigmoid(cls_pred)
            max_conf = cls_scores.max()
            print(f"    æœ€é«˜ç½®ä¿¡åº¦: {max_conf:.6f}")
            
            # reshapeå¹¶æ·»åŠ åˆ°è¾“å‡º
            output = output.permute(0, 2, 3, 1).reshape(output.shape[0], -1, output.shape[1])
            outputs.append(output)
        
        # åˆå¹¶æ‰€æœ‰è¾“å‡º
        final_output = jt.concat(outputs, dim=1)
        print(f"\næœ€ç»ˆè¾“å‡º: {final_output.shape}, èŒƒå›´[{final_output.min():.6f}, {final_output.max():.6f}]")
        
        # åˆ†ææœ€ç»ˆè¾“å‡º
        final_cls_pred = final_output[:, :, :20]
        final_cls_scores = jt.sigmoid(final_cls_pred)
        final_max_conf = final_cls_scores.max()
        
        print(f"æœ€ç»ˆæœ€é«˜ç½®ä¿¡åº¦: {final_max_conf:.6f}")
        
        # å¯¹æ¯”å®Œæ•´Headè¾“å‡º
        complete_output = head(fpn_features)
        diff = jt.abs(final_output - complete_output).max()
        print(f"æ‰‹åŠ¨vså®Œæ•´Headå·®å¼‚: {diff:.10f}")


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ å¼€å§‹Headæ·±åº¦è°ƒè¯•")
    
    # æ£€æŸ¥Headæƒé‡
    check_head_weights()
    
    # æ£€æŸ¥Headåˆå§‹åŒ–
    check_head_initialization()
    
    # é€æ­¥æµ‹è¯•Headå‰å‘ä¼ æ’­
    test_head_forward_step_by_step()
    
    print(f"\nâœ… Headæ·±åº¦è°ƒè¯•å®Œæˆ")


if __name__ == '__main__':
    main()
