#!/usr/bin/env python3
"""
ä¿®å¤ DepthwiseConv æ¢¯åº¦é—®é¢˜çš„è„šæœ¬
å°† DepthwiseConvModule æ›¿æ¢ä¸ºæ ‡å‡† ConvModule
"""

import os
import sys
from pathlib import Path

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent))

import jittor as jt
from jittor import nn


class FixedDepthwiseConvModule(nn.Module):
    """
    ä¿®å¤çš„ DepthwiseConv æ¨¡å—ï¼Œä½¿ç”¨æ ‡å‡†å·ç§¯æ›¿ä»£
    """
    def __init__(self, in_channels, out_channels, kernel_size, stride=1, 
                 padding=0, dilation=1, groups=1, bias=True, 
                 norm_cfg=None, activation=None):
        super().__init__()
        
        # ä½¿ç”¨æ ‡å‡†å·ç§¯æ›¿ä»£ depthwise å·ç§¯
        self.conv = nn.Conv2d(
            in_channels, out_channels, kernel_size, 
            stride=stride, padding=padding, dilation=dilation, 
            groups=1, bias=bias  # å¼ºåˆ¶ä½¿ç”¨ groups=1
        )
        
        # æ‰¹å½’ä¸€åŒ–
        if norm_cfg is not None:
            if norm_cfg.get('type') == 'BN':
                self.bn = nn.BatchNorm2d(out_channels)
            else:
                self.bn = None
        else:
            self.bn = None
            
        # æ¿€æ´»å‡½æ•°
        if activation == 'LeakyReLU':
            self.activation = nn.LeakyReLU(0.1)
        elif activation == 'ReLU':
            self.activation = nn.ReLU()
        else:
            self.activation = None
    
    def execute(self, x):
        x = self.conv(x)
        if self.bn is not None:
            x = self.bn(x)
        if self.activation is not None:
            x = self.activation(x)
        return x


def patch_depthwise_conv():
    """
    ä¿®è¡¥ DepthwiseConv ç›¸å…³çš„æ¨¡å—
    """
    print("æ­£åœ¨ä¿®è¡¥ DepthwiseConv æ¨¡å—...")
    
    # å¯¼å…¥éœ€è¦ä¿®è¡¥çš„æ¨¡å—
    try:
        from nanodet.model.module.conv import DepthwiseConvModule
        from nanodet.model.module import conv as conv_module
        
        # æ›¿æ¢ DepthwiseConvModule
        conv_module.DepthwiseConvModule = FixedDepthwiseConvModule
        
        print("âœ… DepthwiseConvModule å·²æ›¿æ¢ä¸ºæ ‡å‡†å·ç§¯å®ç°")
        
    except ImportError as e:
        print(f"âŒ å¯¼å…¥æ¨¡å—å¤±è´¥: {e}")
        return False
    
    return True


def test_fixed_conv():
    """
    æµ‹è¯•ä¿®å¤åçš„å·ç§¯æ¨¡å—
    """
    print("æµ‹è¯•ä¿®å¤åçš„å·ç§¯æ¨¡å—...")
    
    try:
        # åˆ›å»ºæµ‹è¯•æ¨¡å—
        conv = FixedDepthwiseConvModule(
            in_channels=96, 
            out_channels=96, 
            kernel_size=5, 
            padding=2,
            norm_cfg={'type': 'BN'},
            activation='LeakyReLU'
        )
        
        # æµ‹è¯•å‰å‘ä¼ æ’­
        x = jt.randn(1, 96, 40, 40)
        y = conv(x)
        
        print(f"âœ… å‰å‘ä¼ æ’­æµ‹è¯•é€šè¿‡: {x.shape} -> {y.shape}")
        
        # æµ‹è¯•æ¢¯åº¦è®¡ç®—
        loss = y.sum()
        grad = jt.grad(loss, conv.parameters())
        
        print(f"âœ… æ¢¯åº¦è®¡ç®—æµ‹è¯•é€šè¿‡: è®¡ç®—äº† {len(grad)} ä¸ªå‚æ•°çš„æ¢¯åº¦")
        
        return True
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False


def main():
    print("=== DepthwiseConv ä¿®å¤å·¥å…· ===")
    
    # ä¿®è¡¥æ¨¡å—
    if not patch_depthwise_conv():
        print("âŒ ä¿®è¡¥å¤±è´¥")
        return 1
    
    # æµ‹è¯•ä¿®å¤
    if not test_fixed_conv():
        print("âŒ æµ‹è¯•å¤±è´¥")
        return 1
    
    print("ğŸ‰ DepthwiseConv ä¿®å¤å®Œæˆï¼")
    return 0


if __name__ == '__main__':
    sys.exit(main())
