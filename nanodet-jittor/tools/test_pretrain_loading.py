#!/usr/bin/env python3
"""
æµ‹è¯•é¢„è®­ç»ƒæƒé‡åŠ è½½
é€æ­¥è°ƒè¯•é¢„è®­ç»ƒæƒé‡åŠ è½½é—®é¢˜
"""

import os
import sys
import logging
import jittor as jt
from pathlib import Path
import traceback

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

def setup_logging():
    """è®¾ç½®æ—¥å¿—"""
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    return logging.getLogger(__name__)

def test_pytorch_import():
    """æµ‹è¯•PyTorchå¯¼å…¥"""
    logger = logging.getLogger(__name__)
    logger.info("=" * 50)
    logger.info("æµ‹è¯•PyTorchå¯¼å…¥")
    logger.info("=" * 50)
    
    try:
        import torch
        logger.info(f"âœ… PyTorchç‰ˆæœ¬: {torch.__version__}")
        
        # æµ‹è¯•ç®€å•æ“ä½œ
        x = torch.randn(2, 3)
        logger.info(f"âœ… PyTorchå¼ é‡æ“ä½œæ­£å¸¸: {x.shape}")
        
        return True
        
    except Exception as e:
        logger.error(f"âŒ PyTorchå¯¼å…¥å¤±è´¥: {e}")
        return False

def test_backbone_with_pretrain():
    """æµ‹è¯•å¸¦é¢„è®­ç»ƒæƒé‡çš„Backbone"""
    logger = logging.getLogger(__name__)
    logger.info("=" * 50)
    logger.info("æµ‹è¯•å¸¦é¢„è®­ç»ƒæƒé‡çš„Backbone")
    logger.info("=" * 50)
    
    try:
        # å¼ºåˆ¶CPUæ¨¡å¼
        jt.flags.use_cuda = 0
        
        from nanodet.model.backbone import build_backbone
        
        backbone_cfg = {
            'name': 'ShuffleNetV2',
            'model_size': '1.0x',
            'out_stages': [2, 3, 4],
            'activation': 'LeakyReLU',
            'pretrain': True  # å¯ç”¨é¢„è®­ç»ƒ
        }
        
        logger.info("æ„å»ºå¸¦é¢„è®­ç»ƒæƒé‡çš„Backbone...")
        backbone = build_backbone(backbone_cfg)
        logger.info("âœ… Backboneæ„å»ºæˆåŠŸï¼ˆå¸¦é¢„è®­ç»ƒï¼‰")
        
        # æµ‹è¯•å‰å‘ä¼ æ’­
        x = jt.randn(1, 3, 320, 320)
        outputs = backbone(x)
        logger.info(f"âœ… å‰å‘ä¼ æ’­æˆåŠŸ: {[out.shape for out in outputs]}")
        
        return backbone
        
    except Exception as e:
        logger.error(f"âŒ å¸¦é¢„è®­ç»ƒBackboneæµ‹è¯•å¤±è´¥: {e}")
        traceback.print_exc()
        return None

def test_full_model_with_pretrain():
    """æµ‹è¯•å®Œæ•´æ¨¡å‹ï¼ˆå¸¦é¢„è®­ç»ƒï¼‰"""
    logger = logging.getLogger(__name__)
    logger.info("=" * 50)
    logger.info("æµ‹è¯•å®Œæ•´æ¨¡å‹ï¼ˆå¸¦é¢„è®­ç»ƒï¼‰")
    logger.info("=" * 50)
    
    try:
        from nanodet.model import build_model
        
        model_cfg = {
            'name': 'NanoDetPlus',
            'backbone': {
                'name': 'ShuffleNetV2',
                'model_size': '1.0x',
                'out_stages': [2, 3, 4],
                'activation': 'LeakyReLU',
                'pretrain': True  # å¯ç”¨é¢„è®­ç»ƒ
            },
            'fpn': {
                'name': 'GhostPAN',
                'in_channels': [116, 232, 464],
                'out_channels': 96,
                'kernel_size': 5,
                'num_extra_level': 1,
                'use_depthwise': True,
                'activation': 'LeakyReLU'
            },
            'head': {
                'name': 'NanoDetPlusHead',
                'num_classes': 20,
                'input_channel': 96,
                'feat_channels': 96,
                'stacked_convs': 2,
                'kernel_size': 5,
                'strides': [8, 16, 32, 64],
                'activation': 'LeakyReLU',
                'reg_max': 7,
                'norm_cfg': {'type': 'BN'},
                'loss': {
                    'loss_qfl': {
                        'name': 'QualityFocalLoss',
                        'use_sigmoid': True,
                        'beta': 2.0,
                        'loss_weight': 1.0
                    },
                    'loss_dfl': {
                        'name': 'DistributionFocalLoss',
                        'loss_weight': 0.25
                    },
                    'loss_bbox': {
                        'name': 'GIoULoss',
                        'loss_weight': 2.0
                    }
                }
            },
            'aux_head': {
                'name': 'SimpleConvHead',
                'num_classes': 20,
                'input_channel': 192,
                'feat_channels': 192,
                'stacked_convs': 4,
                'strides': [8, 16, 32, 64],
                'activation': 'LeakyReLU',
                'norm_cfg': {'type': 'BN'}
            },
            'detach_epoch': 10
        }
        
        logger.info("æ„å»ºå®Œæ•´æ¨¡å‹ï¼ˆå¸¦é¢„è®­ç»ƒï¼‰...")
        model = build_model(model_cfg)
        logger.info("âœ… å®Œæ•´æ¨¡å‹æ„å»ºæˆåŠŸï¼ˆå¸¦é¢„è®­ç»ƒï¼‰")
        
        # æµ‹è¯•æ¨ç†
        x = jt.randn(1, 3, 320, 320)
        model.eval()
        with jt.no_grad():
            outputs = model(x)
        logger.info(f"âœ… æ¨ç†æˆåŠŸ: {outputs.shape}")
        
        return model
        
    except Exception as e:
        logger.error(f"âŒ å®Œæ•´æ¨¡å‹æµ‹è¯•å¤±è´¥: {e}")
        traceback.print_exc()
        return None

def test_training_with_pretrain():
    """æµ‹è¯•è®­ç»ƒï¼ˆå¸¦é¢„è®­ç»ƒï¼‰"""
    logger = logging.getLogger(__name__)
    logger.info("=" * 50)
    logger.info("æµ‹è¯•è®­ç»ƒï¼ˆå¸¦é¢„è®­ç»ƒï¼‰")
    logger.info("=" * 50)
    
    try:
        model = test_full_model_with_pretrain()
        if model is None:
            return False
        
        model.train()
        
        # åˆ›å»ºä¼˜åŒ–å™¨
        optimizer = jt.optim.SGD(model.parameters(), lr=0.01, momentum=0.9, weight_decay=0.0001)
        
        # åˆ›å»ºæ¨¡æ‹Ÿæ•°æ®
        batch_size = 2
        images = jt.randn(batch_size, 3, 320, 320)
        gt_meta = {
            'img': images,
            'gt_bboxes': [jt.randn(3, 4) * 100 + 50 for _ in range(batch_size)],
            'gt_labels': [jt.randint(0, 20, (3,)) for _ in range(batch_size)],
            'img_info': [
                {'height': 320, 'width': 320, 'id': i} for i in range(batch_size)
            ]
        }
        
        # è®­ç»ƒå‡ ä¸ªæ­¥éª¤
        logger.info("å¼€å§‹è®­ç»ƒæµ‹è¯•ï¼ˆå¸¦é¢„è®­ç»ƒï¼‰...")
        for step in range(3):
            # å‰å‘ä¼ æ’­
            head_out, loss, loss_states = model.forward_train(gt_meta)
            
            # åå‘ä¼ æ’­
            optimizer.zero_grad()
            optimizer.backward(loss)
            optimizer.step()
            
            logger.info(f"Step {step + 1}: loss = {loss.item():.4f}")
            
            if loss_states:
                for key, value in loss_states.items():
                    if hasattr(value, 'item'):
                        logger.info(f"  {key}: {value.item():.4f}")
        
        logger.info("âœ… è®­ç»ƒæµ‹è¯•æˆåŠŸï¼ˆå¸¦é¢„è®­ç»ƒï¼‰")
        return True
        
    except Exception as e:
        logger.error(f"âŒ è®­ç»ƒæµ‹è¯•å¤±è´¥: {e}")
        traceback.print_exc()
        return False

def main():
    """ä¸»å‡½æ•°"""
    # å¼ºåˆ¶CPUæ¨¡å¼
    jt.flags.use_cuda = 0
    
    logger = setup_logging()
    logger.info("å¼€å§‹é¢„è®­ç»ƒæƒé‡åŠ è½½æµ‹è¯•...")
    
    # æµ‹è¯•1: PyTorchå¯¼å…¥
    pytorch_ok = test_pytorch_import()
    if not pytorch_ok:
        logger.error("PyTorchå¯¼å…¥å¤±è´¥ï¼Œæ— æ³•åŠ è½½é¢„è®­ç»ƒæƒé‡")
        return False
    
    # æµ‹è¯•2: Backboneé¢„è®­ç»ƒ
    backbone = test_backbone_with_pretrain()
    if backbone is None:
        logger.error("Backboneé¢„è®­ç»ƒåŠ è½½å¤±è´¥")
        return False
    
    # æµ‹è¯•3: å®Œæ•´æ¨¡å‹é¢„è®­ç»ƒ
    model = test_full_model_with_pretrain()
    if model is None:
        logger.error("å®Œæ•´æ¨¡å‹é¢„è®­ç»ƒåŠ è½½å¤±è´¥")
        return False
    
    # æµ‹è¯•4: è®­ç»ƒ
    training_ok = test_training_with_pretrain()
    if not training_ok:
        logger.error("é¢„è®­ç»ƒæ¨¡å‹è®­ç»ƒå¤±è´¥")
        return False
    
    logger.info("ğŸ‰ æ‰€æœ‰é¢„è®­ç»ƒæƒé‡æµ‹è¯•é€šè¿‡ï¼")
    return True

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)
