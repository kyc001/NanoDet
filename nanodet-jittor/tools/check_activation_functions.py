#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
æ£€æŸ¥æ¿€æ´»å‡½æ•°çš„å®ç°å·®å¼‚
ç‰¹åˆ«æ˜¯LeakyReLUçš„å‚æ•°å’Œè¡Œä¸º
"""

import torch
import jittor as jt
import numpy as np


def test_leaky_relu():
    """æµ‹è¯•LeakyReLUçš„è¡Œä¸ºå·®å¼‚"""
    print("ğŸ” æµ‹è¯•LeakyReLUè¡Œä¸ºå·®å¼‚")
    print("=" * 50)
    
    # åˆ›å»ºæµ‹è¯•æ•°æ®
    test_data = np.array([-2.0, -1.0, -0.5, 0.0, 0.5, 1.0, 2.0], dtype=np.float32)
    
    print(f"æµ‹è¯•æ•°æ®: {test_data}")
    
    # PyTorch LeakyReLU
    torch_data = torch.from_numpy(test_data)
    torch_leaky_relu = torch.nn.LeakyReLU()
    torch_result = torch_leaky_relu(torch_data)
    
    print(f"PyTorch LeakyReLUç»“æœ: {torch_result.numpy()}")
    print(f"PyTorch LeakyReLUè´Ÿæ–œç‡: {torch_leaky_relu.negative_slope}")
    
    # Jittor LeakyReLU
    jittor_data = jt.array(test_data)
    jittor_leaky_relu = jt.nn.LeakyReLU()
    jittor_result = jittor_leaky_relu(jittor_data)
    
    print(f"Jittor LeakyReLUç»“æœ: {jittor_result.numpy()}")
    
    # æ£€æŸ¥Jittor LeakyReLUçš„å‚æ•°
    if hasattr(jittor_leaky_relu, 'negative_slope'):
        print(f"Jittor LeakyReLUè´Ÿæ–œç‡: {jittor_leaky_relu.negative_slope}")
    else:
        print("Jittor LeakyReLUæ²¡æœ‰negative_slopeå±æ€§")
    
    # è®¡ç®—å·®å¼‚
    diff = np.abs(torch_result.numpy() - jittor_result.numpy())
    print(f"å·®å¼‚: {diff}")
    print(f"æœ€å¤§å·®å¼‚: {diff.max()}")
    
    if diff.max() < 1e-6:
        print("âœ… LeakyReLUè¡Œä¸ºä¸€è‡´")
    else:
        print("âŒ LeakyReLUè¡Œä¸ºä¸ä¸€è‡´")
    
    return diff.max() < 1e-6


def test_batch_norm():
    """æµ‹è¯•BatchNormçš„è¡Œä¸ºå·®å¼‚"""
    print("\nğŸ” æµ‹è¯•BatchNormè¡Œä¸ºå·®å¼‚")
    print("=" * 50)
    
    # åˆ›å»ºæµ‹è¯•æ•°æ®
    test_data = np.random.randn(2, 64, 8, 8).astype(np.float32)
    
    print(f"æµ‹è¯•æ•°æ®å½¢çŠ¶: {test_data.shape}")
    print(f"æµ‹è¯•æ•°æ®èŒƒå›´: [{test_data.min():.6f}, {test_data.max():.6f}]")
    
    # PyTorch BatchNorm
    torch_data = torch.from_numpy(test_data)
    torch_bn = torch.nn.BatchNorm2d(64)
    torch_bn.eval()  # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
    torch_result = torch_bn(torch_data)
    
    print(f"PyTorch BatchNormç»“æœèŒƒå›´: [{torch_result.min():.6f}, {torch_result.max():.6f}]")
    
    # Jittor BatchNorm
    jittor_data = jt.array(test_data)
    jittor_bn = jt.nn.BatchNorm2d(64)
    jittor_bn.eval()  # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
    jittor_result = jittor_bn(jittor_data)
    
    print(f"Jittor BatchNormç»“æœèŒƒå›´: [{jittor_result.min():.6f}, {jittor_result.max():.6f}]")
    
    # è®¡ç®—å·®å¼‚
    diff = np.abs(torch_result.detach().numpy() - jittor_result.numpy())
    print(f"æœ€å¤§å·®å¼‚: {diff.max():.6f}")
    print(f"å¹³å‡å·®å¼‚: {diff.mean():.6f}")
    
    if diff.max() < 1e-4:
        print("âœ… BatchNormè¡Œä¸ºåŸºæœ¬ä¸€è‡´")
        return True
    else:
        print("âŒ BatchNormè¡Œä¸ºå·®å¼‚è¾ƒå¤§")
        return False


def test_conv2d():
    """æµ‹è¯•Conv2dçš„è¡Œä¸ºå·®å¼‚"""
    print("\nğŸ” æµ‹è¯•Conv2dè¡Œä¸ºå·®å¼‚")
    print("=" * 50)
    
    # åˆ›å»ºæµ‹è¯•æ•°æ®
    test_data = np.random.randn(1, 3, 32, 32).astype(np.float32)
    
    print(f"æµ‹è¯•æ•°æ®å½¢çŠ¶: {test_data.shape}")
    print(f"æµ‹è¯•æ•°æ®èŒƒå›´: [{test_data.min():.6f}, {test_data.max():.6f}]")
    
    # åˆ›å»ºç›¸åŒçš„æƒé‡
    weight = np.random.randn(64, 3, 3, 3).astype(np.float32)
    bias = np.random.randn(64).astype(np.float32)
    
    # PyTorch Conv2d
    torch_data = torch.from_numpy(test_data)
    torch_conv = torch.nn.Conv2d(3, 64, 3, padding=1)
    torch_conv.weight.data = torch.from_numpy(weight)
    torch_conv.bias.data = torch.from_numpy(bias)
    torch_result = torch_conv(torch_data)
    
    print(f"PyTorch Conv2dç»“æœå½¢çŠ¶: {torch_result.shape}")
    print(f"PyTorch Conv2dç»“æœèŒƒå›´: [{torch_result.min():.6f}, {torch_result.max():.6f}]")
    
    # Jittor Conv2d
    jittor_data = jt.array(test_data)
    jittor_conv = jt.nn.Conv2d(3, 64, 3, padding=1)
    jittor_conv.weight.assign(jt.array(weight))
    jittor_conv.bias.assign(jt.array(bias))
    jittor_result = jittor_conv(jittor_data)
    
    print(f"Jittor Conv2dç»“æœå½¢çŠ¶: {jittor_result.shape}")
    print(f"Jittor Conv2dç»“æœèŒƒå›´: [{jittor_result.min():.6f}, {jittor_result.max():.6f}]")
    
    # è®¡ç®—å·®å¼‚
    diff = np.abs(torch_result.detach().numpy() - jittor_result.numpy())
    print(f"æœ€å¤§å·®å¼‚: {diff.max():.6f}")
    print(f"å¹³å‡å·®å¼‚: {diff.mean():.6f}")
    
    if diff.max() < 1e-4:
        print("âœ… Conv2dè¡Œä¸ºåŸºæœ¬ä¸€è‡´")
        return True
    else:
        print("âŒ Conv2dè¡Œä¸ºå·®å¼‚è¾ƒå¤§")
        return False


def test_combined_operations():
    """æµ‹è¯•ç»„åˆæ“ä½œçš„è¡Œä¸ºå·®å¼‚"""
    print("\nğŸ” æµ‹è¯•ç»„åˆæ“ä½œè¡Œä¸ºå·®å¼‚")
    print("=" * 50)
    
    # åˆ›å»ºæµ‹è¯•æ•°æ®
    test_data = np.random.randn(1, 64, 16, 16).astype(np.float32)
    
    print(f"æµ‹è¯•æ•°æ®å½¢çŠ¶: {test_data.shape}")
    print(f"æµ‹è¯•æ•°æ®èŒƒå›´: [{test_data.min():.6f}, {test_data.max():.6f}]")
    
    # åˆ›å»ºç›¸åŒçš„æƒé‡
    weight = np.random.randn(64, 64, 3, 3).astype(np.float32)
    bias = np.random.randn(64).astype(np.float32)
    
    # PyTorchç»„åˆæ“ä½œ: Conv2d + BatchNorm + LeakyReLU
    torch_data = torch.from_numpy(test_data)
    
    torch_conv = torch.nn.Conv2d(64, 64, 3, padding=1)
    torch_conv.weight.data = torch.from_numpy(weight)
    torch_conv.bias.data = torch.from_numpy(bias)
    
    torch_bn = torch.nn.BatchNorm2d(64)
    torch_bn.eval()
    
    torch_leaky = torch.nn.LeakyReLU()
    
    # PyTorchå‰å‘ä¼ æ’­
    torch_x = torch_conv(torch_data)
    torch_x = torch_bn(torch_x)
    torch_result = torch_leaky(torch_x)
    
    print(f"PyTorchç»„åˆæ“ä½œç»“æœèŒƒå›´: [{torch_result.min():.6f}, {torch_result.max():.6f}]")
    
    # Jittorç»„åˆæ“ä½œ
    jittor_data = jt.array(test_data)
    
    jittor_conv = jt.nn.Conv2d(64, 64, 3, padding=1)
    jittor_conv.weight.assign(jt.array(weight))
    jittor_conv.bias.assign(jt.array(bias))
    
    jittor_bn = jt.nn.BatchNorm2d(64)
    jittor_bn.eval()
    
    jittor_leaky = jt.nn.LeakyReLU()
    
    # Jittorå‰å‘ä¼ æ’­
    jittor_x = jittor_conv(jittor_data)
    jittor_x = jittor_bn(jittor_x)
    jittor_result = jittor_leaky(jittor_x)
    
    print(f"Jittorç»„åˆæ“ä½œç»“æœèŒƒå›´: [{jittor_result.min():.6f}, {jittor_result.max():.6f}]")
    
    # è®¡ç®—å·®å¼‚
    diff = np.abs(torch_result.detach().numpy() - jittor_result.numpy())
    print(f"æœ€å¤§å·®å¼‚: {diff.max():.6f}")
    print(f"å¹³å‡å·®å¼‚: {diff.mean():.6f}")
    
    if diff.max() < 1e-3:
        print("âœ… ç»„åˆæ“ä½œè¡Œä¸ºåŸºæœ¬ä¸€è‡´")
        return True
    else:
        print("âŒ ç»„åˆæ“ä½œè¡Œä¸ºå·®å¼‚è¾ƒå¤§")
        return False


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ å¼€å§‹æ£€æŸ¥æ¿€æ´»å‡½æ•°å’ŒåŸºç¡€æ“ä½œå·®å¼‚")
    
    # æµ‹è¯•å„ä¸ªç»„ä»¶
    leaky_relu_ok = test_leaky_relu()
    batch_norm_ok = test_batch_norm()
    conv2d_ok = test_conv2d()
    combined_ok = test_combined_operations()
    
    print(f"\nğŸ“Š æµ‹è¯•æ€»ç»“:")
    print(f"  LeakyReLU: {'âœ…' if leaky_relu_ok else 'âŒ'}")
    print(f"  BatchNorm: {'âœ…' if batch_norm_ok else 'âŒ'}")
    print(f"  Conv2d: {'âœ…' if conv2d_ok else 'âŒ'}")
    print(f"  ç»„åˆæ“ä½œ: {'âœ…' if combined_ok else 'âŒ'}")
    
    if all([leaky_relu_ok, batch_norm_ok, conv2d_ok, combined_ok]):
        print(f"\nâœ… æ‰€æœ‰åŸºç¡€æ“ä½œéƒ½ä¸€è‡´ï¼Œé—®é¢˜å¯èƒ½åœ¨æ›´é«˜å±‚çš„å®ç°")
    else:
        print(f"\nâŒ å‘ç°åŸºç¡€æ“ä½œå·®å¼‚ï¼Œè¿™å¯èƒ½æ˜¯é—®é¢˜æ ¹æº")
    
    print(f"\nâœ… æ£€æŸ¥å®Œæˆ")


if __name__ == '__main__':
    main()
