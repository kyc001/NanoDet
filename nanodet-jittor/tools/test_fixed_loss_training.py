#!/usr/bin/env python3
"""
æµ‹è¯•ä¿®å¤åçš„æŸå¤±å‡½æ•°è®­ç»ƒ
éªŒè¯æŸå¤±å‡½æ•°æ˜¯å¦æ­£ç¡®å·¥ä½œ
"""

import os
import sys
import logging
import jittor as jt
from pathlib import Path
import time
import json

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

def setup_logging():
    """è®¾ç½®æ—¥å¿—"""
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    return logging.getLogger(__name__)

def create_model_with_pretrain():
    """åˆ›å»ºæ¨¡å‹ï¼ˆå¸¦é¢„è®­ç»ƒæƒé‡ï¼‰"""
    from nanodet.model import build_model
    
    model_cfg = {
        'name': 'NanoDetPlus',
        'backbone': {
            'name': 'ShuffleNetV2',
            'model_size': '1.0x',
            'out_stages': [2, 3, 4],
            'activation': 'LeakyReLU',
            'pretrain': True  # å¯ç”¨é¢„è®­ç»ƒ
        },
        'fpn': {
            'name': 'GhostPAN',
            'in_channels': [116, 232, 464],
            'out_channels': 96,
            'kernel_size': 5,
            'num_extra_level': 1,
            'use_depthwise': True,
            'activation': 'LeakyReLU'
        },
        'head': {
            'name': 'NanoDetPlusHead',
            'num_classes': 20,
            'input_channel': 96,
            'feat_channels': 96,
            'stacked_convs': 2,
            'kernel_size': 5,
            'strides': [8, 16, 32, 64],
            'activation': 'LeakyReLU',
            'reg_max': 7,
            'norm_cfg': {'type': 'BN'},
            'loss': {
                'loss_qfl': {
                    'name': 'QualityFocalLoss',
                    'use_sigmoid': True,
                    'beta': 2.0,
                    'loss_weight': 1.0
                },
                'loss_dfl': {
                    'name': 'DistributionFocalLoss',
                    'loss_weight': 0.25
                },
                'loss_bbox': {
                    'name': 'GIoULoss',
                    'loss_weight': 2.0
                }
            }
        },
        'aux_head': {
            'name': 'SimpleConvHead',
            'num_classes': 20,
            'input_channel': 192,
            'feat_channels': 192,
            'stacked_convs': 4,
            'strides': [8, 16, 32, 64],
            'activation': 'LeakyReLU',
            'norm_cfg': {'type': 'BN'}
        },
        'detach_epoch': 10
    }
    
    return build_model(model_cfg)

def create_simple_voc_dataloader():
    """åˆ›å»ºç®€åŒ–çš„VOCæ•°æ®åŠ è½½å™¨"""
    logger = logging.getLogger(__name__)
    
    try:
        # è¯»å–æ ‡æ³¨æ–‡ä»¶
        data_root = project_root / "data"
        train_ann_file = data_root / "annotations" / "voc_train.json"
        
        with open(train_ann_file, 'r') as f:
            coco_data = json.load(f)
        
        images = coco_data['images']
        annotations = coco_data['annotations']
        
        # æ„å»ºå›¾åƒIDåˆ°æ ‡æ³¨çš„æ˜ å°„
        img_id_to_anns = {}
        for ann in annotations:
            img_id = ann['image_id']
            if img_id not in img_id_to_anns:
                img_id_to_anns[img_id] = []
            img_id_to_anns[img_id].append(ann)
        
        logger.info(f"åŠ è½½äº†{len(images)}å¼ å›¾åƒï¼Œ{len(annotations)}ä¸ªæ ‡æ³¨")
        
        # ç®€åŒ–çš„æ•°æ®ç”Ÿæˆå™¨
        def data_generator():
            import cv2
            import numpy as np
            
            img_dir = data_root / "VOCdevkit" / "VOC2007" / "JPEGImages"
            batch_size = 4
            batch_images = []
            batch_bboxes = []
            batch_labels = []
            batch_info = []
            
            for i, img_info in enumerate(images[:100]):  # ä½¿ç”¨å‰100å¼ å›¾åƒ
                img_path = img_dir / img_info['file_name']
                
                if not img_path.exists():
                    continue
                
                # è¯»å–å›¾åƒ
                img = cv2.imread(str(img_path))
                if img is None:
                    continue
                
                # è°ƒæ•´å¤§å°åˆ°320x320
                img = cv2.resize(img, (320, 320))
                img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
                img = img.astype(np.float32) / 255.0
                img = np.transpose(img, (2, 0, 1))  # HWC -> CHW
                
                # è·å–æ ‡æ³¨
                img_id = img_info['id']
                anns = img_id_to_anns.get(img_id, [])
                
                if len(anns) == 0:
                    continue
                
                # å¤„ç†bboxå’Œæ ‡ç­¾
                bboxes = []
                labels = []
                for ann in anns:
                    bbox = ann['bbox']  # [x, y, w, h]
                    # è½¬æ¢ä¸º[x1, y1, x2, y2]å¹¶ç¼©æ”¾åˆ°320x320
                    x1 = bbox[0] * 320 / img_info['width']
                    y1 = bbox[1] * 320 / img_info['height']
                    x2 = (bbox[0] + bbox[2]) * 320 / img_info['width']
                    y2 = (bbox[1] + bbox[3]) * 320 / img_info['height']
                    
                    bboxes.append([x1, y1, x2, y2])
                    labels.append(ann['category_id'] - 1)  # COCOç±»åˆ«IDä»1å¼€å§‹ï¼Œè½¬æ¢ä¸º0å¼€å§‹
                
                if len(bboxes) == 0:
                    continue
                
                batch_images.append(jt.array(img))
                batch_bboxes.append(jt.array(bboxes))
                batch_labels.append(jt.array(labels))
                batch_info.append({
                    'height': 320,
                    'width': 320,
                    'id': img_id
                })
                
                if len(batch_images) == batch_size:
                    # è¿”å›ä¸€ä¸ªbatch
                    gt_meta = {
                        'img': jt.stack(batch_images),
                        'gt_bboxes': batch_bboxes,
                        'gt_labels': batch_labels,
                        'img_info': batch_info
                    }
                    
                    yield gt_meta
                    
                    # é‡ç½®batch
                    batch_images = []
                    batch_bboxes = []
                    batch_labels = []
                    batch_info = []
        
        return data_generator()
        
    except Exception as e:
        logger.error(f"æ•°æ®åŠ è½½å™¨åˆ›å»ºå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return None

def test_fixed_loss_training():
    """æµ‹è¯•ä¿®å¤åçš„æŸå¤±å‡½æ•°è®­ç»ƒ"""
    logger = logging.getLogger(__name__)
    logger.info("=" * 50)
    logger.info("æµ‹è¯•ä¿®å¤åçš„æŸå¤±å‡½æ•°è®­ç»ƒ")
    logger.info("=" * 50)
    
    try:
        # å¼ºåˆ¶CPUæ¨¡å¼
        jt.flags.use_cuda = 0
        logger.info("ä½¿ç”¨CPUæ¨¡å¼")
        
        # åˆ›å»ºæ¨¡å‹
        logger.info("åˆ›å»ºæ¨¡å‹ï¼ˆå¸¦é¢„è®­ç»ƒæƒé‡ï¼‰...")
        model = create_model_with_pretrain()
        model.train()
        logger.info("âœ… æ¨¡å‹åˆ›å»ºæˆåŠŸ")
        
        # åˆ›å»ºæ•°æ®åŠ è½½å™¨
        logger.info("åˆ›å»ºçœŸå®æ•°æ®åŠ è½½å™¨...")
        dataloader = create_simple_voc_dataloader()
        if dataloader is None:
            logger.error("âŒ æ•°æ®åŠ è½½å™¨åˆ›å»ºå¤±è´¥")
            return False
        
        # åˆ›å»ºä¼˜åŒ–å™¨
        optimizer = jt.optim.SGD(model.parameters(), lr=0.01, momentum=0.9, weight_decay=0.0001)
        logger.info("âœ… ä¼˜åŒ–å™¨åˆ›å»ºæˆåŠŸ")
        
        # è®­ç»ƒå¾ªç¯
        num_epochs = 5
        loss_history = []
        
        logger.info(f"å¼€å§‹è®­ç»ƒ {num_epochs} ä¸ªepoch...")
        
        for epoch in range(num_epochs):
            logger.info(f"\n--- Epoch {epoch + 1}/{num_epochs} ---")
            
            # è®­ç»ƒé˜¶æ®µ
            model.train()
            epoch_loss = 0
            num_batches = 0
            
            start_time = time.time()
            
            batch_count = 0
            for gt_meta in dataloader:
                if batch_count >= 10:  # æ¯ä¸ªepochè®­ç»ƒ10ä¸ªbatch
                    break
                
                try:
                    # å‰å‘ä¼ æ’­
                    head_out, loss, loss_states = model.forward_train(gt_meta)
                    
                    # åå‘ä¼ æ’­
                    optimizer.zero_grad()
                    optimizer.backward(loss)
                    optimizer.step()
                    
                    epoch_loss += loss.item()
                    num_batches += 1
                    
                    if batch_count % 3 == 0:
                        logger.info(f"  Batch {batch_count}: loss = {loss.item():.4f}")
                        
                        # æ‰“å°è¯¦ç»†æŸå¤±
                        if loss_states:
                            for key, value in loss_states.items():
                                if hasattr(value, 'item'):
                                    logger.info(f"    {key}: {value.item():.4f}")
                                else:
                                    logger.info(f"    {key}: {value:.4f}")
                    
                    batch_count += 1
                
                except Exception as e:
                    logger.warning(f"è®­ç»ƒbatch {batch_count}å¤±è´¥: {e}")
                    import traceback
                    traceback.print_exc()
                    batch_count += 1
                    continue
            
            train_time = time.time() - start_time
            avg_loss = epoch_loss / max(num_batches, 1)
            loss_history.append(avg_loss)
            
            logger.info(f"  è®­ç»ƒå®Œæˆ: å¹³å‡æŸå¤± = {avg_loss:.4f}, æ—¶é—´ = {train_time:.1f}s")
            
            # æ£€æŸ¥æŸå¤±è¶‹åŠ¿
            if len(loss_history) > 1:
                loss_change = avg_loss - loss_history[-2]
                loss_trend = "â†“" if loss_change < 0 else "â†‘" if loss_change > 0 else "â†’"
                logger.info(f"  æŸå¤±å˜åŒ–: {loss_change:+.4f} {loss_trend}")
        
        # æ€»ç»“ç»“æœ
        logger.info("\n" + "=" * 50)
        logger.info("ä¿®å¤åæŸå¤±å‡½æ•°è®­ç»ƒå®Œæˆ")
        logger.info("=" * 50)
        
        logger.info("è®­ç»ƒå†å²:")
        for i, loss_val in enumerate(loss_history):
            logger.info(f"  Epoch {i+1}: æŸå¤±={loss_val:.4f}")
        
        # æ£€æŸ¥å­¦ä¹ æ•ˆæœ
        if len(loss_history) >= 2:
            final_loss_improvement = loss_history[0] - loss_history[-1]
            
            logger.info(f"\næ€»ä½“æ”¹è¿›:")
            logger.info(f"  æŸå¤±æ”¹è¿›: {final_loss_improvement:+.4f}")
            
            if final_loss_improvement > 0:
                logger.info("âœ… æŸå¤±ä¸‹é™ï¼Œæ¨¡å‹æ­£åœ¨å­¦ä¹ ï¼")
                logger.info("ğŸ‰ ä¿®å¤åæŸå¤±å‡½æ•°è®­ç»ƒéªŒè¯æˆåŠŸï¼")
                return True
            else:
                logger.warning("âš ï¸ æŸå¤±æ²¡æœ‰æ˜æ˜¾ä¸‹é™")
                logger.info("âœ… ä½†è®­ç»ƒæµç¨‹æ­£å¸¸å®Œæˆ")
                return True
        else:
            logger.info("âœ… è®­ç»ƒæµç¨‹æ­£å¸¸å®Œæˆ")
            return True
        
    except Exception as e:
        logger.error(f"âŒ ä¿®å¤åæŸå¤±å‡½æ•°è®­ç»ƒå¤±è´¥: {str(e)}")
        import traceback
        traceback.print_exc()
        return False

def main():
    """ä¸»å‡½æ•°"""
    logger = setup_logging()
    logger.info("å¼€å§‹æµ‹è¯•ä¿®å¤åçš„æŸå¤±å‡½æ•°è®­ç»ƒ...")
    
    success = test_fixed_loss_training()
    
    if success:
        logger.info("ğŸ‰ ä¿®å¤åæŸå¤±å‡½æ•°è®­ç»ƒéªŒè¯æˆåŠŸï¼")
        logger.info("âœ… æŸå¤±å‡½æ•°ä¿®å¤å®Œæˆï¼")
        return True
    else:
        logger.error("âŒ ä¿®å¤åæŸå¤±å‡½æ•°è®­ç»ƒéªŒè¯å¤±è´¥ï¼")
        return False

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)
