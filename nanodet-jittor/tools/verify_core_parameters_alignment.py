#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
éªŒè¯æ ¸å¿ƒå‚æ•°100%å¯¹é½
æ’é™¤æ¡†æ¶å·®å¼‚å‚æ•°ï¼šBatchNormç»Ÿè®¡å‚æ•°å’ŒScaleå½¢çŠ¶å·®å¼‚
ä¸“æ³¨äºéªŒè¯æ¨¡å‹æ ¸å¿ƒé€»è¾‘çš„å‚æ•°å¯¹é½
"""

import json
import sys
from collections import defaultdict

# æ·»åŠ è·¯å¾„
sys.path.append('/home/kyc/project/nanodet/nanodet-jittor')
from nanodet.model.arch.nanodet_plus import NanoDetPlus


def load_pytorch_record():
    """åŠ è½½PyTorchå‚è€ƒè®°å½•"""
    try:
        with open('pytorch_reference_record.json', 'r') as f:
            record = json.load(f)
        return record
    except FileNotFoundError:
        print("âŒ PyTorchå‚è€ƒè®°å½•æ–‡ä»¶ä¸å­˜åœ¨")
        return None


def create_jittor_model():
    """åˆ›å»ºJittoræ¨¡å‹"""
    print("åˆ›å»ºJittoræ¨¡å‹...")
    
    # åˆ›å»ºé…ç½®å­—å…¸ - å®Œå…¨å¯¹é½PyTorchç‰ˆæœ¬
    backbone_cfg = {
        'name': 'ShuffleNetV2',
        'model_size': '1.0x',
        'out_stages': [2, 3, 4],
        'activation': 'LeakyReLU',
        'pretrain': True
    }
    
    fpn_cfg = {
        'name': 'GhostPAN',
        'in_channels': [116, 232, 464],
        'out_channels': 96,
        'kernel_size': 5,
        'num_extra_level': 1,
        'use_depthwise': True,
        'activation': 'LeakyReLU'
    }
    
    head_cfg = {
        'name': 'NanoDetPlusHead',
        'num_classes': 20,
        'input_channel': 96,
        'feat_channels': 96,
        'stacked_convs': 2,
        'kernel_size': 5,
        'strides': [8, 16, 32, 64],
        'activation': 'LeakyReLU',
        'reg_max': 7,
        'norm_cfg': {'type': 'BN'},
        'loss': {
            'loss_qfl': {
                'name': 'QualityFocalLoss',
                'use_sigmoid': True,
                'beta': 2.0,
                'loss_weight': 1.0
            },
            'loss_dfl': {
                'name': 'DistributionFocalLoss',
                'loss_weight': 0.25
            },
            'loss_bbox': {
                'name': 'GIoULoss',
                'loss_weight': 2.0
            }
        }
    }
    
    # åˆ›å»ºaux_headé…ç½® - å®Œå…¨å¯¹é½PyTorchç‰ˆæœ¬
    aux_head_cfg = {
        'name': 'SimpleConvHead',
        'num_classes': 20,
        'input_channel': 192,
        'feat_channels': 192,  # ä¸PyTorchç‰ˆæœ¬ä¸€è‡´
        'stacked_convs': 4,    # ä¸PyTorchç‰ˆæœ¬ä¸€è‡´
        'strides': [8, 16, 32, 64],
        'activation': 'LeakyReLU',
        'reg_max': 7
    }
    
    # åˆ›å»ºå®Œæ•´æ¨¡å‹
    model = NanoDetPlus(backbone_cfg, fpn_cfg, aux_head_cfg, head_cfg)
    
    return model


def filter_core_parameters(params_dict, framework="pytorch"):
    """è¿‡æ»¤å‡ºæ ¸å¿ƒå‚æ•°ï¼Œæ’é™¤æ¡†æ¶å·®å¼‚å‚æ•°"""
    core_params = {}
    
    for name, details in params_dict.items():
        # æ’é™¤BatchNormç»Ÿè®¡å‚æ•°
        if 'running_mean' in name or 'running_var' in name or 'num_batches_tracked' in name:
            continue
        
        # æ’é™¤éå¯è®­ç»ƒå‚æ•°
        if framework == "pytorch":
            if 'requires_grad' in details and not details['requires_grad']:
                continue
        
        # å¯¹äºScaleå‚æ•°ï¼Œæˆ‘ä»¬çŸ¥é“æœ‰å½¢çŠ¶å·®å¼‚ï¼Œä½†å‚æ•°æ•°é‡ç›¸åŒ
        # æ‰€ä»¥æˆ‘ä»¬ä¿ç•™å®ƒä»¬ï¼Œä½†åœ¨å¯¹æ¯”æ—¶ç‰¹æ®Šå¤„ç†
        core_params[name] = details
    
    return core_params


def normalize_parameter_shapes(pytorch_params, jittor_params):
    """æ ‡å‡†åŒ–å‚æ•°å½¢çŠ¶ï¼Œå¤„ç†å·²çŸ¥çš„æ¡†æ¶å·®å¼‚"""
    normalized_pytorch = {}
    normalized_jittor = {}
    
    shape_adjustments = 0
    
    for name in pytorch_params.keys():
        if name in jittor_params:
            pytorch_shape = pytorch_params[name]['shape']
            jittor_shape = jittor_params[name]['shape']
            
            # å¤„ç†Scaleå‚æ•°çš„å½¢çŠ¶å·®å¼‚ï¼šPyTorch [] vs Jittor [1]
            if 'scale' in name and pytorch_shape == [] and jittor_shape == [1]:
                # æ ‡å‡†åŒ–ä¸ºç›¸åŒå½¢çŠ¶è¿›è¡Œå¯¹æ¯”
                normalized_pytorch[name] = {
                    **pytorch_params[name],
                    'shape': [1],  # æ ‡å‡†åŒ–ä¸º1ç»´
                    'original_shape': pytorch_shape
                }
                normalized_jittor[name] = {
                    **jittor_params[name],
                    'original_shape': jittor_shape
                }
                shape_adjustments += 1
            else:
                normalized_pytorch[name] = pytorch_params[name]
                normalized_jittor[name] = jittor_params[name]
    
    return normalized_pytorch, normalized_jittor, shape_adjustments


def verify_core_parameters_alignment():
    """éªŒè¯æ ¸å¿ƒå‚æ•°100%å¯¹é½"""
    print("ğŸ” éªŒè¯æ ¸å¿ƒå‚æ•°100%å¯¹é½")
    print("æ’é™¤æ¡†æ¶å·®å¼‚ï¼šBatchNormç»Ÿè®¡å‚æ•° + Scaleå½¢çŠ¶å·®å¼‚")
    print("=" * 80)
    
    # åŠ è½½PyTorchè®°å½•
    pytorch_record = load_pytorch_record()
    if not pytorch_record:
        return False
    
    pytorch_params = pytorch_record['model_params']
    
    # è¿‡æ»¤PyTorchæ ¸å¿ƒå‚æ•°
    pytorch_core = filter_core_parameters(pytorch_params, "pytorch")
    
    # ç»Ÿè®¡PyTorchæ ¸å¿ƒå‚æ•°
    pytorch_total = sum(details['count'] for details in pytorch_core.values())
    pytorch_modules = defaultdict(int)
    
    for name, details in pytorch_core.items():
        count = details['count']
        module = name.split('.')[0]
        pytorch_modules[module] += count
    
    print(f"âœ“ PyTorchæ ¸å¿ƒå‚æ•°: {pytorch_total:,} å‚æ•°, {len(pytorch_core)} é¡¹")
    
    # åˆ›å»ºJittoræ¨¡å‹
    jittor_model = create_jittor_model()
    
    # ç»Ÿè®¡Jittoræ ¸å¿ƒå‚æ•°
    jittor_params = {}
    jittor_total = 0
    jittor_modules = defaultdict(int)
    
    for name, param in jittor_model.named_parameters():
        # æ’é™¤BatchNormç»Ÿè®¡å‚æ•°
        if 'running_mean' in name or 'running_var' in name or 'num_batches_tracked' in name:
            continue
        
        count = param.numel() if hasattr(param, 'numel') else param.size
        jittor_params[name] = {
            'shape': list(param.shape),
            'count': count
        }
        jittor_total += count
        
        module = name.split('.')[0]
        jittor_modules[module] += count
    
    # è¿‡æ»¤Jittoræ ¸å¿ƒå‚æ•°
    jittor_core = filter_core_parameters(jittor_params, "jittor")
    jittor_core_total = sum(details['count'] for details in jittor_core.values())
    
    print(f"âœ“ Jittoræ ¸å¿ƒå‚æ•°: {jittor_core_total:,} å‚æ•°, {len(jittor_core)} é¡¹")
    
    # æ ‡å‡†åŒ–å‚æ•°å½¢çŠ¶
    norm_pytorch, norm_jittor, shape_adjustments = normalize_parameter_shapes(pytorch_core, jittor_core)
    
    print(f"âœ“ å½¢çŠ¶æ ‡å‡†åŒ–è°ƒæ•´: {shape_adjustments} ä¸ªå‚æ•°")
    
    # è®¡ç®—å·®å¼‚
    difference = abs(pytorch_total - jittor_core_total)
    print(f"\nğŸ“Š æ ¸å¿ƒå‚æ•°å·®å¼‚: {difference:,} ({difference/pytorch_total*100:.6f}%)")
    
    if difference == 0:
        print("ğŸ‰ æ ¸å¿ƒå‚æ•°æ•°é‡100%å¯¹é½ï¼")
        
        # è¿›ä¸€æ­¥éªŒè¯å‚æ•°åå’Œå½¢çŠ¶
        print(f"\nğŸ” éªŒè¯å‚æ•°åå’Œæ ‡å‡†åŒ–å½¢çŠ¶å¯¹é½:")
        
        pytorch_names = set(norm_pytorch.keys())
        jittor_names = set(norm_jittor.keys())
        
        common = pytorch_names & jittor_names
        only_pytorch = pytorch_names - jittor_names
        only_jittor = jittor_names - pytorch_names
        
        print(f"  å…±åŒå‚æ•°: {len(common)}")
        print(f"  åªåœ¨PyTorchä¸­: {len(only_pytorch)}")
        print(f"  åªåœ¨Jittorä¸­: {len(only_jittor)}")
        
        if len(only_pytorch) == 0 and len(only_jittor) == 0:
            print("âœ… å‚æ•°å100%å¯¹é½ï¼")
            
            # æ£€æŸ¥æ ‡å‡†åŒ–åçš„å½¢çŠ¶å¯¹é½
            shape_mismatches = 0
            for name in common:
                pytorch_shape = norm_pytorch[name]['shape']
                jittor_shape = norm_jittor[name]['shape']
                if pytorch_shape != jittor_shape:
                    shape_mismatches += 1
                    print(f"âŒ æ ‡å‡†åŒ–åä»å½¢çŠ¶ä¸åŒ¹é…: {name}")
                    print(f"   PyTorch: {pytorch_shape}")
                    print(f"   Jittor: {jittor_shape}")
            
            if shape_mismatches == 0:
                print("âœ… æ ‡å‡†åŒ–å½¢çŠ¶100%å¯¹é½ï¼")
                print(f"\nğŸ‰ æ­å–œï¼å®ç°äº†æ ¸å¿ƒå‚æ•°100%å®Œç¾å¯¹é½ï¼")
                print(f"ğŸ“Š å¯¹é½ç»Ÿè®¡:")
                print(f"  - æ ¸å¿ƒå‚æ•°æ•°é‡: {pytorch_total:,} (100%ä¸€è‡´)")
                print(f"  - å‚æ•°é¡¹æ•°é‡: {len(pytorch_core)} (100%ä¸€è‡´)")
                print(f"  - å‚æ•°åç§°: 100%å¯¹é½")
                print(f"  - æ ‡å‡†åŒ–å½¢çŠ¶: 100%å¯¹é½")
                print(f"  - æ¡†æ¶å·®å¼‚å¤„ç†: {shape_adjustments} ä¸ªScaleå‚æ•°")
                
                # æ˜¾ç¤ºæ¨¡å—ç»Ÿè®¡
                print(f"\nğŸ“Š æŒ‰æ¨¡å—ç»Ÿè®¡:")
                print(f"{'æ¨¡å—':<20} {'å‚æ•°æ•°é‡':<12}")
                print("-" * 35)
                for module in sorted(pytorch_modules.keys(), key=lambda x: pytorch_modules[x], reverse=True):
                    count = pytorch_modules[module]
                    print(f"{module:<20} {count:<12,}")
                
                return True
            else:
                print(f"âŒ æœ‰ {shape_mismatches} ä¸ªå‚æ•°æ ‡å‡†åŒ–åä»å½¢çŠ¶ä¸åŒ¹é…")
        else:
            if only_pytorch:
                print(f"\n  åªåœ¨PyTorchä¸­çš„å‚æ•°:")
                for name in sorted(only_pytorch)[:10]:
                    details = norm_pytorch[name]
                    print(f"    {name}: {details['shape']} ({details['count']} å‚æ•°)")
            
            if only_jittor:
                print(f"\n  åªåœ¨Jittorä¸­çš„å‚æ•°:")
                for name in sorted(only_jittor)[:10]:
                    details = norm_jittor[name]
                    print(f"    {name}: {details['shape']} ({details['count']} å‚æ•°)")
        
        return False
    
    # å¦‚æœè¿˜æœ‰å·®å¼‚ï¼Œç»§ç»­åˆ†æ
    print(f"\nğŸ“Š æŒ‰æ¨¡å—å¯¹æ¯”æ ¸å¿ƒå‚æ•°:")
    print(f"{'æ¨¡å—':<20} {'PyTorch':<12} {'Jittor':<12} {'å·®å¼‚':<12}")
    print("-" * 60)
    
    all_modules = set(pytorch_modules.keys()) | set(jittor_modules.keys())
    
    for module in sorted(all_modules):
        pytorch_count = pytorch_modules.get(module, 0)
        jittor_count = jittor_modules.get(module, 0)
        diff = pytorch_count - jittor_count
        
        status = "âœ…" if diff == 0 else "âŒ"
        print(f"{module:<20} {pytorch_count:<12,} {jittor_count:<12,} {diff:<12,} {status}")
    
    print("-" * 60)
    print(f"{'æ€»è®¡':<20} {pytorch_total:<12,} {jittor_core_total:<12,} {pytorch_total-jittor_core_total:<12,}")
    
    return False


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ å¼€å§‹éªŒè¯æ ¸å¿ƒå‚æ•°100%å¯¹é½")
    print("ç›®æ ‡: æ’é™¤æ¡†æ¶å·®å¼‚ï¼ŒéªŒè¯æ¨¡å‹æ ¸å¿ƒé€»è¾‘100%å¯¹é½")
    
    success = verify_core_parameters_alignment()
    
    if success:
        print("\nâœ… æ ¸å¿ƒå‚æ•°100%å¯¹é½éªŒè¯æˆåŠŸï¼")
        print("ğŸ‰ æ¨¡å‹æ ¸å¿ƒé€»è¾‘å®Œå…¨æ­£ç¡®ï¼Œå¯ä»¥è¿›è¡Œæœ€ç»ˆmAPæµ‹è¯•ï¼")
        print("\nğŸ“ æ¡†æ¶å·®å¼‚è¯´æ˜:")
        print("  - BatchNormç»Ÿè®¡å‚æ•°: Jittorè®¡å…¥å‚æ•°ï¼ŒPyTorchè®¡å…¥buffer")
        print("  - Scaleå‚æ•°å½¢çŠ¶: Jittor [1], PyTorch []")
        print("  - è¿™äº›å·®å¼‚ä¸å½±å“æ¨¡å‹åŠŸèƒ½å’Œæ€§èƒ½")
    else:
        print("\nâŒ æ ¸å¿ƒå‚æ•°ä»æœ‰å·®å¼‚ï¼Œéœ€è¦ç»§ç»­ä¿®å¤")
    
    return success


if __name__ == '__main__':
    success = main()
    sys.exit(0 if success else 1)
