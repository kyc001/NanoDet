#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
æ·±åº¦è°ƒè¯•GhostBlocks
æ‰¾å‡ºtop_down_block1å†…éƒ¨çš„å…·ä½“é—®é¢˜
"""

import os
import sys
import torch
import jittor as jt
import numpy as np

# æ·»åŠ è·¯å¾„
sys.path.append('/home/kyc/project/nanodet/nanodet-jittor')
from nanodet.model.arch.nanodet_plus import NanoDetPlus


def create_test_input():
    """åˆ›å»ºå›ºå®šçš„æµ‹è¯•è¾“å…¥"""
    np.random.seed(42)
    torch.manual_seed(42)
    jt.set_global_seed(42)
    
    # ä½¿ç”¨å›ºå®šçš„æµ‹è¯•æ•°æ®
    if os.path.exists("fixed_input_data.npy"):
        input_data = np.load("fixed_input_data.npy")
    else:
        input_data = np.random.randn(1, 3, 320, 320).astype(np.float32)
        np.save("fixed_input_data.npy", input_data)
    
    return input_data


def create_jittor_model():
    """åˆ›å»ºJittoræ¨¡å‹"""
    print("ğŸ” åˆ›å»ºJittoræ¨¡å‹...")
    
    backbone_cfg = {
        'name': 'ShuffleNetV2',
        'model_size': '1.0x',
        'out_stages': [2, 3, 4],
        'activation': 'LeakyReLU',
        'pretrain': True
    }
    
    fpn_cfg = {
        'name': 'GhostPAN',
        'in_channels': [116, 232, 464],
        'out_channels': 96,
        'kernel_size': 5,
        'num_extra_level': 1,
        'use_depthwise': True,
        'activation': 'LeakyReLU'
    }
    
    head_cfg = {
        'name': 'NanoDetPlusHead',
        'num_classes': 20,
        'input_channel': 96,
        'feat_channels': 96,
        'stacked_convs': 2,
        'kernel_size': 5,
        'strides': [8, 16, 32, 64],
        'activation': 'LeakyReLU',
        'reg_max': 7,
        'norm_cfg': {'type': 'BN'},
        'loss': {
            'loss_qfl': {
                'name': 'QualityFocalLoss',
                'use_sigmoid': True,
                'beta': 2.0,
                'loss_weight': 1.0
            },
            'loss_dfl': {
                'name': 'DistributionFocalLoss',
                'loss_weight': 0.25
            },
            'loss_bbox': {
                'name': 'GIoULoss',
                'loss_weight': 2.0
            }
        }
    }
    
    aux_head_cfg = {
        'name': 'SimpleConvHead',
        'num_classes': 20,
        'input_channel': 192,
        'feat_channels': 192,
        'stacked_convs': 4,
        'strides': [8, 16, 32, 64],
        'activation': 'LeakyReLU',
        'reg_max': 7
    }
    
    model = NanoDetPlus(backbone_cfg, fpn_cfg, aux_head_cfg, head_cfg)
    
    # åŠ è½½æƒé‡
    print("åŠ è½½PyTorchæƒé‡...")
    checkpoint_path = "/home/kyc/project/nanodet/nanodet-pytorch/workspace/nanodet-plus-m_320_voc_bs64/model_best/model_best.ckpt"
    ckpt = torch.load(checkpoint_path, map_location='cpu')
    state_dict = ckpt.get('state_dict', ckpt)
    
    jittor_state_dict = {}
    for name, param in model.named_parameters():
        jittor_state_dict[name] = param
    
    # æƒé‡åŠ è½½
    for pytorch_name, pytorch_param in state_dict.items():
        jittor_name = pytorch_name
        if jittor_name.startswith("model."):
            jittor_name = jittor_name[6:]
        
        if "num_batches_tracked" in jittor_name or jittor_name.startswith("avg_"):
            continue
        
        if "distribution_project.project" in jittor_name:
            continue
        
        if jittor_name in jittor_state_dict:
            jittor_param = jittor_state_dict[jittor_name]
            
            if list(pytorch_param.shape) == list(jittor_param.shape):
                jittor_param.assign(jt.array(pytorch_param.detach().numpy()))
            elif "scale" in jittor_name and len(pytorch_param.shape) == 0 and list(jittor_param.shape) == [1]:
                jittor_param.assign(jt.array([pytorch_param.detach().numpy()]))
    
    model.eval()
    return model


def debug_ghost_blocks_internal():
    """æ·±åº¦è°ƒè¯•GhostBlockså†…éƒ¨"""
    print(f"ğŸ” æ·±åº¦è°ƒè¯•GhostBlockså†…éƒ¨")
    print("=" * 60)
    
    # åˆ›å»ºæ¨¡å‹
    model = create_jittor_model()
    
    # åˆ›å»ºæµ‹è¯•è¾“å…¥
    input_data = create_test_input()
    jittor_input = jt.array(input_data)
    
    with jt.no_grad():
        # è·å–åˆ°top_down_block1çš„è¾“å…¥
        backbone_features = model.backbone(jittor_input)
        fpn = model.fpn
        
        # 1x1 conv to reduce channels
        reduced_inputs = []
        for i, (input_x, reduce) in enumerate(zip(backbone_features, fpn.reduce_layers)):
            reduced = reduce(input_x)
            reduced_inputs.append(reduced)
        
        # è·å–åˆ°top_down_block1çš„è¾“å…¥
        feat_high = reduced_inputs[-1]  # [1,96,10,10]
        feat_low = reduced_inputs[1]    # [1,96,20,20]
        
        # upsample
        upsample_feat = fpn.upsample(feat_high)
        
        # concat - è¿™æ˜¯top_down_block1çš„è¾“å…¥
        concat_feat = jt.concat([upsample_feat, feat_low], dim=1)
        print(f"top_down_block1è¾“å…¥: {concat_feat.shape}, èŒƒå›´[{concat_feat.min():.6f}, {concat_feat.max():.6f}]")
        
        # è·å–top_down_block1
        top_down_block1 = fpn.top_down_blocks[1]
        print(f"top_down_block1ç±»å‹: {type(top_down_block1)}")
        
        # æ£€æŸ¥top_down_block1çš„ç»“æ„
        print(f"\ntop_down_block1ç»“æ„:")
        for name, module in top_down_block1.named_modules():
            if name:  # è·³è¿‡æ ¹æ¨¡å—
                print(f"  {name}: {type(module)}")
        
        # æ£€æŸ¥top_down_block1çš„å‚æ•°
        print(f"\ntop_down_block1å‚æ•°:")
        for name, param in top_down_block1.named_parameters():
            print(f"  {name}: {param.shape}, èŒƒå›´[{param.min():.6f}, {param.max():.6f}]")
        
        # æ‰‹åŠ¨æ‰§è¡Œtop_down_block1çš„æ¯ä¸€æ­¥
        print(f"\nğŸ” æ‰‹åŠ¨æ‰§è¡Œtop_down_block1:")
        
        # top_down_block1æ˜¯GhostBlocks
        ghost_blocks = top_down_block1
        x = concat_feat
        
        print(f"  è¾“å…¥: {x.shape}, èŒƒå›´[{x.min():.6f}, {x.max():.6f}]")
        
        # æ£€æŸ¥æ˜¯å¦æœ‰reduce_conv
        if hasattr(ghost_blocks, 'reduce_conv') and ghost_blocks.use_res:
            reduce_conv_out = ghost_blocks.reduce_conv(x)
            print(f"  reduce_convè¾“å‡º: {reduce_conv_out.shape}, èŒƒå›´[{reduce_conv_out.min():.6f}, {reduce_conv_out.max():.6f}]")
        
        # æ‰§è¡Œblocks
        blocks_input = x
        print(f"  blocksè¾“å…¥: {blocks_input.shape}, èŒƒå›´[{blocks_input.min():.6f}, {blocks_input.max():.6f}]")
        
        # é€ä¸ªæ‰§è¡ŒGhostBottleneck
        current_x = blocks_input
        for i, block in enumerate(ghost_blocks.blocks):
            print(f"\n  ğŸ” GhostBottleneck {i}:")
            print(f"    è¾“å…¥: {current_x.shape}, èŒƒå›´[{current_x.min():.6f}, {current_x.max():.6f}]")
            
            # æ‰‹åŠ¨æ‰§è¡ŒGhostBottleneckçš„æ¯ä¸€æ­¥
            residual = current_x
            
            # 1st ghost bottleneck
            ghost1_out = block.ghost1(current_x)
            print(f"    ghost1è¾“å‡º: {ghost1_out.shape}, èŒƒå›´[{ghost1_out.min():.6f}, {ghost1_out.max():.6f}]")
            
            # Depth-wise convolution (å¦‚æœstride > 1)
            if block.stride > 1:
                dw_out = block.conv_dw(ghost1_out)
                dw_out = block.bn_dw(dw_out)
                print(f"    dw_convè¾“å‡º: {dw_out.shape}, èŒƒå›´[{dw_out.min():.6f}, {dw_out.max():.6f}]")
                ghost1_out = dw_out
            
            # Squeeze-and-excitation (å¦‚æœæœ‰)
            if block.se is not None:
                se_out = block.se(ghost1_out)
                print(f"    seè¾“å‡º: {se_out.shape}, èŒƒå›´[{se_out.min():.6f}, {se_out.max():.6f}]")
                ghost1_out = se_out
            
            # 2nd ghost bottleneck
            ghost2_out = block.ghost2(ghost1_out)
            print(f"    ghost2è¾“å‡º: {ghost2_out.shape}, èŒƒå›´[{ghost2_out.min():.6f}, {ghost2_out.max():.6f}]")
            
            # shortcut
            shortcut_out = block.shortcut(residual)
            print(f"    shortcutè¾“å‡º: {shortcut_out.shape}, èŒƒå›´[{shortcut_out.min():.6f}, {shortcut_out.max():.6f}]")
            
            # ç›¸åŠ 
            block_out = ghost2_out + shortcut_out
            print(f"    blockè¾“å‡º: {block_out.shape}, èŒƒå›´[{block_out.min():.6f}, {block_out.max():.6f}]")
            
            # å¯¹æ¯”å®Œæ•´blockè¾“å‡º
            full_block_out = block(current_x)
            diff = jt.abs(block_out - full_block_out).max()
            print(f"    æ‰‹åŠ¨vså®Œæ•´å·®å¼‚: {diff:.10f}")
            
            current_x = block_out
        
        blocks_out = current_x
        print(f"\n  blockså®Œæ•´è¾“å‡º: {blocks_out.shape}, èŒƒå›´[{blocks_out.min():.6f}, {blocks_out.max():.6f}]")
        
        # æœ€ç»ˆè¾“å‡º
        if ghost_blocks.use_res:
            final_out = blocks_out + ghost_blocks.reduce_conv(concat_feat)
        else:
            final_out = blocks_out
        
        print(f"  æœ€ç»ˆè¾“å‡º: {final_out.shape}, èŒƒå›´[{final_out.min():.6f}, {final_out.max():.6f}]")
        
        # å¯¹æ¯”å®Œæ•´GhostBlocksè¾“å‡º
        full_ghost_blocks_out = ghost_blocks(concat_feat)
        diff = jt.abs(final_out - full_ghost_blocks_out).max()
        print(f"  æ‰‹åŠ¨vså®Œæ•´GhostBlockså·®å¼‚: {diff:.10f}")
        
        if diff < 1e-6:
            print(f"  âœ… æ‰‹åŠ¨è®¡ç®—ä¸å®Œæ•´GhostBlocksä¸€è‡´")
        else:
            print(f"  âŒ æ‰‹åŠ¨è®¡ç®—ä¸å®Œæ•´GhostBlocksä¸ä¸€è‡´")


def check_ghost_blocks_weights():
    """æ£€æŸ¥GhostBlocksçš„æƒé‡åŠ è½½æƒ…å†µ"""
    print(f"\nğŸ” æ£€æŸ¥GhostBlocksçš„æƒé‡åŠ è½½æƒ…å†µ")
    print("=" * 60)
    
    # åˆ›å»ºæ¨¡å‹
    model = create_jittor_model()
    
    # è·å–top_down_block1
    top_down_block1 = model.fpn.top_down_blocks[1]
    
    # æ£€æŸ¥æƒé‡
    print(f"top_down_block1æƒé‡:")
    for name, param in top_down_block1.named_parameters():
        print(f"  {name}: {param.shape}, èŒƒå›´[{param.min():.6f}, {param.max():.6f}], å‡å€¼{param.mean():.6f}")
    
    # åŠ è½½PyTorchæƒé‡å¹¶å¯¹æ¯”
    checkpoint_path = "/home/kyc/project/nanodet/nanodet-pytorch/workspace/nanodet-plus-m_320_voc_bs64/model_best/model_best.ckpt"
    ckpt = torch.load(checkpoint_path, map_location='cpu')
    state_dict = ckpt.get('state_dict', ckpt)
    
    print(f"\nå¯¹åº”çš„PyTorchæƒé‡:")
    for pytorch_name, pytorch_param in state_dict.items():
        if "fpn.top_down_blocks.1" in pytorch_name:
            jittor_name = pytorch_name
            if jittor_name.startswith("model."):
                jittor_name = jittor_name[6:]
            
            pytorch_data = pytorch_param.detach().numpy()
            print(f"  {pytorch_name} -> {jittor_name}: {pytorch_data.shape}, èŒƒå›´[{pytorch_data.min():.6f}, {pytorch_data.max():.6f}], å‡å€¼{pytorch_data.mean():.6f}")
            
            # æ£€æŸ¥æ˜¯å¦åœ¨Jittoræ¨¡å‹ä¸­
            jittor_param = None
            for name, param in top_down_block1.named_parameters():
                full_name = f"fpn.top_down_blocks.1.{name}"
                if full_name == jittor_name:
                    jittor_param = param
                    break
            
            if jittor_param is not None:
                diff = np.abs(pytorch_data - jittor_param.numpy()).max()
                print(f"    æƒé‡å·®å¼‚: {diff:.10f}")
                if diff < 1e-6:
                    print(f"    âœ… æƒé‡ä¸€è‡´")
                else:
                    print(f"    âŒ æƒé‡ä¸ä¸€è‡´")
            else:
                print(f"    âŒ åœ¨Jittorä¸­æœªæ‰¾åˆ°å¯¹åº”å‚æ•°")


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ å¼€å§‹æ·±åº¦è°ƒè¯•GhostBlocks")
    
    # æ·±åº¦è°ƒè¯•GhostBlockså†…éƒ¨
    debug_ghost_blocks_internal()
    
    # æ£€æŸ¥æƒé‡åŠ è½½
    check_ghost_blocks_weights()
    
    print(f"\nâœ… æ·±åº¦è°ƒè¯•å®Œæˆ")


if __name__ == '__main__':
    main()
