#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
æ£€æŸ¥BatchNormå‚æ•°è®¾ç½®
æ‰¾å‡ºPyTorchå’ŒJittor BatchNormçš„å‚æ•°å·®å¼‚
"""

import torch
import jittor as jt
import numpy as np


def check_batchnorm_parameters():
    """æ£€æŸ¥BatchNormå‚æ•°è®¾ç½®"""
    print("ğŸ” æ£€æŸ¥BatchNormå‚æ•°è®¾ç½®")
    print("=" * 60)
    
    # PyTorch BatchNorm
    print("PyTorch BatchNorm2dé»˜è®¤å‚æ•°:")
    pytorch_bn = torch.nn.BatchNorm2d(64)
    print(f"  num_features: {pytorch_bn.num_features}")
    print(f"  eps: {pytorch_bn.eps}")
    print(f"  momentum: {pytorch_bn.momentum}")
    print(f"  affine: {pytorch_bn.affine}")
    print(f"  track_running_stats: {pytorch_bn.track_running_stats}")
    print(f"  training: {pytorch_bn.training}")
    
    # Jittor BatchNorm
    print(f"\nJittor BatchNorm2dé»˜è®¤å‚æ•°:")
    jittor_bn = jt.nn.BatchNorm2d(64)
    print(f"  num_features: {jittor_bn.num_features}")
    print(f"  eps: {jittor_bn.eps}")
    print(f"  momentum: {jittor_bn.momentum}")
    print(f"  affine: {jittor_bn.affine}")
    print(f"  is_train: {jittor_bn.is_train}")
    
    # å¯¹æ¯”å‚æ•°
    print(f"\nå‚æ•°å¯¹æ¯”:")
    params_match = True
    
    if pytorch_bn.eps != jittor_bn.eps:
        print(f"  âŒ epsä¸åŒ¹é…: PyTorch={pytorch_bn.eps}, Jittor={jittor_bn.eps}")
        params_match = False
    else:
        print(f"  âœ… epsåŒ¹é…: {pytorch_bn.eps}")
    
    if pytorch_bn.momentum != jittor_bn.momentum:
        print(f"  âŒ momentumä¸åŒ¹é…: PyTorch={pytorch_bn.momentum}, Jittor={jittor_bn.momentum}")
        params_match = False
    else:
        print(f"  âœ… momentumåŒ¹é…: {pytorch_bn.momentum}")
    
    if pytorch_bn.affine != jittor_bn.affine:
        print(f"  âŒ affineä¸åŒ¹é…: PyTorch={pytorch_bn.affine}, Jittor={jittor_bn.affine}")
        params_match = False
    else:
        print(f"  âœ… affineåŒ¹é…: {pytorch_bn.affine}")
    
    return params_match


def test_batchnorm_behavior():
    """æµ‹è¯•BatchNormè¡Œä¸ºå·®å¼‚"""
    print(f"\nğŸ” æµ‹è¯•BatchNormè¡Œä¸ºå·®å¼‚")
    print("=" * 60)
    
    # åˆ›å»ºæµ‹è¯•æ•°æ®
    np.random.seed(42)
    torch.manual_seed(42)
    jt.set_global_seed(42)
    
    test_data = np.random.randn(2, 64, 8, 8).astype(np.float32)
    
    # PyTorch BatchNorm
    pytorch_bn = torch.nn.BatchNorm2d(64)
    pytorch_input = torch.from_numpy(test_data)
    
    # Jittor BatchNorm
    jittor_bn = jt.nn.BatchNorm2d(64)
    jittor_input = jt.array(test_data)
    
    # è®¾ç½®ç›¸åŒçš„æƒé‡
    weight = np.random.randn(64).astype(np.float32)
    bias = np.random.randn(64).astype(np.float32)
    running_mean = np.random.randn(64).astype(np.float32)
    running_var = np.random.randn(64).astype(np.float32)
    
    pytorch_bn.weight.data = torch.from_numpy(weight)
    pytorch_bn.bias.data = torch.from_numpy(bias)
    pytorch_bn.running_mean.data = torch.from_numpy(running_mean)
    pytorch_bn.running_var.data = torch.from_numpy(running_var)
    
    jittor_bn.weight.assign(jt.array(weight))
    jittor_bn.bias.assign(jt.array(bias))
    jittor_bn.running_mean.assign(jt.array(running_mean))
    jittor_bn.running_var.assign(jt.array(running_var))
    
    print(f"æµ‹è¯•æ•°æ®: {test_data.shape}, èŒƒå›´[{test_data.min():.6f}, {test_data.max():.6f}]")
    
    # æµ‹è¯•è®­ç»ƒæ¨¡å¼
    print(f"\nè®­ç»ƒæ¨¡å¼æµ‹è¯•:")
    pytorch_bn.train()
    jittor_bn.train()
    
    pytorch_output_train = pytorch_bn(pytorch_input)
    jittor_output_train = jittor_bn(jittor_input)
    
    print(f"  PyTorchè¾“å‡º: èŒƒå›´[{pytorch_output_train.min():.6f}, {pytorch_output_train.max():.6f}]")
    print(f"  Jittorè¾“å‡º: èŒƒå›´[{jittor_output_train.min():.6f}, {jittor_output_train.max():.6f}]")
    
    train_diff = np.abs(pytorch_output_train.detach().numpy() - jittor_output_train.numpy()).max()
    print(f"  è®­ç»ƒæ¨¡å¼å·®å¼‚: {train_diff:.8f}")
    
    # æµ‹è¯•è¯„ä¼°æ¨¡å¼
    print(f"\nè¯„ä¼°æ¨¡å¼æµ‹è¯•:")
    pytorch_bn.eval()
    jittor_bn.eval()
    
    pytorch_output_eval = pytorch_bn(pytorch_input)
    jittor_output_eval = jittor_bn(jittor_input)
    
    print(f"  PyTorchè¾“å‡º: èŒƒå›´[{pytorch_output_eval.min():.6f}, {pytorch_output_eval.max():.6f}]")
    print(f"  Jittorè¾“å‡º: èŒƒå›´[{jittor_output_eval.min():.6f}, {jittor_output_eval.max():.6f}]")
    
    eval_diff = np.abs(pytorch_output_eval.detach().numpy() - jittor_output_eval.numpy()).max()
    print(f"  è¯„ä¼°æ¨¡å¼å·®å¼‚: {eval_diff:.8f}")
    
    # æ£€æŸ¥running statsæ›´æ–°
    print(f"\næ£€æŸ¥running statsæ›´æ–°:")
    print(f"  PyTorch running_mean: èŒƒå›´[{pytorch_bn.running_mean.min():.6f}, {pytorch_bn.running_mean.max():.6f}]")
    print(f"  Jittor running_mean: èŒƒå›´[{jittor_bn.running_mean.min():.6f}, {jittor_bn.running_mean.max():.6f}]")
    
    running_mean_diff = np.abs(pytorch_bn.running_mean.detach().numpy() - jittor_bn.running_mean.numpy()).max()
    print(f"  running_meanå·®å¼‚: {running_mean_diff:.8f}")
    
    print(f"  PyTorch running_var: èŒƒå›´[{pytorch_bn.running_var.min():.6f}, {pytorch_bn.running_var.max():.6f}]")
    print(f"  Jittor running_var: èŒƒå›´[{jittor_bn.running_var.min():.6f}, {jittor_bn.running_var.max():.6f}]")
    
    running_var_diff = np.abs(pytorch_bn.running_var.detach().numpy() - jittor_bn.running_var.numpy()).max()
    print(f"  running_varå·®å¼‚: {running_var_diff:.8f}")
    
    return train_diff < 1e-4 and eval_diff < 1e-4


def check_model_batchnorm_settings():
    """æ£€æŸ¥æ¨¡å‹ä¸­BatchNormçš„è®¾ç½®"""
    print(f"\nğŸ” æ£€æŸ¥æ¨¡å‹ä¸­BatchNormçš„è®¾ç½®")
    print("=" * 60)
    
    import sys
    sys.path.append('/home/kyc/project/nanodet/nanodet-jittor')
    from nanodet.model.arch.nanodet_plus import NanoDetPlus
    
    # åˆ›å»ºæ¨¡å‹
    backbone_cfg = {
        'name': 'ShuffleNetV2',
        'model_size': '1.0x',
        'out_stages': [2, 3, 4],
        'activation': 'LeakyReLU',
        'pretrain': False  # ä¸åŠ è½½é¢„è®­ç»ƒæƒé‡ï¼Œä¸“æ³¨äºå‚æ•°æ£€æŸ¥
    }
    
    fpn_cfg = {
        'name': 'GhostPAN',
        'in_channels': [116, 232, 464],
        'out_channels': 96,
        'kernel_size': 5,
        'num_extra_level': 1,
        'use_depthwise': True,
        'activation': 'LeakyReLU'
    }
    
    head_cfg = {
        'name': 'NanoDetPlusHead',
        'num_classes': 20,
        'input_channel': 96,
        'feat_channels': 96,
        'stacked_convs': 2,
        'kernel_size': 5,
        'strides': [8, 16, 32, 64],
        'activation': 'LeakyReLU',
        'reg_max': 7,
        'norm_cfg': {'type': 'BN'},
        'loss': {
            'loss_qfl': {
                'name': 'QualityFocalLoss',
                'use_sigmoid': True,
                'beta': 2.0,
                'loss_weight': 1.0
            },
            'loss_dfl': {
                'name': 'DistributionFocalLoss',
                'loss_weight': 0.25
            },
            'loss_bbox': {
                'name': 'GIoULoss',
                'loss_weight': 2.0
            }
        }
    }
    
    aux_head_cfg = {
        'name': 'SimpleConvHead',
        'num_classes': 20,
        'input_channel': 192,
        'feat_channels': 192,
        'stacked_convs': 4,
        'strides': [8, 16, 32, 64],
        'activation': 'LeakyReLU',
        'reg_max': 7
    }
    
    model = NanoDetPlus(backbone_cfg, fpn_cfg, aux_head_cfg, head_cfg)
    model.eval()
    
    # æ£€æŸ¥æ¨¡å‹ä¸­çš„BatchNormå±‚
    bn_layers = []
    for name, module in model.named_modules():
        if isinstance(module, jt.nn.BatchNorm2d) or isinstance(module, jt.nn.BatchNorm):
            bn_layers.append((name, module))
    
    print(f"æ‰¾åˆ° {len(bn_layers)} ä¸ªBatchNormå±‚:")
    
    for i, (name, bn) in enumerate(bn_layers[:10]):  # åªæ˜¾ç¤ºå‰10ä¸ª
        print(f"  {name}:")
        print(f"    eps: {bn.eps}")
        print(f"    momentum: {bn.momentum}")
        print(f"    affine: {bn.affine}")
        print(f"    is_train: {bn.is_train}")
        
        # æ£€æŸ¥æƒé‡å’Œbiasçš„èŒƒå›´
        if hasattr(bn, 'weight') and bn.weight is not None:
            print(f"    weightèŒƒå›´: [{bn.weight.min():.6f}, {bn.weight.max():.6f}]")
        if hasattr(bn, 'bias') and bn.bias is not None:
            print(f"    biasèŒƒå›´: [{bn.bias.min():.6f}, {bn.bias.max():.6f}]")
        if hasattr(bn, 'running_mean') and bn.running_mean is not None:
            print(f"    running_meanèŒƒå›´: [{bn.running_mean.min():.6f}, {bn.running_mean.max():.6f}]")
        if hasattr(bn, 'running_var') and bn.running_var is not None:
            print(f"    running_varèŒƒå›´: [{bn.running_var.min():.6f}, {bn.running_var.max():.6f}]")


def test_specific_batchnorm_issue():
    """æµ‹è¯•ç‰¹å®šçš„BatchNormé—®é¢˜"""
    print(f"\nğŸ” æµ‹è¯•ç‰¹å®šçš„BatchNormé—®é¢˜")
    print("=" * 60)
    
    # åˆ›å»ºä¸æ¨¡å‹ä¸­ç›¸åŒé…ç½®çš„BatchNorm
    pytorch_bn = torch.nn.BatchNorm2d(64, eps=1e-5, momentum=0.1, affine=True, track_running_stats=True)
    jittor_bn = jt.nn.BatchNorm2d(64, eps=1e-5, momentum=0.1, affine=True)
    
    # è®¾ç½®ç›¸åŒçš„åˆå§‹çŠ¶æ€
    weight = np.ones(64, dtype=np.float32)
    bias = np.zeros(64, dtype=np.float32)
    running_mean = np.zeros(64, dtype=np.float32)
    running_var = np.ones(64, dtype=np.float32)
    
    pytorch_bn.weight.data = torch.from_numpy(weight)
    pytorch_bn.bias.data = torch.from_numpy(bias)
    pytorch_bn.running_mean.data = torch.from_numpy(running_mean)
    pytorch_bn.running_var.data = torch.from_numpy(running_var)
    
    jittor_bn.weight.assign(jt.array(weight))
    jittor_bn.bias.assign(jt.array(bias))
    jittor_bn.running_mean.assign(jt.array(running_mean))
    jittor_bn.running_var.assign(jt.array(running_var))
    
    # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
    pytorch_bn.eval()
    jittor_bn.eval()
    
    # åˆ›å»ºç‰¹å®šçš„æµ‹è¯•è¾“å…¥
    test_input = np.array([[[[1.0, 2.0], [3.0, 4.0]]]], dtype=np.float32)  # [1, 1, 2, 2]
    test_input = np.repeat(test_input, 64, axis=1)  # [1, 64, 2, 2]
    
    pytorch_input = torch.from_numpy(test_input)
    jittor_input = jt.array(test_input)
    
    print(f"æµ‹è¯•è¾“å…¥: {test_input.shape}")
    print(f"è¾“å…¥èŒƒå›´: [{test_input.min():.6f}, {test_input.max():.6f}]")
    
    # å‰å‘ä¼ æ’­
    pytorch_output = pytorch_bn(pytorch_input)
    jittor_output = jittor_bn(jittor_input)
    
    print(f"\nPyTorchè¾“å‡º: èŒƒå›´[{pytorch_output.min():.6f}, {pytorch_output.max():.6f}]")
    print(f"Jittorè¾“å‡º: èŒƒå›´[{jittor_output.min():.6f}, {jittor_output.max():.6f}]")
    
    diff = np.abs(pytorch_output.detach().numpy() - jittor_output.numpy()).max()
    print(f"å·®å¼‚: {diff:.10f}")
    
    if diff < 1e-6:
        print(f"âœ… BatchNormè¡Œä¸ºä¸€è‡´")
        return True
    else:
        print(f"âŒ BatchNormè¡Œä¸ºä¸ä¸€è‡´")
        
        # è¯¦ç»†åˆ†æ
        pytorch_np = pytorch_output.detach().numpy()
        jittor_np = jittor_output.numpy()
        
        print(f"PyTorchè¯¦ç»†è¾“å‡º: {pytorch_np.flatten()[:10]}")
        print(f"Jittorè¯¦ç»†è¾“å‡º: {jittor_np.flatten()[:10]}")
        
        return False


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ å¼€å§‹æ£€æŸ¥BatchNormå‚æ•°è®¾ç½®")
    
    # æ£€æŸ¥é»˜è®¤å‚æ•°
    params_ok = check_batchnorm_parameters()
    
    # æµ‹è¯•è¡Œä¸ºå·®å¼‚
    behavior_ok = test_batchnorm_behavior()
    
    # æ£€æŸ¥æ¨¡å‹ä¸­çš„è®¾ç½®
    check_model_batchnorm_settings()
    
    # æµ‹è¯•ç‰¹å®šé—®é¢˜
    specific_ok = test_specific_batchnorm_issue()
    
    print(f"\nğŸ“Š æ£€æŸ¥æ€»ç»“:")
    print(f"  é»˜è®¤å‚æ•°: {'âœ…' if params_ok else 'âŒ'}")
    print(f"  è¡Œä¸ºæµ‹è¯•: {'âœ…' if behavior_ok else 'âŒ'}")
    print(f"  ç‰¹å®šæµ‹è¯•: {'âœ…' if specific_ok else 'âŒ'}")
    
    if all([params_ok, behavior_ok, specific_ok]):
        print(f"\nâœ… BatchNormæ²¡æœ‰é—®é¢˜ï¼Œéœ€è¦æ£€æŸ¥å…¶ä»–åŸå› ")
    else:
        print(f"\nâŒ å‘ç°BatchNormé—®é¢˜ï¼Œéœ€è¦ä¿®å¤")
    
    print(f"\nâœ… æ£€æŸ¥å®Œæˆ")


if __name__ == '__main__':
    main()
