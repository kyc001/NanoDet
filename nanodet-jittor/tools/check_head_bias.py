#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
æ£€æŸ¥Head biasçš„é—®é¢˜
æ‰¾å‡ºä¸ºä»€ä¹ˆåˆ†ç±»é¢„æµ‹å…¨æ˜¯è´Ÿå€¼
"""

import os
import sys
import torch
import jittor as jt
import numpy as np

# æ·»åŠ è·¯å¾„
sys.path.append('/home/kyc/project/nanodet/nanodet-jittor')
from nanodet.model.arch.nanodet_plus import NanoDetPlus


def check_head_bias():
    """æ£€æŸ¥Head biasçš„è¯¦ç»†æƒ…å†µ"""
    print("ğŸ” æ£€æŸ¥Head biasè¯¦ç»†æƒ…å†µ")
    print("=" * 60)
    
    # åˆ›å»ºæ¨¡å‹
    print("1ï¸âƒ£ åˆ›å»ºJittoræ¨¡å‹...")
    
    backbone_cfg = {
        'name': 'ShuffleNetV2',
        'model_size': '1.0x',
        'out_stages': [2, 3, 4],
        'activation': 'LeakyReLU',
        'pretrain': True
    }
    
    fpn_cfg = {
        'name': 'GhostPAN',
        'in_channels': [116, 232, 464],
        'out_channels': 96,
        'kernel_size': 5,
        'num_extra_level': 1,
        'use_depthwise': True,
        'activation': 'LeakyReLU'
    }
    
    head_cfg = {
        'name': 'NanoDetPlusHead',
        'num_classes': 20,
        'input_channel': 96,
        'feat_channels': 96,
        'stacked_convs': 2,
        'kernel_size': 5,
        'strides': [8, 16, 32, 64],
        'activation': 'LeakyReLU',
        'reg_max': 7,
        'norm_cfg': {'type': 'BN'},
        'loss': {
            'loss_qfl': {
                'name': 'QualityFocalLoss',
                'use_sigmoid': True,
                'beta': 2.0,
                'loss_weight': 1.0
            },
            'loss_dfl': {
                'name': 'DistributionFocalLoss',
                'loss_weight': 0.25
            },
            'loss_bbox': {
                'name': 'GIoULoss',
                'loss_weight': 2.0
            }
        }
    }
    
    aux_head_cfg = {
        'name': 'SimpleConvHead',
        'num_classes': 20,
        'input_channel': 192,
        'feat_channels': 192,
        'stacked_convs': 4,
        'strides': [8, 16, 32, 64],
        'activation': 'LeakyReLU',
        'reg_max': 7
    }
    
    model = NanoDetPlus(backbone_cfg, fpn_cfg, aux_head_cfg, head_cfg)
    
    print("2ï¸âƒ£ æ£€æŸ¥åˆå§‹åŒ–åçš„Head bias...")
    
    # æ£€æŸ¥åˆå§‹åŒ–åçš„bias
    for i, gfl_cls in enumerate(model.head.gfl_cls):
        if hasattr(gfl_cls, 'bias') and gfl_cls.bias is not None:
            bias_value = gfl_cls.bias.numpy()
            print(f"   gfl_cls[{i}] bias: {bias_value[:5]}... (å‰5ä¸ªå€¼)")
            print(f"   gfl_cls[{i}] biasèŒƒå›´: [{bias_value.min():.6f}, {bias_value.max():.6f}]")
        else:
            print(f"   gfl_cls[{i}] æ²¡æœ‰bias")
    
    print("\n3ï¸âƒ£ åŠ è½½PyTorchæƒé‡...")
    
    # åŠ è½½PyTorchæƒé‡
    checkpoint_path = "/home/kyc/project/nanodet/nanodet-pytorch/workspace/nanodet-plus-m_320_voc_bs64/model_best/model_best.ckpt"
    ckpt = torch.load(checkpoint_path, map_location='cpu')
    state_dict = ckpt.get('state_dict', ckpt)
    
    # æ£€æŸ¥PyTorchæƒé‡ä¸­çš„Head bias
    print("\n4ï¸âƒ£ æ£€æŸ¥PyTorchæƒé‡ä¸­çš„Head bias...")
    
    head_bias_params = {}
    for name, param in state_dict.items():
        if 'head.gfl_cls' in name and 'bias' in name:
            head_bias_params[name] = param
            print(f"   {name}: {param.shape}, èŒƒå›´[{param.min():.6f}, {param.max():.6f}]")
            print(f"     å‰5ä¸ªå€¼: {param[:5].tolist()}")
    
    print(f"\n5ï¸âƒ£ åŠ è½½æƒé‡åˆ°Jittoræ¨¡å‹...")
    
    # è·å–Jittoræ¨¡å‹çš„å‚æ•°å­—å…¸
    jittor_state_dict = {}
    for name, param in model.named_parameters():
        jittor_state_dict[name] = param
    
    # æƒé‡åŠ è½½
    loaded_head_bias = []
    for pytorch_name, pytorch_param in state_dict.items():
        jittor_name = pytorch_name
        if jittor_name.startswith("model."):
            jittor_name = jittor_name[6:]
        
        if "num_batches_tracked" in jittor_name or jittor_name.startswith("avg_"):
            continue
        
        if "distribution_project.project" in jittor_name:
            continue
        
        if jittor_name in jittor_state_dict:
            jittor_param = jittor_state_dict[jittor_name]
            
            if list(pytorch_param.shape) == list(jittor_param.shape):
                jittor_param.assign(jt.array(pytorch_param.detach().numpy()))
                
                # è®°å½•Head biasçš„åŠ è½½
                if 'head.gfl_cls' in jittor_name and 'bias' in jittor_name:
                    loaded_head_bias.append((jittor_name, pytorch_param.detach().numpy()))
            elif "scale" in jittor_name and len(pytorch_param.shape) == 0 and list(jittor_param.shape) == [1]:
                jittor_param.assign(jt.array([pytorch_param.detach().numpy()]))
    
    print(f"\n6ï¸âƒ£ æ£€æŸ¥åŠ è½½åçš„Head bias...")
    
    # æ£€æŸ¥åŠ è½½åçš„bias
    for i, gfl_cls in enumerate(model.head.gfl_cls):
        if hasattr(gfl_cls, 'bias') and gfl_cls.bias is not None:
            bias_value = gfl_cls.bias.numpy()
            print(f"   gfl_cls[{i}] bias: {bias_value[:5]}... (å‰5ä¸ªå€¼)")
            print(f"   gfl_cls[{i}] biasèŒƒå›´: [{bias_value.min():.6f}, {bias_value.max():.6f}]")
    
    print(f"\n7ï¸âƒ£ æµ‹è¯•æ¨¡å‹è¾“å‡º...")
    
    # æµ‹è¯•æ¨¡å‹è¾“å‡º
    model.eval()
    test_input = jt.randn(1, 3, 320, 320)
    
    with jt.no_grad():
        output = model(test_input)
    
    cls_preds = output[:, :, :20]
    cls_scores = jt.sigmoid(cls_preds)
    
    print(f"   æ¨¡å‹è¾“å‡ºå½¢çŠ¶: {output.shape}")
    print(f"   åˆ†ç±»é¢„æµ‹èŒƒå›´: [{cls_preds.min():.6f}, {cls_preds.max():.6f}]")
    print(f"   æœ€é«˜ç½®ä¿¡åº¦: {cls_scores.max():.6f}")
    
    # åˆ†ææ¯ä¸ªlevelçš„è¾“å‡º
    print(f"\n8ï¸âƒ£ åˆ†ææ¯ä¸ªlevelçš„è¾“å‡º...")
    
    # é‡æ–°è¿è¡ŒHeadï¼Œè·å–æ¯ä¸ªlevelçš„è¾“å‡º
    backbone_features = model.backbone(test_input)
    fpn_features = model.fpn(backbone_features)
    
    level_outputs = []
    for i, (feat, cls_convs, gfl_cls) in enumerate(zip(
        fpn_features,
        model.head.cls_convs,
        model.head.gfl_cls,
    )):
        for conv in cls_convs:
            feat = conv(feat)
        level_output = gfl_cls(feat)
        level_outputs.append(level_output)
        
        # åˆ†æè¿™ä¸ªlevelçš„è¾“å‡º
        level_flat = level_output.flatten(start_dim=2)
        level_cls = level_flat[:, :20, :]
        level_cls_scores = jt.sigmoid(level_cls)
        
        print(f"   Level {i}: è¾“å‡ºå½¢çŠ¶{level_output.shape}")
        print(f"   Level {i}: åˆ†ç±»èŒƒå›´[{level_cls.min():.6f}, {level_cls.max():.6f}]")
        print(f"   Level {i}: æœ€é«˜ç½®ä¿¡åº¦{level_cls_scores.max():.6f}")
    
    print(f"\nâœ… Head biasæ£€æŸ¥å®Œæˆ")


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ å¼€å§‹æ£€æŸ¥Head biasé—®é¢˜")
    
    check_head_bias()
    
    print("\nâœ… æ£€æŸ¥å®Œæˆ")


if __name__ == '__main__':
    main()
