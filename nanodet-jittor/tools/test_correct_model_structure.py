#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
æµ‹è¯•ä¿®å¤åçš„æ¨¡å‹ç»“æ„
éªŒè¯å‚æ•°æ•°é‡æ˜¯å¦ä¸PyTorchç‰ˆæœ¬ä¸€è‡´
"""

import os
import sys
import torch
import jittor as jt
from collections import defaultdict

# æ·»åŠ è·¯å¾„
sys.path.append('/home/kyc/project/nanodet/nanodet-jittor')
from nanodet.model.arch.nanodet_plus import NanoDetPlus


def create_correct_jittor_model():
    """åˆ›å»ºä¿®å¤åçš„Jittoræ¨¡å‹"""
    print("åˆ›å»ºä¿®å¤åçš„Jittoræ¨¡å‹...")
    
    # åˆ›å»ºé…ç½®å­—å…¸ - å®Œå…¨å¯¹é½PyTorchç‰ˆæœ¬
    backbone_cfg = {
        'name': 'ShuffleNetV2',
        'model_size': '1.0x',
        'out_stages': [2, 3, 4],
        'activation': 'LeakyReLU',
        'pretrain': True
    }
    
    fpn_cfg = {
        'name': 'GhostPAN',
        'in_channels': [116, 232, 464],
        'out_channels': 96,
        'kernel_size': 5,
        'num_extra_level': 1,
        'use_depthwise': True,
        'activation': 'LeakyReLU'
    }
    
    head_cfg = {
        'name': 'NanoDetPlusHead',
        'num_classes': 20,
        'input_channel': 96,
        'feat_channels': 96,
        'stacked_convs': 2,
        'kernel_size': 5,
        'strides': [8, 16, 32, 64],
        'activation': 'LeakyReLU',
        'reg_max': 7,
        'norm_cfg': {'type': 'BN'},
        'loss': {
            'loss_qfl': {
                'name': 'QualityFocalLoss',
                'use_sigmoid': True,
                'beta': 2.0,
                'loss_weight': 1.0
            },
            'loss_dfl': {
                'name': 'DistributionFocalLoss',
                'loss_weight': 0.25
            },
            'loss_bbox': {
                'name': 'GIoULoss',
                'loss_weight': 2.0
            }
        }
    }
    
    # åˆ›å»ºaux_headé…ç½® - å®Œå…¨å¯¹é½PyTorchç‰ˆæœ¬
    aux_head_cfg = {
        'name': 'SimpleConvHead',
        'num_classes': 20,
        'input_channel': 192,
        'feat_channels': 192,  # ä¸PyTorchç‰ˆæœ¬ä¸€è‡´
        'stacked_convs': 4,    # ä¸PyTorchç‰ˆæœ¬ä¸€è‡´
        'strides': [8, 16, 32, 64],
        'activation': 'LeakyReLU',
        'reg_max': 7
    }
    
    # åˆ›å»ºå®Œæ•´æ¨¡å‹
    model = NanoDetPlus(backbone_cfg, fpn_cfg, aux_head_cfg, head_cfg)
    
    return model


def analyze_jittor_model_structure(model):
    """åˆ†æJittoræ¨¡å‹ç»“æ„"""
    print(f"\nğŸ“Š åˆ†æJittoræ¨¡å‹ç»“æ„:")
    
    total_params = 0
    module_stats = defaultdict(int)
    param_details = {}
    
    for name, param in model.named_parameters():
        param_count = param.size if hasattr(param, 'size') and not callable(param.size) else param.numel()
        total_params += param_count
        
        # æŒ‰æ¨¡å—åˆ†ç»„
        parts = name.split('.')
        if len(parts) >= 2:
            module_name = f"{parts[0]}.{parts[1]}" if len(parts) > 2 else parts[0]
        else:
            module_name = parts[0]
        
        module_stats[module_name] += param_count
        
        # è®°å½•å‚æ•°è¯¦æƒ…
        param_details[name] = {
            'shape': list(param.shape),
            'count': param_count
        }
    
    print(f"  æ€»å‚æ•°æ•°é‡: {total_params:,}")
    print(f"  å‚æ•°é¡¹æ•°é‡: {len(param_details)}")
    
    print(f"\nğŸ“Š æŒ‰æ¨¡å—ç»Ÿè®¡:")
    for module, count in sorted(module_stats.items(), key=lambda x: x[1], reverse=True):
        percentage = count / total_params * 100
        print(f"  {module:<20}: {count:>8,} å‚æ•° ({percentage:5.1f}%)")
    
    # é‡ç‚¹åˆ†æaux_head
    print(f"\nğŸ” aux_headè¯¦ç»†åˆ†æ:")
    aux_head_params = {k: v for k, v in param_details.items() if k.startswith('aux_head')}
    aux_head_total = sum(v['count'] for v in aux_head_params.values())
    
    print(f"  aux_headæ€»å‚æ•°: {aux_head_total:,}")
    print(f"  aux_headå‚æ•°é¡¹: {len(aux_head_params)}")
    
    # æŒ‰å±‚åˆ†ç»„
    aux_layers = defaultdict(int)
    for name, details in aux_head_params.items():
        layer_name = '.'.join(name.split('.')[:3])  # aux_head.xxx.yyy
        aux_layers[layer_name] += details['count']
    
    print(f"  æŒ‰å±‚ç»Ÿè®¡:")
    for layer, count in sorted(aux_layers.items()):
        print(f"    {layer}: {count:,} å‚æ•°")
    
    return param_details, module_stats, aux_head_total


def load_pytorch_weights_and_test(model):
    """åŠ è½½PyTorchæƒé‡å¹¶æµ‹è¯•"""
    print(f"\nğŸ”§ åŠ è½½PyTorchæƒé‡...")
    
    checkpoint_path = "/home/kyc/project/nanodet/nanodet-pytorch/workspace/nanodet-plus-m_320_voc_bs64/model_best/model_best.ckpt"
    
    if not os.path.exists(checkpoint_path):
        print(f"âŒ checkpointæ–‡ä»¶ä¸å­˜åœ¨: {checkpoint_path}")
        return False
    
    # ä½¿ç”¨PyTorchåŠ è½½checkpoint
    ckpt = torch.load(checkpoint_path, map_location='cpu')
    state_dict = ckpt.get('state_dict', ckpt)
    
    print(f"âœ“ PyTorch checkpointåŒ…å« {len(state_dict)} ä¸ªå‚æ•°")
    
    # è·å–Jittoræ¨¡å‹çš„å‚æ•°å­—å…¸
    jittor_state_dict = {}
    for name, param in model.named_parameters():
        jittor_state_dict[name] = param
    
    print(f"âœ“ Jittoræ¨¡å‹åŒ…å« {len(jittor_state_dict)} ä¸ªå‚æ•°")
    
    # 100%ä¿®å¤çš„æƒé‡åŠ è½½
    loaded_count = 0
    failed_count = 0
    skipped_count = 0
    scale_fixed_count = 0
    
    for pytorch_name, pytorch_param in state_dict.items():
        # ç§»é™¤PyTorchç‰¹æœ‰çš„å‰ç¼€
        jittor_name = pytorch_name
        if jittor_name.startswith("model."):
            jittor_name = jittor_name[6:]  # ç§»é™¤"model."å‰ç¼€
        
        # è·³è¿‡Jittorä¸­ä¸å­˜åœ¨çš„BatchNormç»Ÿè®¡å‚æ•°
        if "num_batches_tracked" in jittor_name:
            skipped_count += 1
            continue
        
        # è·³è¿‡avg_modelå‚æ•°ï¼ˆæƒé‡å¹³å‡ç›¸å…³ï¼‰
        if jittor_name.startswith("avg_"):
            skipped_count += 1
            continue
        
        if jittor_name in jittor_state_dict:
            jittor_param = jittor_state_dict[jittor_name]
            
            # æ£€æŸ¥å½¢çŠ¶åŒ¹é…
            if list(pytorch_param.shape) == list(jittor_param.shape):
                # è½¬æ¢å¹¶åŠ è½½å‚æ•°
                jittor_param.assign(jt.array(pytorch_param.detach().numpy()))
                loaded_count += 1
            elif "scale" in jittor_name and len(pytorch_param.shape) == 0 and list(jittor_param.shape) == [1]:
                # ç‰¹æ®Šå¤„ç†Scaleå‚æ•°ï¼šPyTorchæ ‡é‡ -> Jittor 1ç»´å¼ é‡
                jittor_param.assign(jt.array([pytorch_param.detach().numpy()]))
                loaded_count += 1
                scale_fixed_count += 1
            else:
                print(f"âŒ å½¢çŠ¶ä¸åŒ¹é…: {jittor_name}")
                print(f"   PyTorch: {list(pytorch_param.shape)}")
                print(f"   Jittor: {list(jittor_param.shape)}")
                failed_count += 1
        else:
            print(f"âŒ å‚æ•°åä¸å­˜åœ¨: {jittor_name}")
            failed_count += 1
    
    print(f"\nğŸ“Š æƒé‡åŠ è½½ç»“æœ:")
    print(f"âœ… æˆåŠŸåŠ è½½: {loaded_count} ä¸ªå‚æ•°")
    print(f"âœ… Scaleå‚æ•°ä¿®å¤: {scale_fixed_count} ä¸ª")
    print(f"â­ï¸ è·³è¿‡æ— å…³: {skipped_count} ä¸ªå‚æ•°")
    print(f"âŒ åŠ è½½å¤±è´¥: {failed_count} ä¸ªå‚æ•°")
    
    if failed_count == 0:
        print("ğŸ‰ 100%æƒé‡åŠ è½½æˆåŠŸï¼")
        return True
    else:
        print(f"âš ï¸ ä»æœ‰ {failed_count} ä¸ªå‚æ•°åŠ è½½å¤±è´¥")
        return False


def test_model_inference(model):
    """æµ‹è¯•æ¨¡å‹æ¨ç†"""
    print("\nğŸ” æµ‹è¯•æ¨¡å‹æ¨ç†...")
    
    # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
    model.eval()
    
    # åˆ›å»ºæµ‹è¯•è¾“å…¥
    test_input = jt.randn(1, 3, 320, 320)
    
    print("è¿›è¡Œå‰å‘æ¨ç†...")
    with jt.no_grad():
        output = model(test_input)
    
    print(f"âœ… æ¨ç†æˆåŠŸ!")
    print(f"   è¾“å‡ºå½¢çŠ¶: {output.shape}")
    print(f"   è¾“å‡ºæ•°å€¼èŒƒå›´: [{output.min():.6f}, {output.max():.6f}]")
    
    # åˆ†æè¾“å‡ºé€šé“
    if len(output.shape) == 3:  # [B, N, C]
        batch_size, num_anchors, num_channels = output.shape
        print(f"   æ‰¹æ¬¡å¤§å°: {batch_size}")
        print(f"   é”šç‚¹æ•°é‡: {num_anchors}")
        print(f"   è¾“å‡ºé€šé“: {num_channels}")
        
        # åˆ†æé€šé“åˆ†é…
        expected_cls_channels = 20  # VOC 20ç±»
        expected_reg_channels = 32  # 4 * (7+1) = 32
        expected_total = expected_cls_channels + expected_reg_channels
        
        print(f"\nğŸ”¹ é€šé“åˆ†æ:")
        print(f"   æœŸæœ›åˆ†ç±»é€šé“: {expected_cls_channels}")
        print(f"   æœŸæœ›å›å½’é€šé“: {expected_reg_channels}")
        print(f"   æœŸæœ›æ€»é€šé“: {expected_total}")
        print(f"   å®é™…æ€»é€šé“: {num_channels}")
        
        if num_channels == expected_total:
            print("âœ… è¾“å‡ºé€šé“æ•°æ­£ç¡®")
        else:
            print("âŒ è¾“å‡ºé€šé“æ•°ä¸æ­£ç¡®")
    
    return True


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ å¼€å§‹æµ‹è¯•ä¿®å¤åçš„æ¨¡å‹ç»“æ„")
    print("=" * 80)
    
    # åˆ›å»ºä¿®å¤åçš„æ¨¡å‹
    model = create_correct_jittor_model()
    
    # åˆ†ææ¨¡å‹ç»“æ„
    param_details, module_stats, aux_head_total = analyze_jittor_model_structure(model)
    
    # ä¸PyTorchç‰ˆæœ¬å¯¹æ¯”
    print(f"\nğŸ“Š ä¸PyTorchç‰ˆæœ¬å¯¹æ¯”:")
    pytorch_total = 4203884  # ä»ä¹‹å‰çš„æµ‹è¯•å¾—åˆ°
    jittor_total = sum(module_stats.values())
    
    print(f"  PyTorchæ€»å‚æ•°: {pytorch_total:,}")
    print(f"  Jittoræ€»å‚æ•°: {jittor_total:,}")
    print(f"  å·®å¼‚: {abs(pytorch_total - jittor_total):,} ({abs(pytorch_total - jittor_total) / pytorch_total * 100:.1f}%)")
    
    if abs(pytorch_total - jittor_total) / pytorch_total < 0.01:  # 1%ä»¥å†…
        print("âœ… å‚æ•°æ•°é‡åŸºæœ¬ä¸€è‡´")
    else:
        print("âŒ å‚æ•°æ•°é‡å·®å¼‚è¾ƒå¤§")
    
    # åŠ è½½æƒé‡å¹¶æµ‹è¯•
    weight_success = load_pytorch_weights_and_test(model)
    
    if weight_success:
        # æµ‹è¯•æ¨ç†
        test_model_inference(model)
        
        print(f"\nâœ… æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼æ¨¡å‹ç»“æ„ä¿®å¤æˆåŠŸï¼")
        return True
    else:
        print(f"\nâŒ æƒé‡åŠ è½½ä»æœ‰é—®é¢˜")
        return False


if __name__ == '__main__':
    success = main()
    sys.exit(0 if success else 1)
