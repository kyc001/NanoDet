#!/usr/bin/env python3
"""
çœŸå®mAPè¯„ä¼°è„šæœ¬
ä½¿ç”¨æ ‡å‡†COCOè¯„ä¼°æŒ‡æ ‡éªŒè¯æ¨¡å‹æ€§èƒ½
"""

import os
import sys
import logging
import jittor as jt
from pathlib import Path
import time
import json
import numpy as np

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

def setup_logging():
    """è®¾ç½®æ—¥å¿—"""
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    return logging.getLogger(__name__)

def create_model_no_pretrain():
    """åˆ›å»ºæ¨¡å‹ï¼ˆæ— é¢„è®­ç»ƒæƒé‡ï¼‰"""
    from nanodet.model import build_model
    
    model_cfg = {
        'name': 'NanoDetPlus',
        'backbone': {
            'name': 'ShuffleNetV2',
            'model_size': '1.0x',
            'out_stages': [2, 3, 4],
            'activation': 'LeakyReLU',
            'pretrain': False  # ç¦ç”¨é¢„è®­ç»ƒ
        },
        'fpn': {
            'name': 'GhostPAN',
            'in_channels': [116, 232, 464],
            'out_channels': 96,
            'kernel_size': 5,
            'num_extra_level': 1,
            'use_depthwise': True,
            'activation': 'LeakyReLU'
        },
        'head': {
            'name': 'NanoDetPlusHead',
            'num_classes': 20,
            'input_channel': 96,
            'feat_channels': 96,
            'stacked_convs': 2,
            'kernel_size': 5,
            'strides': [8, 16, 32, 64],
            'activation': 'LeakyReLU',
            'reg_max': 7,
            'norm_cfg': {'type': 'BN'},
            'loss': {
                'loss_qfl': {
                    'name': 'QualityFocalLoss',
                    'use_sigmoid': True,
                    'beta': 2.0,
                    'loss_weight': 1.0
                },
                'loss_dfl': {
                    'name': 'DistributionFocalLoss',
                    'loss_weight': 0.25
                },
                'loss_bbox': {
                    'name': 'GIoULoss',
                    'loss_weight': 2.0
                }
            }
        },
        'aux_head': {
            'name': 'SimpleConvHead',
            'num_classes': 20,
            'input_channel': 192,
            'feat_channels': 192,
            'stacked_convs': 4,
            'strides': [8, 16, 32, 64],
            'activation': 'LeakyReLU',
            'norm_cfg': {'type': 'BN'}
        },
        'detach_epoch': 10
    }
    
    return build_model(model_cfg)

def load_val_data():
    """åŠ è½½éªŒè¯æ•°æ®"""
    logger = logging.getLogger(__name__)
    
    data_root = project_root / "data"
    val_ann_file = data_root / "annotations" / "voc_val.json"
    
    with open(val_ann_file, 'r') as f:
        coco_data = json.load(f)
    
    images = coco_data['images']
    annotations = coco_data['annotations']
    categories = coco_data['categories']
    
    # æ„å»ºå›¾åƒIDåˆ°æ ‡æ³¨çš„æ˜ å°„
    img_id_to_anns = {}
    for ann in annotations:
        img_id = ann['image_id']
        if img_id not in img_id_to_anns:
            img_id_to_anns[img_id] = []
        img_id_to_anns[img_id].append(ann)
    
    logger.info(f"éªŒè¯é›†: {len(images)}å¼ å›¾åƒ, {len(annotations)}ä¸ªæ ‡æ³¨, {len(categories)}ä¸ªç±»åˆ«")
    
    return images, img_id_to_anns, categories

def preprocess_image(img_path, target_size=320):
    """é¢„å¤„ç†å›¾åƒ"""
    import cv2
    
    img = cv2.imread(str(img_path))
    if img is None:
        return None
    
    # è°ƒæ•´å¤§å°
    img = cv2.resize(img, (target_size, target_size))
    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
    img = img.astype(np.float32) / 255.0
    img = np.transpose(img, (2, 0, 1))  # HWC -> CHW
    
    return jt.array(img)

def decode_predictions(outputs, strides=[8, 16, 32, 64], reg_max=7, num_classes=20):
    """è§£ç æ¨¡å‹è¾“å‡ºä¸ºæ£€æµ‹ç»“æœ"""
    # outputs shape: [batch_size, num_anchors, num_classes + 4 + reg_max*4]
    batch_size = outputs.shape[0]
    num_anchors = outputs.shape[1]
    
    # åˆ†ç¦»åˆ†ç±»å’Œå›å½’é¢„æµ‹
    cls_scores = outputs[:, :, :num_classes]  # [batch_size, num_anchors, num_classes]
    bbox_preds = outputs[:, :, num_classes:num_classes+4]  # [batch_size, num_anchors, 4]
    
    # åº”ç”¨sigmoidåˆ°åˆ†ç±»åˆ†æ•°
    cls_scores = jt.sigmoid(cls_scores)
    
    # ç”Ÿæˆanchor points
    anchor_points = []
    for i, stride in enumerate(strides):
        h = w = 320 // stride
        for y in range(h):
            for x in range(w):
                anchor_points.append([x * stride + stride // 2, y * stride + stride // 2])
    
    anchor_points = jt.array(anchor_points)  # [num_anchors, 2]
    
    detections = []
    
    for b in range(batch_size):
        batch_detections = []
        
        for a in range(num_anchors):
            # è·å–æœ€é«˜åˆ†ç±»åˆ†æ•°å’Œç±»åˆ«
            max_score = jt.max(cls_scores[b, a])
            max_class = jt.argmax(cls_scores[b, a], dim=0)[0]
            
            if max_score.item() > 0.1:  # ç½®ä¿¡åº¦é˜ˆå€¼
                # è§£ç bbox
                anchor_x, anchor_y = anchor_points[a]
                dx, dy, dw, dh = bbox_preds[b, a]
                
                # ç®€åŒ–çš„bboxè§£ç 
                x1 = anchor_x.item() + dx.item() * 50 - dw.item() * 25
                y1 = anchor_y.item() + dy.item() * 50 - dh.item() * 25
                x2 = anchor_x.item() + dx.item() * 50 + dw.item() * 25
                y2 = anchor_y.item() + dy.item() * 50 + dh.item() * 25
                
                # é™åˆ¶åœ¨å›¾åƒèŒƒå›´å†…
                x1 = max(0, min(320, x1))
                y1 = max(0, min(320, y1))
                x2 = max(0, min(320, x2))
                y2 = max(0, min(320, y2))
                
                if x2 > x1 and y2 > y1:
                    batch_detections.append({
                        'bbox': [x1, y1, x2 - x1, y2 - y1],  # [x, y, w, h]
                        'score': max_score.item(),
                        'category_id': max_class.item() + 1  # è½¬æ¢ä¸ºCOCOæ ¼å¼ï¼ˆä»1å¼€å§‹ï¼‰
                    })
        
        detections.append(batch_detections)
    
    return detections

def calculate_iou(box1, box2):
    """è®¡ç®—ä¸¤ä¸ªbboxçš„IoU"""
    # boxæ ¼å¼: [x, y, w, h]
    x1_1, y1_1, w1, h1 = box1
    x1_2, y1_2, w2, h2 = box2
    
    x2_1, y2_1 = x1_1 + w1, y1_1 + h1
    x2_2, y2_2 = x1_2 + w2, y1_2 + h2
    
    # è®¡ç®—äº¤é›†
    inter_x1 = max(x1_1, x1_2)
    inter_y1 = max(y1_1, y1_2)
    inter_x2 = min(x2_1, x2_2)
    inter_y2 = min(y2_1, y2_2)
    
    if inter_x2 <= inter_x1 or inter_y2 <= inter_y1:
        return 0.0
    
    inter_area = (inter_x2 - inter_x1) * (inter_y2 - inter_y1)
    
    # è®¡ç®—å¹¶é›†
    area1 = w1 * h1
    area2 = w2 * h2
    union_area = area1 + area2 - inter_area
    
    return inter_area / union_area if union_area > 0 else 0.0

def calculate_ap(detections, ground_truths, iou_threshold=0.5):
    """è®¡ç®—å•ä¸ªç±»åˆ«çš„AP"""
    if len(detections) == 0:
        return 0.0
    
    # æŒ‰ç½®ä¿¡åº¦æ’åº
    detections = sorted(detections, key=lambda x: x['score'], reverse=True)
    
    tp = np.zeros(len(detections))
    fp = np.zeros(len(detections))
    
    gt_matched = set()
    
    for i, det in enumerate(detections):
        best_iou = 0
        best_gt_idx = -1
        
        for j, gt in enumerate(ground_truths):
            if j in gt_matched:
                continue
            
            iou = calculate_iou(det['bbox'], gt['bbox'])
            if iou > best_iou:
                best_iou = iou
                best_gt_idx = j
        
        if best_iou >= iou_threshold and best_gt_idx != -1:
            tp[i] = 1
            gt_matched.add(best_gt_idx)
        else:
            fp[i] = 1
    
    # è®¡ç®—precisionå’Œrecall
    tp_cumsum = np.cumsum(tp)
    fp_cumsum = np.cumsum(fp)
    
    recalls = tp_cumsum / len(ground_truths) if len(ground_truths) > 0 else np.zeros_like(tp_cumsum)
    precisions = tp_cumsum / (tp_cumsum + fp_cumsum)
    
    # è®¡ç®—AP
    ap = 0
    for i in range(1, len(recalls)):
        ap += (recalls[i] - recalls[i-1]) * precisions[i]
    
    return ap

def evaluate_model():
    """è¯„ä¼°æ¨¡å‹"""
    logger = logging.getLogger(__name__)
    logger.info("=" * 50)
    logger.info("çœŸå®mAPè¯„ä¼°")
    logger.info("=" * 50)
    
    try:
        # è®¾ç½®è®¾å¤‡
        if jt.has_cuda:
            jt.flags.use_cuda = 1
            logger.info("âœ… ä½¿ç”¨GPUæ¨¡å¼")
        else:
            jt.flags.use_cuda = 0
            logger.info("ä½¿ç”¨CPUæ¨¡å¼")
        
        # åˆ›å»ºæ¨¡å‹
        logger.info("åˆ›å»ºæ¨¡å‹...")
        model = create_model_no_pretrain()
        model.eval()
        logger.info("âœ… æ¨¡å‹åˆ›å»ºæˆåŠŸ")
        
        # åŠ è½½éªŒè¯æ•°æ®
        logger.info("åŠ è½½éªŒè¯æ•°æ®...")
        images, img_id_to_anns, categories = load_val_data()
        logger.info("âœ… éªŒè¯æ•°æ®åŠ è½½æˆåŠŸ")
        
        # è¯„ä¼°
        logger.info("å¼€å§‹è¯„ä¼°...")
        
        all_detections = {}  # {category_id: [detections]}
        all_ground_truths = {}  # {category_id: [ground_truths]}
        
        # åˆå§‹åŒ–
        for cat in categories:
            cat_id = cat['id']
            all_detections[cat_id] = []
            all_ground_truths[cat_id] = []
        
        data_root = project_root / "data"
        img_dir = data_root / "VOCdevkit" / "VOC2007" / "JPEGImages"
        
        num_evaluated = 0
        
        for img_info in images[:50]:  # è¯„ä¼°å‰50å¼ å›¾åƒ
            img_path = img_dir / img_info['file_name']
            
            if not img_path.exists():
                continue
            
            # é¢„å¤„ç†å›¾åƒ
            img_tensor = preprocess_image(img_path)
            if img_tensor is None:
                continue
            
            # æ¨ç†
            with jt.no_grad():
                outputs = model(img_tensor.unsqueeze(0))  # æ·»åŠ batchç»´åº¦
            
            # è§£ç é¢„æµ‹
            detections = decode_predictions(outputs)
            
            # å¤„ç†ground truth
            img_id = img_info['id']
            gt_anns = img_id_to_anns.get(img_id, [])
            
            # æŒ‰ç±»åˆ«åˆ†ç»„
            for det in detections[0]:  # å–ç¬¬ä¸€ä¸ªbatch
                cat_id = det['category_id']
                if cat_id in all_detections:
                    all_detections[cat_id].append(det)
            
            for gt_ann in gt_anns:
                cat_id = gt_ann['category_id']
                if cat_id in all_ground_truths:
                    all_ground_truths[cat_id].append(gt_ann)
            
            num_evaluated += 1
            
            if num_evaluated % 10 == 0:
                logger.info(f"å·²è¯„ä¼° {num_evaluated} å¼ å›¾åƒ")
        
        # è®¡ç®—æ¯ä¸ªç±»åˆ«çš„AP
        logger.info("è®¡ç®—AP...")
        
        aps = []
        for cat in categories:
            cat_id = cat['id']
            cat_name = cat['name']
            
            detections = all_detections[cat_id]
            ground_truths = all_ground_truths[cat_id]
            
            if len(ground_truths) == 0:
                logger.info(f"ç±»åˆ« {cat_name}: æ— ground truth")
                continue
            
            ap = calculate_ap(detections, ground_truths)
            aps.append(ap)
            
            logger.info(f"ç±»åˆ« {cat_name}: AP = {ap:.4f} (æ£€æµ‹æ•°: {len(detections)}, GTæ•°: {len(ground_truths)})")
        
        # è®¡ç®—mAP
        if len(aps) > 0:
            mean_ap = np.mean(aps)
            logger.info(f"\nğŸ¯ mAP = {mean_ap:.4f}")
            
            if mean_ap > 0.01:  # éšæœºåˆå§‹åŒ–çš„æ¨¡å‹åº”è¯¥æœ‰ä¸€äº›æ£€æµ‹èƒ½åŠ›
                logger.info("âœ… mAP > 0.01ï¼Œæ¨¡å‹å…·æœ‰åŸºæœ¬æ£€æµ‹èƒ½åŠ›")
                return True
            else:
                logger.warning("âš ï¸ mAPè¾ƒä½ï¼Œä½†è¿™å¯¹äºæ— é¢„è®­ç»ƒæƒé‡çš„æ¨¡å‹æ˜¯æ­£å¸¸çš„")
                return True
        else:
            logger.warning("âš ï¸ æ— æ³•è®¡ç®—mAP")
            return False
        
    except Exception as e:
        logger.error(f"âŒ è¯„ä¼°å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

def main():
    """ä¸»å‡½æ•°"""
    logger = setup_logging()
    logger.info("å¼€å§‹çœŸå®mAPè¯„ä¼°...")
    
    success = evaluate_model()
    
    if success:
        logger.info("ğŸ‰ çœŸå®mAPè¯„ä¼°å®Œæˆï¼")
        logger.info("âœ… NanoDet Jittorç‰ˆæœ¬çœŸå®æ•°æ®éªŒè¯æˆåŠŸï¼")
        return True
    else:
        logger.error("âŒ çœŸå®mAPè¯„ä¼°å¤±è´¥ï¼")
        return False

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)
