#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
æ·±åº¦åˆ†æPyTorchè®­ç»ƒå¥½çš„æ¨¡å‹æ¶æ„
ç²¾ç¡®åˆ°æ¯ä¸ªå‚æ•°çš„å½¢çŠ¶ã€é€šé“æ•°ã€å±‚çº§ç»“æ„
"""

import os
import sys
import torch
import json
from collections import OrderedDict

def analyze_pytorch_model():
    """æ·±åº¦åˆ†æPyTorchæ¨¡å‹"""
    
    # åŠ è½½PyTorchæ¨¡å‹
    model_path = "/home/kyc/project/nanodet/nanodet-pytorch/workspace/nanodet-plus-m_320_voc_bs64/model_best/model_best.ckpt"
    
    print("=" * 80)
    print("ğŸ” æ·±åº¦åˆ†æPyTorch NanoDetæ¨¡å‹æ¶æ„")
    print("=" * 80)
    
    if not os.path.exists(model_path):
        print(f"âŒ æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {model_path}")
        return False
    
    # åŠ è½½checkpoint
    checkpoint = torch.load(model_path, map_location='cpu')
    state_dict = checkpoint.get('state_dict', checkpoint)
    
    print(f"âœ“ æˆåŠŸåŠ è½½æ¨¡å‹: {model_path}")
    print(f"âœ“ æ€»å‚æ•°æ•°é‡: {len(state_dict)}")
    
    # åˆ†æå‚æ•°ç»“æ„
    analysis_result = {
        "model_info": {
            "total_params": len(state_dict),
            "model_path": model_path
        },
        "backbone": {},
        "fpn": {},
        "head": {},
        "aux_head": {},
        "other": {}
    }
    
    # æŒ‰æ¨¡å—åˆ†ç±»åˆ†æ
    for param_name, param_tensor in state_dict.items():
        param_shape = list(param_tensor.shape)
        param_numel = param_tensor.numel()
        param_dtype = str(param_tensor.dtype)
        
        # å¤„ç†ä¸åŒæ•°æ®ç±»å‹
        if param_tensor.dtype in [torch.float32, torch.float64, torch.float16]:
            param_info = {
                "shape": param_shape,
                "numel": param_numel,
                "dtype": param_dtype,
                "mean": float(param_tensor.mean()),
                "std": float(param_tensor.std()),
                "min": float(param_tensor.min()),
                "max": float(param_tensor.max())
            }
        else:
            param_info = {
                "shape": param_shape,
                "numel": param_numel,
                "dtype": param_dtype,
                "mean": "N/A",
                "std": "N/A",
                "min": float(param_tensor.min()),
                "max": float(param_tensor.max())
            }
        
        # åˆ†ç±»åˆ°ä¸åŒæ¨¡å—
        if "backbone" in param_name:
            analysis_result["backbone"][param_name] = param_info
        elif "fpn" in param_name:
            analysis_result["fpn"][param_name] = param_info
        elif "head" in param_name and "aux_head" not in param_name:
            analysis_result["head"][param_name] = param_info
        elif "aux_head" in param_name:
            analysis_result["aux_head"][param_name] = param_info
        else:
            analysis_result["other"][param_name] = param_info
    
    # æ‰“å°è¯¦ç»†åˆ†æ
    print("\n" + "=" * 80)
    print("ğŸ“Š æ¨¡å—å‚æ•°ç»Ÿè®¡")
    print("=" * 80)
    
    for module_name, module_params in analysis_result.items():
        if module_name == "model_info":
            continue
            
        if module_params:
            total_params = sum(p["numel"] for p in module_params.values())
            print(f"\nğŸ”¹ {module_name.upper()}:")
            print(f"   å‚æ•°æ•°é‡: {len(module_params)}")
            print(f"   æ€»å‚æ•°é‡: {total_params:,}")
            
            # æ˜¾ç¤ºå‰5ä¸ªå‚æ•°çš„è¯¦ç»†ä¿¡æ¯
            for i, (param_name, param_info) in enumerate(module_params.items()):
                if i < 5:  # åªæ˜¾ç¤ºå‰5ä¸ª
                    print(f"   {param_name}:")
                    print(f"     å½¢çŠ¶: {param_info['shape']}")
                    print(f"     å‚æ•°é‡: {param_info['numel']:,}")
                    if isinstance(param_info['min'], float):
                        print(f"     æ•°å€¼èŒƒå›´: [{param_info['min']:.6f}, {param_info['max']:.6f}]")
                    else:
                        print(f"     æ•°å€¼èŒƒå›´: [{param_info['min']}, {param_info['max']}]")
                elif i == 5:
                    print(f"   ... è¿˜æœ‰ {len(module_params) - 5} ä¸ªå‚æ•°")
                    break
    
    # ä¿å­˜è¯¦ç»†åˆ†æç»“æœ
    output_file = "pytorch_model_analysis.json"
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(analysis_result, f, indent=2, ensure_ascii=False)
    
    print(f"\nâœ“ è¯¦ç»†åˆ†æç»“æœå·²ä¿å­˜åˆ°: {output_file}")
    
    # ç”Ÿæˆå‚æ•°æ£€æŸ¥æ¸…å•
    generate_parameter_checklist(analysis_result)
    
    return True


def generate_parameter_checklist(analysis_result):
    """ç”Ÿæˆå‚æ•°æ£€æŸ¥æ¸…å•"""
    
    print("\n" + "=" * 80)
    print("ğŸ“‹ ç”Ÿæˆå‚æ•°æ£€æŸ¥æ¸…å•")
    print("=" * 80)
    
    checklist = []
    
    for module_name, module_params in analysis_result.items():
        if module_name == "model_info":
            continue
            
        if module_params:
            checklist.append(f"\n## {module_name.upper()} æ¨¡å—å‚æ•°æ£€æŸ¥")
            checklist.append(f"æ€»å‚æ•°æ•°: {len(module_params)}")
            checklist.append("")
            
            for param_name, param_info in module_params.items():
                status = "[ ]"  # æœªæ£€æŸ¥
                checklist.append(f"{status} **{param_name}**")
                checklist.append(f"   - å½¢çŠ¶: {param_info['shape']}")
                checklist.append(f"   - å‚æ•°é‡: {param_info['numel']:,}")
                if isinstance(param_info['min'], float):
                    checklist.append(f"   - æ•°å€¼èŒƒå›´: [{param_info['min']:.6f}, {param_info['max']:.6f}]")
                else:
                    checklist.append(f"   - æ•°å€¼èŒƒå›´: [{param_info['min']}, {param_info['max']}]")
                checklist.append(f"   - Jittorå¯¹åº”: _å¾…æ£€æŸ¥_")
                checklist.append("")
    
    # ä¿å­˜æ£€æŸ¥æ¸…å•
    checklist_file = "parameter_checklist.md"
    with open(checklist_file, 'w', encoding='utf-8') as f:
        f.write("# NanoDetå‚æ•°æ£€æŸ¥æ¸…å•\n\n")
        f.write("## æ£€æŸ¥è¯´æ˜\n")
        f.write("- [ ] æœªæ£€æŸ¥\n")
        f.write("- [/] æ£€æŸ¥ä¸­\n") 
        f.write("- [x] æ£€æŸ¥é€šè¿‡\n")
        f.write("- [-] æ£€æŸ¥å¤±è´¥\n\n")
        f.write("\n".join(checklist))
    
    print(f"âœ“ å‚æ•°æ£€æŸ¥æ¸…å•å·²ç”Ÿæˆ: {checklist_file}")


def analyze_specific_layers():
    """åˆ†æç‰¹å®šå±‚çš„è¯¦ç»†ç»“æ„"""
    
    print("\n" + "=" * 80)
    print("ğŸ”¬ åˆ†æå…³é”®å±‚ç»“æ„")
    print("=" * 80)
    
    model_path = "/home/kyc/project/nanodet/nanodet-pytorch/workspace/nanodet-plus-m_320_voc_bs64/model_best/model_best.ckpt"
    checkpoint = torch.load(model_path, map_location='cpu')
    state_dict = checkpoint.get('state_dict', checkpoint)
    
    # åˆ†æbackboneç¬¬ä¸€å±‚
    print("\nğŸ”¹ Backboneç¬¬ä¸€å±‚åˆ†æ:")
    for name, param in state_dict.items():
        if "backbone.conv1" in name:
            print(f"   {name}: {list(param.shape)}")
    
    # åˆ†æheadå±‚
    print("\nğŸ”¹ Headå±‚åˆ†æ:")
    head_params = {}
    for name, param in state_dict.items():
        if "head." in name and "aux_head" not in name:
            head_params[name] = param.shape
    
    for name, shape in sorted(head_params.items()):
        print(f"   {name}: {list(shape)}")
    
    # åˆ†æè¾“å‡ºå±‚
    print("\nğŸ”¹ è¾“å‡ºå±‚åˆ†æ:")
    for name, param in state_dict.items():
        if "gfl_cls" in name or "gfl_reg" in name:
            print(f"   {name}: {list(param.shape)}")
            if len(param.shape) >= 2:
                print(f"     è¾“å…¥é€šé“: {param.shape[1]}")
                print(f"     è¾“å‡ºé€šé“: {param.shape[0]}")


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ å¼€å§‹æ·±åº¦åˆ†æPyTorch NanoDetæ¨¡å‹")
    
    success = analyze_pytorch_model()
    
    if success:
        analyze_specific_layers()
        print("\nğŸ‰ æ¨¡å‹åˆ†æå®Œæˆ!")
        print("ğŸ“‹ ä¸‹ä¸€æ­¥: ä½¿ç”¨parameter_checklist.mdé€ä¸ªæ£€æŸ¥Jittorå‚æ•°")
    else:
        print("\nâŒ æ¨¡å‹åˆ†æå¤±è´¥")
        return False
    
    return True


if __name__ == '__main__':
    success = main()
    sys.exit(0 if success else 1)
