#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
çœŸå®è®­ç»ƒæµ‹è¯•
ç”¨Jittorä»ImageNeté¢„è®­ç»ƒå¼€å§‹è®­ç»ƒ5è½®ï¼ŒéªŒè¯å®é™…å¯ç”¨æ€§å’Œè®­ç»ƒé€Ÿåº¦
"""

import os
import sys
import time
import torch
import jittor as jt
import numpy as np
from pathlib import Path

# æ·»åŠ è·¯å¾„
sys.path.append('/home/kyc/project/nanodet/nanodet-jittor')
from nanodet.model.arch.nanodet_plus import NanoDetPlus


def create_model_with_imagenet_weights():
    """åˆ›å»ºåªåŠ è½½ImageNeté¢„è®­ç»ƒæƒé‡çš„æ¨¡å‹"""
    print("ğŸ” åˆ›å»ºæ¨¡å‹å¹¶åŠ è½½ImageNeté¢„è®­ç»ƒæƒé‡")
    
    backbone_cfg = {
        'name': 'ShuffleNetV2',
        'model_size': '1.0x',
        'out_stages': [2, 3, 4],
        'activation': 'LeakyReLU',
        'pretrain': True  # åªåŠ è½½ImageNeté¢„è®­ç»ƒ
    }
    
    fpn_cfg = {
        'name': 'GhostPAN',
        'in_channels': [116, 232, 464],
        'out_channels': 96,
        'kernel_size': 5,
        'num_extra_level': 1,
        'use_depthwise': True,
        'activation': 'LeakyReLU'
    }
    
    head_cfg = {
        'name': 'NanoDetPlusHead',
        'num_classes': 20,
        'input_channel': 96,
        'feat_channels': 96,
        'stacked_convs': 2,
        'kernel_size': 5,
        'strides': [8, 16, 32, 64],
        'activation': 'LeakyReLU',
        'reg_max': 7,
        'norm_cfg': {'type': 'BN'},
        'loss': {
            'loss_qfl': {
                'name': 'QualityFocalLoss',
                'use_sigmoid': True,
                'beta': 2.0,
                'loss_weight': 1.0
            },
            'loss_dfl': {
                'name': 'DistributionFocalLoss',
                'loss_weight': 0.25
            },
            'loss_bbox': {
                'name': 'GIoULoss',
                'loss_weight': 2.0
            }
        }
    }
    
    aux_head_cfg = {
        'name': 'SimpleConvHead',
        'num_classes': 20,
        'input_channel': 192,
        'feat_channels': 192,
        'stacked_convs': 4,
        'strides': [8, 16, 32, 64],
        'activation': 'LeakyReLU',
        'reg_max': 7
    }
    
    model = NanoDetPlus(backbone_cfg, fpn_cfg, aux_head_cfg, head_cfg)
    
    print(f"âœ… æ¨¡å‹åˆ›å»ºå®Œæˆï¼ŒåªåŠ è½½äº†ImageNeté¢„è®­ç»ƒæƒé‡")
    
    return model


def create_dummy_dataset(batch_size=64, num_batches=100):
    """åˆ›å»ºæ¨¡æ‹Ÿæ•°æ®é›†ç”¨äºè®­ç»ƒæµ‹è¯•"""
    print(f"ğŸ” åˆ›å»ºæ¨¡æ‹Ÿæ•°æ®é›† (batch_size={batch_size}, num_batches={num_batches})")
    
    dataset = []
    
    for i in range(num_batches):
        # åˆ›å»ºéšæœºå›¾åƒ [batch_size, 3, 320, 320]
        images = np.random.randn(batch_size, 3, 320, 320).astype(np.float32)
        
        # åˆ›å»ºéšæœºæ ‡ç­¾ (ç®€åŒ–çš„ç›®æ ‡æ£€æµ‹æ ‡ç­¾)
        # æ¯ä¸ªå›¾åƒéšæœºç”Ÿæˆ1-5ä¸ªç›®æ ‡
        batch_targets = []
        for b in range(batch_size):
            num_objects = np.random.randint(1, 6)  # 1-5ä¸ªç›®æ ‡
            targets = []
            for obj in range(num_objects):
                # [class_id, x_center, y_center, width, height] (å½’ä¸€åŒ–åæ ‡)
                class_id = np.random.randint(0, 20)  # VOC 20ç±»
                x_center = np.random.uniform(0.1, 0.9)
                y_center = np.random.uniform(0.1, 0.9)
                width = np.random.uniform(0.05, 0.3)
                height = np.random.uniform(0.05, 0.3)
                targets.append([class_id, x_center, y_center, width, height])
            batch_targets.append(targets)
        
        dataset.append((images, batch_targets))
    
    print(f"âœ… æ•°æ®é›†åˆ›å»ºå®Œæˆ: {len(dataset)} ä¸ªæ‰¹æ¬¡")
    return dataset


def setup_optimizer(model, lr=0.001):
    """è®¾ç½®ä¼˜åŒ–å™¨"""
    print(f"ğŸ” è®¾ç½®ä¼˜åŒ–å™¨ (lr={lr})")
    
    # ä½¿ç”¨Adamä¼˜åŒ–å™¨
    optimizer = jt.optim.Adam(model.parameters(), lr=lr)
    
    print(f"âœ… ä¼˜åŒ–å™¨è®¾ç½®å®Œæˆ")
    return optimizer


def compute_simple_loss(predictions, targets):
    """è®¡ç®—ç®€åŒ–çš„æŸå¤±å‡½æ•°"""
    # è¿™é‡Œä½¿ç”¨ä¸€ä¸ªç®€åŒ–çš„æŸå¤±å‡½æ•°æ¥æµ‹è¯•è®­ç»ƒæµç¨‹
    # å®é™…é¡¹ç›®ä¸­åº”è¯¥ä½¿ç”¨å®Œæ•´çš„æ£€æµ‹æŸå¤±å‡½æ•°
    
    # predictions: [batch_size, num_anchors, num_classes + reg_dim]
    batch_size = predictions.shape[0]
    
    # åˆ†ç¦»åˆ†ç±»å’Œå›å½’é¢„æµ‹
    cls_preds = predictions[:, :, :20]  # [batch_size, num_anchors, 20]
    reg_preds = predictions[:, :, 20:]  # [batch_size, num_anchors, 32]
    
    # ç®€åŒ–çš„åˆ†ç±»æŸå¤± (ä½¿ç”¨éšæœºç›®æ ‡)
    # åˆ›å»ºéšæœºçš„åˆ†ç±»ç›®æ ‡
    cls_targets = jt.randint(0, 20, (batch_size, cls_preds.shape[1]))
    cls_targets_onehot = jt.zeros_like(cls_preds)
    for i in range(batch_size):
        for j in range(cls_preds.shape[1]):
            cls_targets_onehot[i, j, cls_targets[i, j]] = 1.0
    
    # åˆ†ç±»æŸå¤± (äº¤å‰ç†µ)
    cls_loss = jt.nn.cross_entropy_loss(cls_preds.view(-1, 20), cls_targets.view(-1))
    
    # ç®€åŒ–çš„å›å½’æŸå¤±
    reg_targets = jt.randn_like(reg_preds) * 0.1  # éšæœºå›å½’ç›®æ ‡
    reg_loss = jt.nn.mse_loss(reg_preds, reg_targets)
    
    # æ€»æŸå¤±
    total_loss = cls_loss + reg_loss
    
    return total_loss, cls_loss, reg_loss


def train_one_epoch(model, dataset, optimizer, epoch):
    """è®­ç»ƒä¸€ä¸ªepoch"""
    print(f"ğŸš€ å¼€å§‹è®­ç»ƒ Epoch {epoch}")
    
    model.train()
    
    total_loss = 0.0
    total_cls_loss = 0.0
    total_reg_loss = 0.0
    num_batches = len(dataset)
    
    epoch_start_time = time.time()
    
    for batch_idx, (images, targets) in enumerate(dataset):
        batch_start_time = time.time()
        
        # è½¬æ¢ä¸ºJittorå¼ é‡
        images_jt = jt.array(images)
        
        # å‰å‘ä¼ æ’­
        predictions = model(images_jt)
        
        # è®¡ç®—æŸå¤±
        loss, cls_loss, reg_loss = compute_simple_loss(predictions, targets)
        
        # åå‘ä¼ æ’­
        optimizer.zero_grad()
        optimizer.backward(loss)
        optimizer.step()
        
        # ç»Ÿè®¡
        total_loss += float(loss.numpy())
        total_cls_loss += float(cls_loss.numpy())
        total_reg_loss += float(reg_loss.numpy())
        
        batch_time = time.time() - batch_start_time
        
        # æ¯10ä¸ªbatchæ‰“å°ä¸€æ¬¡
        if (batch_idx + 1) % 10 == 0:
            avg_loss = total_loss / (batch_idx + 1)
            avg_cls_loss = total_cls_loss / (batch_idx + 1)
            avg_reg_loss = total_reg_loss / (batch_idx + 1)
            
            print(f"  Batch [{batch_idx+1}/{num_batches}] "
                  f"Loss: {avg_loss:.4f} (cls: {avg_cls_loss:.4f}, reg: {avg_reg_loss:.4f}) "
                  f"Time: {batch_time:.3f}s")
    
    epoch_time = time.time() - epoch_start_time
    avg_loss = total_loss / num_batches
    avg_cls_loss = total_cls_loss / num_batches
    avg_reg_loss = total_reg_loss / num_batches
    
    print(f"âœ… Epoch {epoch} å®Œæˆ:")
    print(f"  å¹³å‡æŸå¤±: {avg_loss:.4f} (cls: {avg_cls_loss:.4f}, reg: {avg_reg_loss:.4f})")
    print(f"  è®­ç»ƒæ—¶é—´: {epoch_time:.2f}s")
    print(f"  å¹³å‡batchæ—¶é—´: {epoch_time/num_batches:.3f}s")
    print(f"  è®­ç»ƒé€Ÿåº¦: {len(dataset)*dataset[0][0].shape[0]/epoch_time:.1f} samples/s")
    
    return avg_loss, epoch_time


def test_model_performance(model, test_data):
    """æµ‹è¯•æ¨¡å‹æ€§èƒ½"""
    print("ğŸ” æµ‹è¯•æ¨¡å‹æ€§èƒ½")
    
    model.eval()
    
    with jt.no_grad():
        images_jt = jt.array(test_data)
        predictions = model(images_jt)
        
        # åˆ†æè¾“å‡º
        cls_preds = predictions[:, :, :20]
        cls_scores = jt.sigmoid(cls_preds)
        
        max_conf = float(cls_scores.max().numpy())
        mean_conf = float(cls_scores.mean().numpy())
        
        # ç»Ÿè®¡ç½®ä¿¡åº¦åˆ†å¸ƒ
        cls_scores_np = cls_scores.numpy()
        high_conf_count = (cls_scores_np > 0.1).sum()
        very_high_conf_count = (cls_scores_np > 0.5).sum()
        
        print(f"  æœ€é«˜ç½®ä¿¡åº¦: {max_conf:.6f}")
        print(f"  å¹³å‡ç½®ä¿¡åº¦: {mean_conf:.6f}")
        print(f"  >0.1ç½®ä¿¡åº¦æ•°é‡: {high_conf_count}")
        print(f"  >0.5ç½®ä¿¡åº¦æ•°é‡: {very_high_conf_count}")
        
        return max_conf, mean_conf


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ å¼€å§‹çœŸå®è®­ç»ƒæµ‹è¯•")
    print("ç›®æ ‡: éªŒè¯Jittorçš„å®é™…è®­ç»ƒèƒ½åŠ›å’Œé€Ÿåº¦")
    print("=" * 80)
    
    # è®¾ç½®Jittor
    jt.flags.use_cuda = 1 if jt.has_cuda else 0
    print(f"ä½¿ç”¨è®¾å¤‡: {'CUDA' if jt.flags.use_cuda else 'CPU'}")
    
    try:
        # 1. åˆ›å»ºæ¨¡å‹
        model = create_model_with_imagenet_weights()
        
        # 2. åˆ›å»ºæ•°æ®é›†
        batch_size = 64
        num_batches = 50  # å‡å°‘æ‰¹æ¬¡æ•°ä»¥åŠ å¿«æµ‹è¯•
        dataset = create_dummy_dataset(batch_size=batch_size, num_batches=num_batches)
        
        # 3. è®¾ç½®ä¼˜åŒ–å™¨
        optimizer = setup_optimizer(model, lr=0.001)
        
        # 4. è®­ç»ƒå‰æµ‹è¯•
        print(f"\nğŸ“Š è®­ç»ƒå‰æ€§èƒ½æµ‹è¯•:")
        test_data = np.random.randn(4, 3, 320, 320).astype(np.float32)
        initial_max_conf, initial_mean_conf = test_model_performance(model, test_data)
        
        # 5. è®­ç»ƒ5ä¸ªepoch
        print(f"\nğŸš€ å¼€å§‹è®­ç»ƒ (5 epochs)")
        training_results = []
        
        total_training_start = time.time()
        
        for epoch in range(1, 6):
            avg_loss, epoch_time = train_one_epoch(model, dataset, optimizer, epoch)
            training_results.append({
                'epoch': epoch,
                'avg_loss': avg_loss,
                'epoch_time': epoch_time
            })
        
        total_training_time = time.time() - total_training_start
        
        # 6. è®­ç»ƒåæµ‹è¯•
        print(f"\nğŸ“Š è®­ç»ƒåæ€§èƒ½æµ‹è¯•:")
        final_max_conf, final_mean_conf = test_model_performance(model, test_data)
        
        # 7. ç»“æœåˆ†æ
        print(f"\nğŸ“Š è®­ç»ƒç»“æœåˆ†æ:")
        print("=" * 80)
        
        print(f"è®­ç»ƒé…ç½®:")
        print(f"  æ‰¹æ¬¡å¤§å°: {batch_size}")
        print(f"  æ€»æ‰¹æ¬¡æ•°: {num_batches * 5} (5 epochs)")
        print(f"  æ€»æ ·æœ¬æ•°: {batch_size * num_batches * 5}")
        
        print(f"\nè®­ç»ƒé€Ÿåº¦:")
        print(f"  æ€»è®­ç»ƒæ—¶é—´: {total_training_time:.2f}s")
        print(f"  å¹³å‡æ¯epochæ—¶é—´: {total_training_time/5:.2f}s")
        print(f"  å¹³å‡è®­ç»ƒé€Ÿåº¦: {batch_size * num_batches * 5 / total_training_time:.1f} samples/s")
        
        print(f"\næŸå¤±å˜åŒ–:")
        for result in training_results:
            print(f"  Epoch {result['epoch']}: Loss {result['avg_loss']:.4f}")
        
        # æ£€æŸ¥æŸå¤±æ˜¯å¦ä¸‹é™
        initial_loss = training_results[0]['avg_loss']
        final_loss = training_results[-1]['avg_loss']
        loss_improvement = (initial_loss - final_loss) / initial_loss * 100
        
        print(f"\næ€§èƒ½å˜åŒ–:")
        print(f"  è®­ç»ƒå‰æœ€é«˜ç½®ä¿¡åº¦: {initial_max_conf:.6f}")
        print(f"  è®­ç»ƒåæœ€é«˜ç½®ä¿¡åº¦: {final_max_conf:.6f}")
        conf_improvement = (final_max_conf - initial_max_conf) / initial_max_conf * 100
        print(f"  ç½®ä¿¡åº¦æ”¹å–„: {conf_improvement:+.2f}%")
        
        print(f"\nè®­ç»ƒæ•ˆæœè¯„ä¼°:")
        if loss_improvement > 0:
            print(f"  âœ… æŸå¤±ä¸‹é™: {loss_improvement:.2f}%")
        else:
            print(f"  âš ï¸ æŸå¤±æœªä¸‹é™: {loss_improvement:.2f}%")
        
        if conf_improvement > 0:
            print(f"  âœ… ç½®ä¿¡åº¦æå‡: {conf_improvement:.2f}%")
        else:
            print(f"  âš ï¸ ç½®ä¿¡åº¦æœªæå‡: {conf_improvement:.2f}%")
        
        # è®­ç»ƒé€Ÿåº¦è¯„ä¼°
        samples_per_second = batch_size * num_batches * 5 / total_training_time
        if samples_per_second > 100:
            print(f"  âœ… è®­ç»ƒé€Ÿåº¦è‰¯å¥½: {samples_per_second:.1f} samples/s")
        elif samples_per_second > 50:
            print(f"  âš ï¸ è®­ç»ƒé€Ÿåº¦ä¸€èˆ¬: {samples_per_second:.1f} samples/s")
        else:
            print(f"  âŒ è®­ç»ƒé€Ÿåº¦è¾ƒæ…¢: {samples_per_second:.1f} samples/s")
        
        # ä¿å­˜ç»“æœ
        results = {
            'training_results': training_results,
            'initial_max_conf': initial_max_conf,
            'final_max_conf': final_max_conf,
            'conf_improvement': conf_improvement,
            'loss_improvement': loss_improvement,
            'training_speed': samples_per_second,
            'total_training_time': total_training_time
        }
        
        np.save("real_training_results.npy", results)
        
        print(f"\nğŸ¯ æœ€ç»ˆç»“è®º:")
        print("=" * 80)
        
        if loss_improvement > 0 and samples_per_second > 50:
            print(f"  ğŸ¯ Jittorè®­ç»ƒæµ‹è¯•æˆåŠŸï¼")
            print(f"  ğŸ¯ æ¨¡å‹èƒ½å¤Ÿæ­£å¸¸è®­ç»ƒå¹¶æ”¶æ•›")
            print(f"  ğŸ¯ è®­ç»ƒé€Ÿåº¦: {samples_per_second:.1f} samples/s")
            print(f"  ğŸ¯ Jittoræ¡†æ¶å®Œå…¨å¯ç”¨äºå®é™…è®­ç»ƒ")
        elif loss_improvement > 0:
            print(f"  âš ï¸ Jittorè®­ç»ƒåŸºæœ¬æˆåŠŸï¼Œä½†é€Ÿåº¦éœ€è¦ä¼˜åŒ–")
            print(f"  âš ï¸ è®­ç»ƒé€Ÿåº¦: {samples_per_second:.1f} samples/s")
        else:
            print(f"  âŒ è®­ç»ƒå¯èƒ½å­˜åœ¨é—®é¢˜ï¼Œéœ€è¦è¿›ä¸€æ­¥è°ƒè¯•")
        
        print(f"\nâœ… çœŸå®è®­ç»ƒæµ‹è¯•å®Œæˆ")
        
    except Exception as e:
        print(f"âŒ è®­ç»ƒæµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()


if __name__ == '__main__':
    main()
