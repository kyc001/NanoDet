#!/usr/bin/env python3
"""
å†…å­˜ä¼˜åŒ–çš„è®­ç»ƒè„šæœ¬
ä¸“é—¨é’ˆå¯¹8GBæ˜¾å­˜è¿›è¡Œä¼˜åŒ–
"""

import os
import sys
import argparse
from pathlib import Path

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent))

import jittor as jt
from jittor import nn
import time
import yaml
from copy import deepcopy

from nanodet.util import Logger, cfg, load_config
from nanodet.trainer.task import TrainingTask
from nanodet.data.dataset import build_dataset
from nanodet.evaluator import build_evaluator
from nanodet.util.logger import NanoDetLightningLogger


def setup_memory_optimization():
    """
    è®¾ç½®å†…å­˜ä¼˜åŒ–ç¯å¢ƒ
    """
    # ğŸ”§ æ¿€è¿›çš„å†…å­˜ä¼˜åŒ–è®¾ç½®
    jt.flags.use_cuda = 1
    
    # ğŸ”§ é™åˆ¶å†…å­˜æ± å¤§å°
    os.environ['JITTOR_MEMORY_POOL'] = '1'
    os.environ['JITTOR_MEMORY_POOL_SIZE'] = '4GB'  # é™åˆ¶ä¸º4GB
    
    # ğŸ”§ å¯ç”¨å†…å­˜ä¼˜åŒ–é€‰é¡¹
    jt.flags.use_cuda_managed_allocator = 1
    
    # ğŸ”§ å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹
    os.environ['JITTOR_ENABLE_GRAD_CHECKPOINT'] = '1'
    
    # ğŸ”§ å‡å°‘ç¼–è¯‘ç¼“å­˜
    os.environ['JITTOR_CACHE_SIZE'] = '1GB'
    
    print("âœ… å†…å­˜ä¼˜åŒ–è®¾ç½®å®Œæˆ")


def optimize_config_for_memory(config_path):
    """
    ä¸ºå†…å­˜ä¼˜åŒ–è°ƒæ•´é…ç½®
    """
    with open(config_path, 'r') as f:
        config = yaml.safe_load(f)
    
    # ğŸ”§ å¼ºåˆ¶æœ€å°å†…å­˜è®¾ç½®
    if 'device' in config:
        config['device']['batchsize_per_gpu'] = 1  # æœ€å°batch size
        config['device']['workers_per_gpu'] = 1   # æœ€å°workeræ•°
        config['device']['precision'] = 16        # æ··åˆç²¾åº¦
    
    # ğŸ”§ å‡å°æ¨¡å‹å°ºå¯¸
    if 'model' in config and 'arch' in config['model']:
        arch = config['model']['arch']
        
        # å‡å°FPNé€šé“æ•°
        if 'fpn' in arch:
            arch['fpn']['out_channels'] = min(arch['fpn'].get('out_channels', 96), 64)
        
        # å‡å°Headé€šé“æ•°
        if 'head' in arch:
            arch['head']['feat_channels'] = min(arch['head'].get('feat_channels', 96), 64)
            arch['head']['input_channel'] = min(arch['head'].get('input_channel', 96), 64)
        
        # å‡å°aux_headé€šé“æ•°
        if 'aux_head' in arch:
            arch['aux_head']['feat_channels'] = min(arch['aux_head'].get('feat_channels', 96), 64)
            arch['aux_head']['input_channel'] = min(arch['aux_head'].get('input_channel', 96), 64)
    
    # ğŸ”§ å‡å°æ•°æ®å¢å¼ºå¼ºåº¦
    if 'data' in config and 'train' in config['data']:
        train_cfg = config['data']['train']
        if 'pipeline' in train_cfg:
            # å‡å°å›¾åƒå°ºå¯¸
            if 'input_size' in train_cfg:
                train_cfg['input_size'] = [256, 256]  # ä»320å‡å°åˆ°256
    
    return config


def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("config", help="train config file path")
    parser.add_argument("--local_rank", default=0, type=int, help="node rank for distributed training")
    parser.add_argument("--seed", type=int, default=None, help="random seed")
    args = parser.parse_args()

    # ğŸ”§ è®¾ç½®å†…å­˜ä¼˜åŒ–
    setup_memory_optimization()
    
    # ğŸ”§ ä¼˜åŒ–é…ç½®
    optimized_config = optimize_config_for_memory(args.config)
    
    # ä¿å­˜ä¼˜åŒ–åçš„é…ç½®
    optimized_config_path = args.config.replace('.yml', '_memory_optimized.yml')
    with open(optimized_config_path, 'w') as f:
        yaml.dump(optimized_config, f, default_flow_style=False)
    
    print(f"âœ… ä¼˜åŒ–é…ç½®å·²ä¿å­˜åˆ°: {optimized_config_path}")
    
    # åŠ è½½ä¼˜åŒ–åçš„é…ç½®
    load_config(cfg, optimized_config_path)
    
    if args.seed is not None:
        print(f"è®¾ç½®éšæœºç§å­ä¸º {args.seed}")
        jt.set_global_seed(args.seed)

    # ğŸ”§ å¼ºåˆ¶åƒåœ¾å›æ”¶
    import gc
    gc.collect()

    # æ„å»ºè®­ç»ƒå™¨
    val_dataset = build_dataset(cfg.data.val, 'val')
    evaluator = build_evaluator(cfg.evaluator, val_dataset)
    logger = NanoDetLightningLogger(cfg.save_dir)

    trainer = TrainingTask(cfg, evaluator, logger)

    print("ğŸš€ å¼€å§‹å†…å­˜ä¼˜åŒ–è®­ç»ƒ...")
    trainer.fit()


if __name__ == "__main__":
    main()
