#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
PyTorch vs Jittor ç›´æ¥å¯¹æ¯”å·¥å…·
åœ¨PyTorchç¯å¢ƒä¸­è¿è¡Œï¼Œç›´æ¥å¯¹æ¯”ä¸¤ä¸ªç‰ˆæœ¬çš„è¾“å‡º
"""

import os
import sys
import cv2
import torch
import numpy as np
import subprocess

def run_pytorch_inference():
    """è¿è¡ŒPyTorchç‰ˆæœ¬çš„æ¨ç†"""
    print("ğŸ” è¿è¡ŒPyTorchç‰ˆæœ¬æ¨ç†...")
    
    # åˆ›å»ºPyTorchæ¨ç†è„šæœ¬
    pytorch_script = """
import sys
import torch
import numpy as np
sys.path.append('/home/kyc/project/nanodet/nanodet-pytorch')

# å¯¼å…¥PyTorchç‰ˆæœ¬
from nanodet.model.arch.nanodet_plus import NanoDetPlus

def create_pytorch_model():
    backbone_cfg = {
        'name': 'ShuffleNetV2',
        'model_size': '1.0x',
        'out_stages': [2, 3, 4],
        'activation': 'LeakyReLU',
        'pretrain': False
    }
    
    fpn_cfg = {
        'name': 'GhostPAN',
        'in_channels': [116, 232, 464],
        'out_channels': 96,
        'kernel_size': 5,
        'num_extra_level': 1,
        'use_depthwise': True,
        'activation': 'LeakyReLU'
    }
    
    head_cfg = {
        'name': 'NanoDetPlusHead',
        'num_classes': 20,
        'input_channel': 96,
        'feat_channels': 96,
        'stacked_convs': 2,
        'kernel_size': 5,
        'strides': [8, 16, 32, 64],
        'activation': 'LeakyReLU',
        'reg_max': 7,
        'norm_cfg': {'type': 'BN'},
        'loss': {
            'loss_qfl': {
                'name': 'QualityFocalLoss',
                'use_sigmoid': True,
                'beta': 2.0,
                'loss_weight': 1.0
            },
            'loss_dfl': {
                'name': 'DistributionFocalLoss',
                'loss_weight': 0.25
            },
            'loss_bbox': {
                'name': 'GIoULoss',
                'loss_weight': 2.0
            }
        }
    }
    
    aux_head_cfg = {
        'name': 'SimpleConvHead',
        'num_classes': 20,
        'input_channel': 192,
        'feat_channels': 192,
        'stacked_convs': 4,
        'strides': [8, 16, 32, 64],
        'activation': 'LeakyReLU',
        'reg_max': 7
    }
    
    model = NanoDetPlus(backbone_cfg, fpn_cfg, aux_head_cfg, head_cfg)
    
    # åŠ è½½æƒé‡
    checkpoint_path = "/home/kyc/project/nanodet/nanodet-pytorch/workspace/nanodet-plus-m_320_voc_bs64/model_best/model_best.ckpt"
    ckpt = torch.load(checkpoint_path, map_location='cpu')
    state_dict = ckpt.get('state_dict', ckpt)
    
    # ç§»é™¤'model.'å‰ç¼€
    new_state_dict = {}
    for k, v in state_dict.items():
        if k.startswith('model.'):
            new_state_dict[k[6:]] = v
        else:
            new_state_dict[k] = v
    
    model.load_state_dict(new_state_dict, strict=False)
    model.eval()
    
    return model

def test_pytorch_model():
    model = create_pytorch_model()
    
    # åˆ›å»ºç›¸åŒçš„æµ‹è¯•è¾“å…¥
    np.random.seed(42)
    input_data = np.random.randn(1, 3, 320, 320).astype(np.float32)
    torch_input = torch.tensor(input_data)
    
    print(f"PyTorchè¾“å…¥: {input_data.shape}, èŒƒå›´[{input_data.min():.6f}, {input_data.max():.6f}]")
    
    with torch.no_grad():
        # åˆ†æå„ç»„ä»¶è¾“å‡º
        print("1. PyTorch Backboneè¾“å‡º:")
        backbone_features = model.backbone(torch_input)
        for i, feat in enumerate(backbone_features):
            print(f"  å±‚{i}: {feat.shape}, èŒƒå›´[{feat.min():.6f}, {feat.max():.6f}]")
        
        print("2. PyTorch FPNè¾“å‡º:")
        fpn_features = model.fpn(backbone_features)
        for i, feat in enumerate(fpn_features):
            print(f"  å±‚{i}: {feat.shape}, èŒƒå›´[{feat.min():.6f}, {feat.max():.6f}]")
        
        print("3. PyTorch Headè¾“å‡º:")
        head_output = model.head(fpn_features)
        print(f"  Headè¾“å‡º: {head_output.shape}, èŒƒå›´[{head_output.min():.6f}, {head_output.max():.6f}]")
        
        # åˆ†æåˆ†ç±»é¢„æµ‹
        cls_preds = head_output[:, :, :20]
        cls_scores = torch.sigmoid(cls_preds)
        
        max_conf = float(cls_scores.max().item())
        mean_conf = float(cls_scores.mean().item())
        
        print(f"  åˆ†ç±»é¢„æµ‹: èŒƒå›´[{cls_preds.min():.6f}, {cls_preds.max():.6f}]")
        print(f"  æœ€é«˜ç½®ä¿¡åº¦: {max_conf:.6f}")
        print(f"  å¹³å‡ç½®ä¿¡åº¦: {mean_conf:.6f}")
        
        # ç»Ÿè®¡æ£€æµ‹æ•°é‡
        for threshold in [0.01, 0.05, 0.1]:
            max_scores = torch.max(cls_scores, dim=2)[0]
            valid_detections = int((max_scores > threshold).sum().item())
            print(f"  é˜ˆå€¼{threshold}: {valid_detections}ä¸ªæ£€æµ‹")
        
        # æ£€æŸ¥å…³é”®æƒé‡
        print("4. PyTorchå…³é”®æƒé‡:")
        for name, param in model.named_parameters():
            if 'head.gfl_cls.0.bias' in name:
                weight = param.detach().numpy()
                print(f"  {name}: {weight.shape}, èŒƒå›´[{weight.min():.6f}, {weight.max():.6f}]")
                print(f"    å‰5ä¸ªå€¼: {weight[:5]}")

if __name__ == '__main__':
    test_pytorch_model()
"""
    
    # ä¿å­˜PyTorchè„šæœ¬
    with open('/tmp/pytorch_test.py', 'w') as f:
        f.write(pytorch_script)
    
    # è¿è¡ŒPyTorchè„šæœ¬
    try:
        result = subprocess.run([
            '/home/kyc/miniconda3/envs/nano/bin/python', 
            '/tmp/pytorch_test.py'
        ], 
        capture_output=True, 
        text=True, 
        timeout=120,
        cwd='/home/kyc/project/nanodet/nanodet-pytorch'
        )
        
        if result.returncode == 0:
            print("âœ… PyTorchæ¨ç†æˆåŠŸ")
            print(result.stdout)
            return result.stdout
        else:
            print("âŒ PyTorchæ¨ç†å¤±è´¥")
            print("STDERR:", result.stderr)
            return None
            
    except subprocess.TimeoutExpired:
        print("âŒ PyTorchæ¨ç†è¶…æ—¶")
        return None
    except Exception as e:
        print(f"âŒ PyTorchæ¨ç†å¼‚å¸¸: {e}")
        return None


def run_jittor_inference():
    """è¿è¡ŒJittorç‰ˆæœ¬çš„æ¨ç†"""
    print("ğŸ” è¿è¡ŒJittorç‰ˆæœ¬æ¨ç†...")
    
    # è¿™é‡Œç›´æ¥è°ƒç”¨æˆ‘ä»¬ä¹‹å‰çš„ä»£ç 
    import jittor as jt
    sys.path.append('/home/kyc/project/nanodet/nanodet-jittor')
    from nanodet.model.arch.nanodet_plus import NanoDetPlus
    
    # åˆ›å»ºJittoræ¨¡å‹
    backbone_cfg = {
        'name': 'ShuffleNetV2',
        'model_size': '1.0x',
        'out_stages': [2, 3, 4],
        'activation': 'LeakyReLU',
        'pretrain': False
    }
    
    fpn_cfg = {
        'name': 'GhostPAN',
        'in_channels': [116, 232, 464],
        'out_channels': 96,
        'kernel_size': 5,
        'num_extra_level': 1,
        'use_depthwise': True,
        'activation': 'LeakyReLU'
    }
    
    head_cfg = {
        'name': 'NanoDetPlusHead',
        'num_classes': 20,
        'input_channel': 96,
        'feat_channels': 96,
        'stacked_convs': 2,
        'kernel_size': 5,
        'strides': [8, 16, 32, 64],
        'activation': 'LeakyReLU',
        'reg_max': 7,
        'norm_cfg': {'type': 'BN'},
        'loss': {
            'loss_qfl': {
                'name': 'QualityFocalLoss',
                'use_sigmoid': True,
                'beta': 2.0,
                'loss_weight': 1.0
            },
            'loss_dfl': {
                'name': 'DistributionFocalLoss',
                'loss_weight': 0.25
            },
            'loss_bbox': {
                'name': 'GIoULoss',
                'loss_weight': 2.0
            }
        }
    }
    
    aux_head_cfg = {
        'name': 'SimpleConvHead',
        'num_classes': 20,
        'input_channel': 192,
        'feat_channels': 192,
        'stacked_convs': 4,
        'strides': [8, 16, 32, 64],
        'activation': 'LeakyReLU',
        'reg_max': 7
    }
    
    model = NanoDetPlus(backbone_cfg, fpn_cfg, aux_head_cfg, head_cfg)
    
    # åŠ è½½æƒé‡
    import torch
    checkpoint_path = "/home/kyc/project/nanodet/nanodet-pytorch/workspace/nanodet-plus-m_320_voc_bs64/model_best/model_best.ckpt"
    ckpt = torch.load(checkpoint_path, map_location='cpu')
    state_dict = ckpt.get('state_dict', ckpt)
    
    jittor_state_dict = {}
    for name, param in model.named_parameters():
        jittor_state_dict[name] = param
    
    loaded_count = 0
    total_count = 0
    
    for pytorch_name, pytorch_param in state_dict.items():
        jittor_name = pytorch_name
        if jittor_name.startswith("model."):
            jittor_name = jittor_name[6:]
        
        if "num_batches_tracked" in jittor_name or jittor_name.startswith("avg_"):
            continue
        
        if "distribution_project.project" in jittor_name:
            continue
        
        total_count += 1
        
        if jittor_name in jittor_state_dict:
            jittor_param = jittor_state_dict[jittor_name]
            
            if list(pytorch_param.shape) == list(jittor_param.shape):
                jittor_param.assign(jt.array(pytorch_param.detach().numpy()))
                loaded_count += 1
            elif "scale" in jittor_name and len(pytorch_param.shape) == 0 and list(jittor_param.shape) == [1]:
                jittor_param.assign(jt.array([pytorch_param.detach().numpy()]))
                loaded_count += 1
    
    print(f"âœ… Jittoræƒé‡åŠ è½½: {loaded_count}/{total_count}")
    model.eval()
    
    # åˆ›å»ºç›¸åŒçš„æµ‹è¯•è¾“å…¥
    np.random.seed(42)
    input_data = np.random.randn(1, 3, 320, 320).astype(np.float32)
    jittor_input = jt.array(input_data)
    
    print(f"Jittorè¾“å…¥: {input_data.shape}, èŒƒå›´[{input_data.min():.6f}, {input_data.max():.6f}]")
    
    with jt.no_grad():
        # åˆ†æå„ç»„ä»¶è¾“å‡º
        print("1. Jittor Backboneè¾“å‡º:")
        backbone_features = model.backbone(jittor_input)
        for i, feat in enumerate(backbone_features):
            print(f"  å±‚{i}: {feat.shape}, èŒƒå›´[{feat.min():.6f}, {feat.max():.6f}]")
        
        print("2. Jittor FPNè¾“å‡º:")
        fpn_features = model.fpn(backbone_features)
        for i, feat in enumerate(fpn_features):
            print(f"  å±‚{i}: {feat.shape}, èŒƒå›´[{feat.min():.6f}, {feat.max():.6f}]")
        
        print("3. Jittor Headè¾“å‡º:")
        head_output = model.head(fpn_features)
        print(f"  Headè¾“å‡º: {head_output.shape}, èŒƒå›´[{head_output.min():.6f}, {head_output.max():.6f}]")
        
        # åˆ†æåˆ†ç±»é¢„æµ‹
        cls_preds = head_output[:, :, :20]
        cls_scores = jt.sigmoid(cls_preds)
        
        max_conf = float(cls_scores.max().numpy())
        mean_conf = float(cls_scores.mean().numpy())
        
        print(f"  åˆ†ç±»é¢„æµ‹: èŒƒå›´[{cls_preds.min():.6f}, {cls_preds.max():.6f}]")
        print(f"  æœ€é«˜ç½®ä¿¡åº¦: {max_conf:.6f}")
        print(f"  å¹³å‡ç½®ä¿¡åº¦: {mean_conf:.6f}")
        
        # ç»Ÿè®¡æ£€æµ‹æ•°é‡
        for threshold in [0.01, 0.05, 0.1]:
            max_scores = jt.max(cls_scores, dim=2)[0]
            valid_detections = int((max_scores > threshold).sum().numpy())
            print(f"  é˜ˆå€¼{threshold}: {valid_detections}ä¸ªæ£€æµ‹")
        
        # æ£€æŸ¥å…³é”®æƒé‡
        print("4. Jittorå…³é”®æƒé‡:")
        for name, param in model.named_parameters():
            if 'head.gfl_cls.0.bias' in name:
                weight = param.numpy()
                print(f"  {name}: {weight.shape}, èŒƒå›´[{weight.min():.6f}, {weight.max():.6f}]")
                print(f"    å‰5ä¸ªå€¼: {weight[:5]}")
    
    return max_conf


def compare_results(pytorch_output, jittor_max_conf):
    """å¯¹æ¯”ç»“æœ"""
    print("\nğŸ“Š PyTorch vs Jittor å¯¹æ¯”ç»“æœ:")
    print("=" * 80)
    
    if pytorch_output is None:
        print("âŒ æ— æ³•è·å–PyTorchç»“æœè¿›è¡Œå¯¹æ¯”")
        return
    
    # ä»PyTorchè¾“å‡ºä¸­æå–å…³é”®ä¿¡æ¯
    lines = pytorch_output.split('\n')
    pytorch_max_conf = None
    
    for line in lines:
        if 'æœ€é«˜ç½®ä¿¡åº¦:' in line:
            try:
                pytorch_max_conf = float(line.split(':')[1].strip())
                break
            except:
                pass
    
    if pytorch_max_conf is not None:
        print(f"PyTorchæœ€é«˜ç½®ä¿¡åº¦: {pytorch_max_conf:.6f}")
        print(f"Jittoræœ€é«˜ç½®ä¿¡åº¦: {jittor_max_conf:.6f}")
        
        diff = abs(pytorch_max_conf - jittor_max_conf)
        relative_diff = diff / pytorch_max_conf * 100 if pytorch_max_conf > 0 else 0
        
        print(f"ç»å¯¹å·®å¼‚: {diff:.6f}")
        print(f"ç›¸å¯¹å·®å¼‚: {relative_diff:.2f}%")
        
        if relative_diff < 1:
            print("âœ… ç»“æœé«˜åº¦ä¸€è‡´")
        elif relative_diff < 5:
            print("âš ï¸ ç»“æœåŸºæœ¬ä¸€è‡´")
        else:
            print("âŒ ç»“æœå­˜åœ¨æ˜¾è‘—å·®å¼‚")
    else:
        print("âš ï¸ æ— æ³•ä»PyTorchè¾“å‡ºä¸­æå–ç½®ä¿¡åº¦ä¿¡æ¯")


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ å¼€å§‹PyTorch vs Jittor ç›´æ¥å¯¹æ¯”")
    print("=" * 80)
    
    try:
        # 1. è¿è¡ŒPyTorchæ¨ç†
        pytorch_output = run_pytorch_inference()
        
        print("\n" + "="*80 + "\n")
        
        # 2. è¿è¡ŒJittoræ¨ç†
        jittor_max_conf = run_jittor_inference()
        
        # 3. å¯¹æ¯”ç»“æœ
        compare_results(pytorch_output, jittor_max_conf)
        
        print(f"\nâœ… å¯¹æ¯”å®Œæˆ")
        
    except Exception as e:
        print(f"âŒ å¯¹æ¯”å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()


if __name__ == '__main__':
    main()
