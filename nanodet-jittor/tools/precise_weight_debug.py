#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
ç²¾ç¡®çš„æƒé‡è°ƒè¯•å·¥å…·
é€æ­¥éªŒè¯æƒé‡åŠ è½½è¿‡ç¨‹ï¼Œç¡®ä¿å¾®è°ƒæƒé‡æ­£ç¡®åŠ è½½
"""

import os
import sys
import torch
import jittor as jt
import numpy as np

# æ·»åŠ è·¯å¾„
sys.path.append('/home/kyc/project/nanodet/nanodet-jittor')
from nanodet.model.arch.nanodet_plus import NanoDetPlus


def create_model_step_by_step():
    """é€æ­¥åˆ›å»ºæ¨¡å‹ï¼Œç›‘æ§æ¯ä¸ªæ­¥éª¤"""
    print("ğŸ” é€æ­¥åˆ›å»ºæ¨¡å‹å¹¶ç›‘æ§æƒé‡å˜åŒ–")
    print("=" * 60)
    
    # 1. åˆ›å»ºæ¨¡å‹é…ç½®
    backbone_cfg = {
        'name': 'ShuffleNetV2',
        'model_size': '1.0x',
        'out_stages': [2, 3, 4],
        'activation': 'LeakyReLU',
        'pretrain': False  # æ˜ç¡®ä¸åŠ è½½ImageNeté¢„è®­ç»ƒ
    }
    
    fpn_cfg = {
        'name': 'GhostPAN',
        'in_channels': [116, 232, 464],
        'out_channels': 96,
        'kernel_size': 5,
        'num_extra_level': 1,
        'use_depthwise': True,
        'activation': 'LeakyReLU'
    }
    
    head_cfg = {
        'name': 'NanoDetPlusHead',
        'num_classes': 20,
        'input_channel': 96,
        'feat_channels': 96,
        'stacked_convs': 2,
        'kernel_size': 5,
        'strides': [8, 16, 32, 64],
        'activation': 'LeakyReLU',
        'reg_max': 7,
        'norm_cfg': {'type': 'BN'},
        'loss': {
            'loss_qfl': {
                'name': 'QualityFocalLoss',
                'use_sigmoid': True,
                'beta': 2.0,
                'loss_weight': 1.0
            },
            'loss_dfl': {
                'name': 'DistributionFocalLoss',
                'loss_weight': 0.25
            },
            'loss_bbox': {
                'name': 'GIoULoss',
                'loss_weight': 2.0
            }
        }
    }
    
    aux_head_cfg = {
        'name': 'SimpleConvHead',
        'num_classes': 20,
        'input_channel': 192,
        'feat_channels': 192,
        'stacked_convs': 4,
        'strides': [8, 16, 32, 64],
        'activation': 'LeakyReLU',
        'reg_max': 7
    }
    
    print("1. åˆ›å»ºæ¨¡å‹...")
    model = NanoDetPlus(backbone_cfg, fpn_cfg, aux_head_cfg, head_cfg)
    
    # 2. æ£€æŸ¥åˆå§‹æƒé‡
    print("2. æ£€æŸ¥åˆå§‹æƒé‡...")
    initial_weights = {}
    for name, param in model.named_parameters():
        if 'head.gfl_cls.0.bias' in name:
            initial_weights[name] = param.numpy().copy()
            print(f"  åˆå§‹ {name}: èŒƒå›´[{param.min():.6f}, {param.max():.6f}]")
            print(f"    å‰5ä¸ªå€¼: {param.numpy()[:5]}")
    
    # 3. åŠ è½½PyTorchæƒé‡
    print("3. åŠ è½½PyTorchå¾®è°ƒæƒé‡...")
    checkpoint_path = "/home/kyc/project/nanodet/nanodet-pytorch/workspace/nanodet-plus-m_320_voc_bs64/model_best/model_best.ckpt"
    ckpt = torch.load(checkpoint_path, map_location='cpu')
    state_dict = ckpt.get('state_dict', ckpt)
    
    # æ£€æŸ¥PyTorchæƒé‡
    print("4. æ£€æŸ¥PyTorchæƒé‡...")
    for pytorch_name, pytorch_param in state_dict.items():
        if 'head.gfl_cls.0.bias' in pytorch_name:
            print(f"  PyTorch {pytorch_name}: èŒƒå›´[{pytorch_param.min():.6f}, {pytorch_param.max():.6f}]")
            print(f"    å‰5ä¸ªå€¼: {pytorch_param.numpy()[:5]}")
    
    # 5. æ‰‹åŠ¨åŠ è½½æƒé‡
    print("5. æ‰‹åŠ¨åŠ è½½æƒé‡...")
    jittor_state_dict = {}
    for name, param in model.named_parameters():
        jittor_state_dict[name] = param
    
    loaded_count = 0
    total_count = 0
    
    for pytorch_name, pytorch_param in state_dict.items():
        jittor_name = pytorch_name
        if jittor_name.startswith("model."):
            jittor_name = jittor_name[6:]
        
        if "num_batches_tracked" in jittor_name or jittor_name.startswith("avg_"):
            continue
        
        if "distribution_project.project" in jittor_name:
            continue
        
        total_count += 1
        
        if jittor_name in jittor_state_dict:
            jittor_param = jittor_state_dict[jittor_name]
            
            if list(pytorch_param.shape) == list(jittor_param.shape):
                # è®°å½•åŠ è½½å‰çš„å€¼
                if 'head.gfl_cls.0.bias' in jittor_name:
                    print(f"  åŠ è½½å‰ {jittor_name}: {jittor_param.numpy()[:5]}")
                
                jittor_param.assign(jt.array(pytorch_param.detach().numpy()))
                loaded_count += 1
                
                # è®°å½•åŠ è½½åçš„å€¼
                if 'head.gfl_cls.0.bias' in jittor_name:
                    print(f"  åŠ è½½å {jittor_name}: {jittor_param.numpy()[:5]}")
                    
            elif "scale" in jittor_name and len(pytorch_param.shape) == 0 and list(jittor_param.shape) == [1]:
                jittor_param.assign(jt.array([pytorch_param.detach().numpy()]))
                loaded_count += 1
    
    print(f"âœ… æƒé‡åŠ è½½: {loaded_count}/{total_count} ({loaded_count/total_count*100:.1f}%)")
    
    # 6. éªŒè¯æƒé‡åŠ è½½ç»“æœ
    print("6. éªŒè¯æƒé‡åŠ è½½ç»“æœ...")
    for name, param in model.named_parameters():
        if 'head.gfl_cls.0.bias' in name:
            final_weights = param.numpy()
            print(f"  æœ€ç»ˆ {name}: èŒƒå›´[{param.min():.6f}, {param.max():.6f}]")
            print(f"    å‰5ä¸ªå€¼: {final_weights[:5]}")
            
            # æ£€æŸ¥æ˜¯å¦çœŸçš„æ”¹å˜äº†
            if name in initial_weights:
                diff = np.abs(final_weights - initial_weights[name]).max()
                print(f"    ä¸åˆå§‹å€¼æœ€å¤§å·®å¼‚: {diff:.6f}")
                if diff > 1e-6:
                    print(f"    âœ… æƒé‡å·²æˆåŠŸæ›´æ–°")
                else:
                    print(f"    âŒ æƒé‡æœªæ›´æ–°ï¼")
    
    model.eval()
    return model


def test_model_with_debug():
    """æµ‹è¯•æ¨¡å‹å¹¶è°ƒè¯•è¾“å‡º"""
    print("\nğŸ” æµ‹è¯•æ¨¡å‹å¹¶è°ƒè¯•è¾“å‡º")
    print("=" * 60)
    
    model = create_model_step_by_step()
    
    # åˆ›å»ºæµ‹è¯•è¾“å…¥
    np.random.seed(42)
    input_data = np.random.randn(1, 3, 320, 320).astype(np.float32)
    jittor_input = jt.array(input_data)
    
    print(f"è¾“å…¥æ•°æ®: {input_data.shape}, èŒƒå›´[{input_data.min():.6f}, {input_data.max():.6f}]")
    
    with jt.no_grad():
        # æ¨ç†
        output = model(jittor_input)
        
        print(f"\næ¨¡å‹è¾“å‡º:")
        print(f"  è¾“å‡ºå½¢çŠ¶: {output.shape}")
        print(f"  è¾“å‡ºèŒƒå›´: [{output.min():.6f}, {output.max():.6f}]")
        
        # åˆ†æåˆ†ç±»é¢„æµ‹
        cls_preds = output[:, :, :20]
        cls_scores = jt.sigmoid(cls_preds)
        
        print(f"  åˆ†ç±»é¢„æµ‹: èŒƒå›´[{cls_preds.min():.6f}, {cls_preds.max():.6f}]")
        print(f"  ç½®ä¿¡åº¦: èŒƒå›´[{cls_scores.min():.6f}, {cls_scores.max():.6f}]")
        
        # è¯¦ç»†åˆ†æç½®ä¿¡åº¦
        cls_scores_np = cls_scores.numpy()
        max_conf = cls_scores_np.max()
        mean_conf = cls_scores_np.mean()
        
        print(f"  ç½®ä¿¡åº¦ç»Ÿè®¡:")
        print(f"    æœ€é«˜: {max_conf:.6f}")
        print(f"    å‡å€¼: {mean_conf:.6f}")
        print(f"    >0.01: {(cls_scores_np > 0.01).sum()}")
        print(f"    >0.05: {(cls_scores_np > 0.05).sum()}")
        print(f"    >0.1: {(cls_scores_np > 0.1).sum()}")
        
        # åˆ†æåˆ†ç±»é¢„æµ‹çš„åˆ†å¸ƒ
        print(f"  åˆ†ç±»é¢„æµ‹åˆ†æ:")
        cls_preds_np = cls_preds.numpy()
        print(f"    æœ€å¤§å€¼: {cls_preds_np.max():.6f}")
        print(f"    æœ€å°å€¼: {cls_preds_np.min():.6f}")
        print(f"    å‡å€¼: {cls_preds_np.mean():.6f}")
        print(f"    æ ‡å‡†å·®: {cls_preds_np.std():.6f}")
        
        # æ£€æŸ¥æ˜¯å¦æ‰€æœ‰é¢„æµ‹éƒ½æ˜¯è´Ÿå€¼ï¼ˆè¿™æ˜¯æ­£å¸¸çš„ï¼‰
        positive_preds = (cls_preds_np > 0).sum()
        print(f"    æ­£å€¼é¢„æµ‹æ•°: {positive_preds}")
        
        if positive_preds == 0:
            print(f"    âœ… æ‰€æœ‰åˆ†ç±»é¢„æµ‹éƒ½æ˜¯è´Ÿå€¼ï¼ˆç¬¦åˆé¢„æœŸï¼‰")
        else:
            print(f"    âš ï¸ æœ‰æ­£å€¼é¢„æµ‹ï¼Œå¯èƒ½æœ‰é—®é¢˜")
        
        return max_conf, mean_conf


def compare_with_expected():
    """ä¸é¢„æœŸç»“æœå¯¹æ¯”"""
    print(f"\nğŸ” ä¸é¢„æœŸç»“æœå¯¹æ¯”")
    print("=" * 60)
    
    max_conf, mean_conf = test_model_with_debug()
    
    # é¢„æœŸçš„ç»“æœï¼ˆåŸºäºä¹‹å‰çš„æµ‹è¯•ï¼‰
    expected_max_conf = 0.082834  # ä¹‹å‰æµ‹è¯•çš„ç»“æœ
    
    print(f"ç»“æœå¯¹æ¯”:")
    print(f"  å½“å‰æœ€é«˜ç½®ä¿¡åº¦: {max_conf:.6f}")
    print(f"  é¢„æœŸæœ€é«˜ç½®ä¿¡åº¦: {expected_max_conf:.6f}")
    
    diff = abs(max_conf - expected_max_conf)
    print(f"  å·®å¼‚: {diff:.6f}")
    
    if diff < 0.001:
        print(f"  âœ… ç»“æœä¸€è‡´ï¼Œæƒé‡åŠ è½½æ­£ç¡®")
    elif diff < 0.01:
        print(f"  âš ï¸ ç»“æœåŸºæœ¬ä¸€è‡´ï¼Œå¯èƒ½æœ‰å°çš„å·®å¼‚")
    else:
        print(f"  âŒ ç»“æœå·®å¼‚è¾ƒå¤§ï¼Œæƒé‡åŠ è½½å¯èƒ½æœ‰é—®é¢˜")
    
    # åˆ†æå¯èƒ½çš„é—®é¢˜
    if max_conf < 0.01:
        print(f"\né—®é¢˜åˆ†æ:")
        print(f"  æœ€é«˜ç½®ä¿¡åº¦è¿‡ä½ï¼Œå¯èƒ½çš„åŸå› :")
        print(f"  1. Headçš„biasæ²¡æœ‰æ­£ç¡®åŠ è½½")
        print(f"  2. æŸäº›å…³é”®æƒé‡ç¼ºå¤±")
        print(f"  3. æ¨¡å‹æ¶æ„ä¸PyTorchä¸å®Œå…¨ä¸€è‡´")
        print(f"  4. é¢„å¤„ç†æ–¹å¼ä¸åŒ")


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ å¼€å§‹ç²¾ç¡®çš„æƒé‡è°ƒè¯•")
    print("ç›®æ ‡: ç¡®ä¿å¾®è°ƒæƒé‡æ­£ç¡®åŠ è½½ï¼Œæ‰¾å‡ºmAPä¸º0çš„åŸå› ")
    
    try:
        compare_with_expected()
        
        print(f"\nâœ… ç²¾ç¡®æƒé‡è°ƒè¯•å®Œæˆ")
        
    except Exception as e:
        print(f"âŒ è°ƒè¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()


if __name__ == '__main__':
    main()
