#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
é€å±‚è°ƒè¯•å·¥å…·
æ‰¾å‡ºPyTorchå’ŒJittoræ¨¡å‹å·®å¼‚çš„å…·ä½“å±‚
"""

import os
import sys
import torch
import jittor as jt
import numpy as np

# æ·»åŠ è·¯å¾„
sys.path.append('/home/kyc/project/nanodet/nanodet-jittor')
from nanodet.model.arch.nanodet_plus import NanoDetPlus


def create_jittor_model():
    """åˆ›å»ºJittoræ¨¡å‹"""
    backbone_cfg = {
        'name': 'ShuffleNetV2',
        'model_size': '1.0x',
        'out_stages': [2, 3, 4],
        'activation': 'LeakyReLU',
        'pretrain': True
    }
    
    fpn_cfg = {
        'name': 'GhostPAN',
        'in_channels': [116, 232, 464],
        'out_channels': 96,
        'kernel_size': 5,
        'num_extra_level': 1,
        'use_depthwise': True,
        'activation': 'LeakyReLU'
    }
    
    head_cfg = {
        'name': 'NanoDetPlusHead',
        'num_classes': 20,
        'input_channel': 96,
        'feat_channels': 96,
        'stacked_convs': 2,
        'kernel_size': 5,
        'strides': [8, 16, 32, 64],
        'activation': 'LeakyReLU',
        'reg_max': 7,
        'norm_cfg': {'type': 'BN'},
        'loss': {
            'loss_qfl': {
                'name': 'QualityFocalLoss',
                'use_sigmoid': True,
                'beta': 2.0,
                'loss_weight': 1.0
            },
            'loss_dfl': {
                'name': 'DistributionFocalLoss',
                'loss_weight': 0.25
            },
            'loss_bbox': {
                'name': 'GIoULoss',
                'loss_weight': 2.0
            }
        }
    }
    
    aux_head_cfg = {
        'name': 'SimpleConvHead',
        'num_classes': 20,
        'input_channel': 192,
        'feat_channels': 192,
        'stacked_convs': 4,
        'strides': [8, 16, 32, 64],
        'activation': 'LeakyReLU',
        'reg_max': 7
    }
    
    model = NanoDetPlus(backbone_cfg, fpn_cfg, aux_head_cfg, head_cfg)
    return model


def load_weights_to_jittor_model(model):
    """åŠ è½½æƒé‡åˆ°Jittoræ¨¡å‹"""
    checkpoint_path = "/home/kyc/project/nanodet/nanodet-pytorch/workspace/nanodet-plus-m_320_voc_bs64/model_best/model_best.ckpt"
    ckpt = torch.load(checkpoint_path, map_location='cpu')
    state_dict = ckpt.get('state_dict', ckpt)
    
    # è·å–Jittoræ¨¡å‹çš„å‚æ•°å­—å…¸
    jittor_state_dict = {}
    for name, param in model.named_parameters():
        jittor_state_dict[name] = param
    
    # æƒé‡åŠ è½½
    loaded_count = 0
    for pytorch_name, pytorch_param in state_dict.items():
        jittor_name = pytorch_name
        if jittor_name.startswith("model."):
            jittor_name = jittor_name[6:]
        
        if "num_batches_tracked" in jittor_name or jittor_name.startswith("avg_"):
            continue
        
        if "distribution_project.project" in jittor_name:
            continue
        
        if jittor_name in jittor_state_dict:
            jittor_param = jittor_state_dict[jittor_name]
            
            if list(pytorch_param.shape) == list(jittor_param.shape):
                jittor_param.assign(jt.array(pytorch_param.detach().numpy()))
                loaded_count += 1
            elif "scale" in jittor_name and len(pytorch_param.shape) == 0 and list(jittor_param.shape) == [1]:
                jittor_param.assign(jt.array([pytorch_param.detach().numpy()]))
                loaded_count += 1
    
    return model


def layer_by_layer_debug():
    """é€å±‚è°ƒè¯•"""
    print("ğŸ” å¼€å§‹é€å±‚è°ƒè¯•")
    print("=" * 60)
    
    # è®¾ç½®éšæœºç§å­
    np.random.seed(42)
    torch.manual_seed(42)
    jt.set_global_seed(42)
    
    # åŠ è½½å›ºå®šè¾“å…¥
    if os.path.exists("fixed_input_data.npy"):
        input_data = np.load("fixed_input_data.npy")
        print(f"âœ“ ä½¿ç”¨å›ºå®šè¾“å…¥: {input_data.shape}")
    else:
        input_data = np.random.randn(1, 3, 320, 320).astype(np.float32)
        print(f"âœ“ åˆ›å»ºæ–°çš„å›ºå®šè¾“å…¥: {input_data.shape}")
    
    # åˆ›å»ºJittoræ¨¡å‹
    print(f"\n1ï¸âƒ£ åˆ›å»ºJittoræ¨¡å‹...")
    jittor_model = create_jittor_model()
    jittor_model = load_weights_to_jittor_model(jittor_model)
    jittor_model.eval()
    
    # å‡†å¤‡è¾“å…¥
    jittor_input = jt.array(input_data)
    
    print(f"\n2ï¸âƒ£ é€å±‚åˆ†æJittoræ¨¡å‹...")
    
    # Backbone
    print(f"\nğŸ” Backbone (ShuffleNetV2):")
    with jt.no_grad():
        backbone_features = jittor_model.backbone(jittor_input)
    
    for i, feat in enumerate(backbone_features):
        print(f"   ç‰¹å¾{i}: {feat.shape}, èŒƒå›´[{feat.min():.6f}, {feat.max():.6f}]")
        # ä¿å­˜backboneç‰¹å¾
        np.save(f"jittor_backbone_feat_{i}.npy", feat.numpy())
    
    # FPN
    print(f"\nğŸ” FPN (GhostPAN):")
    with jt.no_grad():
        fpn_features = jittor_model.fpn(backbone_features)
    
    for i, feat in enumerate(fpn_features):
        print(f"   FPNç‰¹å¾{i}: {feat.shape}, èŒƒå›´[{feat.min():.6f}, {feat.max():.6f}]")
        # ä¿å­˜FPNç‰¹å¾
        np.save(f"jittor_fpn_feat_{i}.npy", feat.numpy())
    
    # Head
    print(f"\nğŸ” Head (NanoDetPlusHead):")
    with jt.no_grad():
        head_output = jittor_model.head(fpn_features)
    
    print(f"   Headè¾“å‡º: {head_output.shape}, èŒƒå›´[{head_output.min():.6f}, {head_output.max():.6f}]")
    
    # åˆ†æHeadè¾“å‡º
    cls_preds = head_output[:, :, :20]
    reg_preds = head_output[:, :, 20:]
    cls_scores = jt.sigmoid(cls_preds)
    
    print(f"   åˆ†ç±»é¢„æµ‹: èŒƒå›´[{cls_preds.min():.6f}, {cls_preds.max():.6f}]")
    print(f"   å›å½’é¢„æµ‹: èŒƒå›´[{reg_preds.min():.6f}, {reg_preds.max():.6f}]")
    print(f"   æœ€é«˜ç½®ä¿¡åº¦: {cls_scores.max():.6f}")
    
    # ä¿å­˜Headè¾“å‡º
    np.save("jittor_head_output.npy", head_output.numpy())
    
    # å®Œæ•´æ¨¡å‹
    print(f"\nğŸ” å®Œæ•´æ¨¡å‹:")
    with jt.no_grad():
        full_output = jittor_model(jittor_input)
    
    print(f"   å®Œæ•´è¾“å‡º: {full_output.shape}, èŒƒå›´[{full_output.min():.6f}, {full_output.max():.6f}]")
    
    # æ£€æŸ¥Headè¾“å‡ºå’Œå®Œæ•´è¾“å‡ºæ˜¯å¦ä¸€è‡´
    head_vs_full_diff = np.abs(head_output.numpy() - full_output.numpy()).max()
    print(f"   Head vs å®Œæ•´è¾“å‡ºå·®å¼‚: {head_vs_full_diff:.8f}")
    
    if head_vs_full_diff < 1e-6:
        print(f"   âœ… Headè¾“å‡ºä¸å®Œæ•´è¾“å‡ºä¸€è‡´")
    else:
        print(f"   âŒ Headè¾“å‡ºä¸å®Œæ•´è¾“å‡ºä¸ä¸€è‡´")
    
    print(f"\n3ï¸âƒ£ ä¸PyTorchå‚è€ƒå¯¹æ¯”...")
    
    # å¯¹æ¯”backboneç‰¹å¾
    backbone_diffs = []
    for i in range(len(backbone_features)):
        pytorch_file = f"pytorch_backbone_feat_{i}.npy"
        if os.path.exists(pytorch_file):
            pytorch_feat = np.load(pytorch_file)
            jittor_feat = np.load(f"jittor_backbone_feat_{i}.npy")
            
            diff = np.abs(pytorch_feat - jittor_feat)
            max_diff = diff.max()
            mean_diff = diff.mean()
            
            backbone_diffs.append(max_diff)
            print(f"   Backboneç‰¹å¾{i}å·®å¼‚: æœ€å¤§{max_diff:.6f}, å¹³å‡{mean_diff:.6f}")
        else:
            print(f"   âš ï¸ æ²¡æœ‰æ‰¾åˆ°PyTorch Backboneç‰¹å¾{i}")
    
    # å¯¹æ¯”FPNç‰¹å¾
    fpn_diffs = []
    for i in range(len(fpn_features)):
        pytorch_file = f"pytorch_fpn_feat_{i}.npy"
        if os.path.exists(pytorch_file):
            pytorch_feat = np.load(pytorch_file)
            jittor_feat = np.load(f"jittor_fpn_feat_{i}.npy")
            
            diff = np.abs(pytorch_feat - jittor_feat)
            max_diff = diff.max()
            mean_diff = diff.mean()
            
            fpn_diffs.append(max_diff)
            print(f"   FPNç‰¹å¾{i}å·®å¼‚: æœ€å¤§{max_diff:.6f}, å¹³å‡{mean_diff:.6f}")
        else:
            print(f"   âš ï¸ æ²¡æœ‰æ‰¾åˆ°PyTorch FPNç‰¹å¾{i}")
    
    # å¯¹æ¯”Headè¾“å‡º
    if os.path.exists("pytorch_head_output.npy"):
        pytorch_head = np.load("pytorch_head_output.npy")
        jittor_head = np.load("jittor_head_output.npy")
        
        diff = np.abs(pytorch_head - jittor_head)
        max_diff = diff.max()
        mean_diff = diff.mean()
        
        print(f"   Headè¾“å‡ºå·®å¼‚: æœ€å¤§{max_diff:.6f}, å¹³å‡{mean_diff:.6f}")
    else:
        print(f"   âš ï¸ æ²¡æœ‰æ‰¾åˆ°PyTorch Headè¾“å‡º")
    
    print(f"\nğŸ“Š å·®å¼‚åˆ†ææ€»ç»“:")
    if backbone_diffs:
        print(f"   Backboneæœ€å¤§å·®å¼‚: {max(backbone_diffs):.6f}")
    if fpn_diffs:
        print(f"   FPNæœ€å¤§å·®å¼‚: {max(fpn_diffs):.6f}")
    
    # åˆ¤æ–­é—®é¢˜å‡ºç°åœ¨å“ªä¸€å±‚
    if backbone_diffs and max(backbone_diffs) > 1e-3:
        print(f"   ğŸš¨ é—®é¢˜å¯èƒ½å‡ºç°åœ¨Backboneå±‚")
    elif fpn_diffs and max(fpn_diffs) > 1e-3:
        print(f"   ğŸš¨ é—®é¢˜å¯èƒ½å‡ºç°åœ¨FPNå±‚")
    else:
        print(f"   ğŸš¨ é—®é¢˜å¯èƒ½å‡ºç°åœ¨Headå±‚")
    
    print(f"\nâœ… é€å±‚è°ƒè¯•å®Œæˆ")


def create_pytorch_layer_debug_script():
    """åˆ›å»ºPyTorché€å±‚è°ƒè¯•è„šæœ¬"""
    pytorch_script = '''#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
PyTorché€å±‚è°ƒè¯•è„šæœ¬
"""

import os
import sys
import torch
import numpy as np

# æ·»åŠ PyTorchç‰ˆæœ¬è·¯å¾„
sys.path.insert(0, '/home/kyc/project/nanodet/nanodet-pytorch')

from nanodet.model.arch import build_model
from nanodet.util import cfg, load_config


def main():
    """PyTorché€å±‚è°ƒè¯•"""
    print("ğŸš€ PyTorché€å±‚è°ƒè¯•")
    
    # è®¾ç½®éšæœºç§å­
    torch.manual_seed(42)
    np.random.seed(42)
    
    # åŠ è½½é…ç½®
    config_path = "/home/kyc/project/nanodet/nanodet-pytorch/config/nanodet-plus-m_320_voc.yml"
    load_config(cfg, config_path)
    
    # åˆ›å»ºæ¨¡å‹
    model = build_model(cfg.model)
    
    # åŠ è½½æƒé‡
    checkpoint_path = "/home/kyc/project/nanodet/nanodet-pytorch/workspace/nanodet-plus-m_320_voc_bs64/model_best/model_best.ckpt"
    checkpoint = torch.load(checkpoint_path, map_location='cpu')
    state_dict = checkpoint.get('state_dict', checkpoint)
    
    # ç§»é™¤å‰ç¼€
    new_state_dict = {}
    for key, value in state_dict.items():
        new_key = key.replace('model.', '') if key.startswith('model.') else key
        new_state_dict[new_key] = value
    
    model.load_state_dict(new_state_dict, strict=False)
    model.eval()
    
    print("âœ“ PyTorchæ¨¡å‹åŠ è½½æˆåŠŸ")
    
    # åŠ è½½å›ºå®šè¾“å…¥
    if os.path.exists("fixed_input_data.npy"):
        input_data = np.load("fixed_input_data.npy")
        print("âœ“ ä½¿ç”¨å›ºå®šè¾“å…¥")
    else:
        input_data = np.random.randn(1, 3, 320, 320).astype(np.float32)
        print("âœ“ åˆ›å»ºæ–°çš„å›ºå®šè¾“å…¥")
    
    input_tensor = torch.from_numpy(input_data)
    
    print(f"\\nğŸ” é€å±‚åˆ†æPyTorchæ¨¡å‹...")
    
    # Backbone
    print(f"\\nğŸ” Backbone:")
    with torch.no_grad():
        backbone_features = model.backbone(input_tensor)
    
    for i, feat in enumerate(backbone_features):
        print(f"   ç‰¹å¾{i}: {feat.shape}, èŒƒå›´[{feat.min():.6f}, {feat.max():.6f}]")
        np.save(f"pytorch_backbone_feat_{i}.npy", feat.detach().numpy())
    
    # FPN
    print(f"\\nğŸ” FPN:")
    with torch.no_grad():
        fpn_features = model.fpn(backbone_features)
    
    for i, feat in enumerate(fpn_features):
        print(f"   FPNç‰¹å¾{i}: {feat.shape}, èŒƒå›´[{feat.min():.6f}, {feat.max():.6f}]")
        np.save(f"pytorch_fpn_feat_{i}.npy", feat.detach().numpy())
    
    # Head
    print(f"\\nğŸ” Head:")
    with torch.no_grad():
        head_output = model.head(fpn_features)
    
    print(f"   Headè¾“å‡º: {head_output.shape}, èŒƒå›´[{head_output.min():.6f}, {head_output.max():.6f}]")
    np.save("pytorch_head_output.npy", head_output.detach().numpy())
    
    # å®Œæ•´æ¨¡å‹
    print(f"\\nğŸ” å®Œæ•´æ¨¡å‹:")
    with torch.no_grad():
        full_output = model(input_tensor)
    
    print(f"   å®Œæ•´è¾“å‡º: {full_output.shape}, èŒƒå›´[{full_output.min():.6f}, {full_output.max():.6f}]")
    
    print(f"\\nâœ… PyTorché€å±‚è°ƒè¯•å®Œæˆ")


if __name__ == '__main__':
    main()
'''
    
    with open('pytorch_layer_debug.py', 'w') as f:
        f.write(pytorch_script)
    
    print("âœ“ åˆ›å»ºäº†PyTorché€å±‚è°ƒè¯•è„šæœ¬: pytorch_layer_debug.py")


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ å¼€å§‹é€å±‚è°ƒè¯•")
    
    # åˆ›å»ºPyTorchè°ƒè¯•è„šæœ¬
    create_pytorch_layer_debug_script()
    
    # è¿›è¡Œé€å±‚è°ƒè¯•
    layer_by_layer_debug()
    
    print(f"\nğŸ“ ä¸‹ä¸€æ­¥:")
    print(f"1. è¿è¡Œ: python pytorch_layer_debug.py")
    print(f"2. é‡æ–°è¿è¡Œæ­¤è„šæœ¬æŸ¥çœ‹è¯¦ç»†å¯¹æ¯”")
    
    print(f"\nâœ… é€å±‚è°ƒè¯•å®Œæˆ")


if __name__ == '__main__':
    main()
