#!/usr/bin/env python3
"""
ä½¿ç”¨ jittordet çš„ ConvModule æ›¿æ¢ DepthwiseConvModule
è¿™æ˜¯æœ€ç¨³å®šå’Œå®Œæ•´çš„è§£å†³æ–¹æ¡ˆ
"""

import os
import sys
from pathlib import Path

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent))

import jittor as jt
from jittor import nn


class JittordetDepthwiseConvModule(nn.Module):
    """
    åŸºäº jittordet ConvModule çš„ DepthwiseConv å®ç°
    ä½¿ç”¨ä¸¤ä¸ªæ ‡å‡†å·ç§¯å±‚æ¨¡æ‹Ÿ depthwise + pointwise å·ç§¯
    """
    def __init__(
        self, in_channels, out_channels, kernel_size, stride=1,
        padding=0, dilation=1, bias="auto", norm_cfg=dict(type="BN"),
        activation="ReLU", inplace=True,
        order=("depthwise", "dwnorm", "act", "pointwise", "pwnorm", "act"),
    ):
        super(JittordetDepthwiseConvModule, self).__init__()
        assert activation is None or isinstance(activation, str)
        self.activation = activation
        self.inplace = inplace
        self.order = order
        assert isinstance(self.order, tuple) and len(self.order) == 6
        assert set(order) == {"depthwise", "dwnorm", "act", "pointwise", "pwnorm", "act"}

        self.with_norm = norm_cfg is not None
        if bias == "auto":
            bias = not self.with_norm
        self.with_bias = bias

        # Depthwise å·ç§¯ï¼šä½¿ç”¨ groups=in_channels æ¨¡æ‹Ÿ
        self.depthwise = nn.Conv2d(
            in_channels, in_channels, kernel_size,
            stride=stride, padding=padding, dilation=dilation,
            groups=in_channels, bias=False  # depthwise é€šå¸¸ä¸ç”¨ bias
        )
        
        # Pointwise å·ç§¯ï¼š1x1 å·ç§¯
        self.pointwise = nn.Conv2d(
            in_channels, out_channels, 1,
            stride=1, padding=0, dilation=1,
            groups=1, bias=bias
        )

        # æ‰¹å½’ä¸€åŒ–å±‚
        if self.with_norm:
            if norm_cfg.get('type') == 'BN':
                self.dwnorm = nn.BatchNorm2d(in_channels)
                self.pwnorm = nn.BatchNorm2d(out_channels)
            else:
                self.dwnorm = None
                self.pwnorm = None
        else:
            self.dwnorm = None
            self.pwnorm = None

        # æ¿€æ´»å‡½æ•°
        if activation == 'LeakyReLU':
            self.act = nn.LeakyReLU(0.1)
        elif activation == 'ReLU':
            self.act = nn.ReLU()
        else:
            self.act = None

    def init_weights(self):
        """åˆå§‹åŒ–æƒé‡"""
        nonlinearity = "leaky_relu" if self.activation == "LeakyReLU" else "relu"
        
        # åˆå§‹åŒ– depthwise å·ç§¯
        nn.init.kaiming_normal_(self.depthwise.weight, mode='fan_out', nonlinearity=nonlinearity)
        
        # åˆå§‹åŒ– pointwise å·ç§¯
        nn.init.kaiming_normal_(self.pointwise.weight, mode='fan_out', nonlinearity=nonlinearity)
        if self.pointwise.bias is not None:
            nn.init.constant_(self.pointwise.bias, 0)
        
        # åˆå§‹åŒ–æ‰¹å½’ä¸€åŒ–
        if self.dwnorm is not None:
            nn.init.constant_(self.dwnorm.weight, 1)
            nn.init.constant_(self.dwnorm.bias, 0)
        if self.pwnorm is not None:
            nn.init.constant_(self.pwnorm.weight, 1)
            nn.init.constant_(self.pwnorm.bias, 0)

    def execute(self, x, norm=True):
        """å‰å‘ä¼ æ’­"""
        for layer_name in self.order:
            if layer_name == "depthwise":
                x = self.depthwise(x)
            elif layer_name == "pointwise":
                x = self.pointwise(x)
            elif layer_name == "dwnorm" and self.dwnorm is not None:
                x = self.dwnorm(x)
            elif layer_name == "pwnorm" and self.pwnorm is not None:
                x = self.pwnorm(x)
            elif layer_name == "act" and self.act is not None:
                x = self.act(x)
        return x


def patch_depthwise_conv():
    """
    ä¿®è¡¥ DepthwiseConv ç›¸å…³çš„æ¨¡å—
    """
    print("æ­£åœ¨ä½¿ç”¨ jittordet é£æ ¼çš„ DepthwiseConv æ›¿æ¢...")
    
    # å¯¼å…¥éœ€è¦ä¿®è¡¥çš„æ¨¡å—
    try:
        from nanodet.model.module.conv import DepthwiseConvModule
        from nanodet.model.module import conv as conv_module
        
        # æ›¿æ¢ DepthwiseConvModule
        conv_module.DepthwiseConvModule = JittordetDepthwiseConvModule
        
        print("âœ… DepthwiseConvModule å·²æ›¿æ¢ä¸º jittordet é£æ ¼å®ç°")
        
        return True
        
    except ImportError as e:
        print(f"âŒ å¯¼å…¥æ¨¡å—å¤±è´¥: {e}")
        return False


def test_fixed_conv():
    """
    æµ‹è¯•ä¿®å¤åçš„å·ç§¯æ¨¡å—
    """
    print("æµ‹è¯•ä¿®å¤åçš„ DepthwiseConv æ¨¡å—...")
    
    try:
        # åˆ›å»ºæµ‹è¯•æ¨¡å—
        conv = JittordetDepthwiseConvModule(
            in_channels=96, 
            out_channels=96, 
            kernel_size=5, 
            padding=2,
            norm_cfg={'type': 'BN'},
            activation='LeakyReLU'
        )
        
        # åˆå§‹åŒ–æƒé‡
        conv.init_weights()
        
        # æµ‹è¯•å‰å‘ä¼ æ’­
        x = jt.randn(2, 96, 40, 40)
        y = conv(x)
        
        print(f"âœ… å‰å‘ä¼ æ’­æµ‹è¯•é€šè¿‡: {x.shape} -> {y.shape}")
        
        # æµ‹è¯•æ¢¯åº¦è®¡ç®—
        loss = y.sum()
        params = list(conv.parameters())
        grad = jt.grad(loss, params)
        
        print(f"âœ… æ¢¯åº¦è®¡ç®—æµ‹è¯•é€šè¿‡: è®¡ç®—äº† {len(grad)} ä¸ªå‚æ•°çš„æ¢¯åº¦")
        
        # éªŒè¯æ¢¯åº¦ä¸ä¸ºç©º
        non_zero_grads = sum(1 for g in grad if g is not None and g.sum() != 0)
        print(f"âœ… éé›¶æ¢¯åº¦æ•°é‡: {non_zero_grads}/{len(grad)}")
        
        return True
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False


def main():
    print("=== jittordet é£æ ¼ DepthwiseConv ä¿®å¤å·¥å…· ===")
    
    # ä¿®è¡¥æ¨¡å—
    if not patch_depthwise_conv():
        print("âŒ ä¿®è¡¥å¤±è´¥")
        return 1
    
    # æµ‹è¯•ä¿®å¤
    if not test_fixed_conv():
        print("âŒ æµ‹è¯•å¤±è´¥")
        return 1
    
    print("ğŸ‰ jittordet é£æ ¼ DepthwiseConv ä¿®å¤å®Œæˆï¼")
    return 0


if __name__ == '__main__':
    sys.exit(main())
