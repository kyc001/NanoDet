#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
NanoDet Jittor Test Script
ä½¿ç”¨PyTorchè¯„ä¼°å·¥å…·è¯„ä¼°Jittoræ¨¡å‹ï¼Œç¡®ä¿è¯„ä¼°æ–¹æ³•å®Œå…¨ä¸€è‡´
ä¸PyTorchç‰ˆæœ¬ä¿æŒå®Œå…¨ä¸€è‡´çš„æ¥å£
"""

import os
import sys
import argparse
import subprocess
import json
import re

# æ·»åŠ è·¯å¾„
sys.path.append('/home/kyc/project/nanodet/nanodet-jittor')
from nanodet.util import get_logger


def parse_args():
    """è§£æå‘½ä»¤è¡Œå‚æ•° - ä¸PyTorchç‰ˆæœ¬ä¸€è‡´"""
    parser = argparse.ArgumentParser(description='NanoDet Jittor Test')
    parser.add_argument('--config', required=True, help='test config file path')
    parser.add_argument('--model', required=True, help='model checkpoint path (PyTorch format)')
    parser.add_argument('--task', default='val', choices=['val', 'test'], help='evaluation task')
    parser.add_argument('--save_result', help='save evaluation results to file')
    args = parser.parse_args()
    return args


def call_pytorch_evaluation(checkpoint_path, config_path, task='val'):
    """è°ƒç”¨PyTorchç‰ˆæœ¬çš„è¯„ä¼°å·¥å…·"""
    logger = get_logger('NanoDet')
    logger.info("Calling PyTorch evaluation tools...")
    
    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not os.path.exists(checkpoint_path):
        logger.error(f"Checkpoint file not found: {checkpoint_path}")
        return None
    
    # ä½¿ç”¨PyTorchç‰ˆæœ¬çš„é…ç½®æ–‡ä»¶
    pytorch_config = "/home/kyc/project/nanodet/nanodet-pytorch/config/nanodet-plus-m_320_voc_bs64_50epochs.yml"
    if not os.path.exists(pytorch_config):
        logger.error(f"PyTorch config file not found: {pytorch_config}")
        return None
    
    # æ„å»ºè¯„ä¼°å‘½ä»¤
    pytorch_root = "/home/kyc/project/nanodet/nanodet-pytorch"
    
    cmd = [
        "/home/kyc/miniconda3/envs/nano/bin/python",
        f"{pytorch_root}/tools/test.py",
        "--config", pytorch_config,
        "--model", checkpoint_path,
        "--task", task
    ]
    
    logger.info(f"Evaluation command: {' '.join(cmd)}")
    
    try:
        # è¿è¡Œè¯„ä¼°
        result = subprocess.run(
            cmd,
            capture_output=True,
            text=True,
            timeout=600,  # 10åˆ†é’Ÿè¶…æ—¶
            cwd=pytorch_root
        )
        
        if result.returncode == 0:
            logger.info("PyTorch evaluation completed successfully")
            
            # è§£æè¯„ä¼°ç»“æœ
            map_results = parse_evaluation_output(result.stdout)
            
            return map_results
            
        else:
            logger.error("PyTorch evaluation failed")
            logger.error(f"STDERR: {result.stderr}")
            return None
            
    except subprocess.TimeoutExpired:
        logger.error("PyTorch evaluation timeout")
        return None
    except Exception as e:
        logger.error(f"PyTorch evaluation exception: {e}")
        return None


def parse_evaluation_output(output_text):
    """è§£æè¯„ä¼°è¾“å‡ºï¼Œæå–mAPç»“æœ"""
    logger = get_logger('NanoDet')
    
    map_results = {}
    lines = output_text.split('\n')
    
    for line in lines:
        line = line.strip()
        
        if 'Average Precision' in line:
            if 'IoU=0.50:0.95' in line and 'area=   all' in line:
                match = re.search(r'= ([\d.]+)', line)
                if match:
                    map_results['mAP'] = float(match.group(1))
            
            elif 'IoU=0.50' in line and 'area=   all' in line and '0.50:0.95' not in line:
                match = re.search(r'= ([\d.]+)', line)
                if match:
                    map_results['mAP_50'] = float(match.group(1))
    
    return map_results


def main():
    """ä¸»å‡½æ•°"""
    args = parse_args()
    
    logger = get_logger('NanoDet')
    logger.info("Starting NanoDet Jittor model evaluation")
    logger.info(f"Model: {args.model}")
    logger.info(f"Task: {args.task}")
    
    # è°ƒç”¨è¯„ä¼°
    results = call_pytorch_evaluation(args.model, args.config, args.task)
    
    if results:
        logger.info("Evaluation completed successfully")
        
        # æ˜¾ç¤ºç»“æœ
        print("\n" + "="*60)
        print("ğŸ“Š Evaluation Results:")
        print("="*60)
        
        for metric, value in results.items():
            print(f"  {metric}: {value:.4f}")
        
        # ä¸PyTorchåŸºå‡†å¯¹æ¯”
        pytorch_map = 0.275
        if 'mAP' in results:
            jittor_map = results['mAP']
            relative_performance = jittor_map / pytorch_map * 100
            
            print(f"\nComparison with PyTorch baseline:")
            print(f"  PyTorch mAP: {pytorch_map:.4f}")
            print(f"  Jittor mAP:  {jittor_map:.4f}")
            print(f"  Relative:    {relative_performance:.1f}%")
        
        print("="*60)
        
        # ä¿å­˜ç»“æœ
        if args.save_result:
            os.makedirs(os.path.dirname(args.save_result), exist_ok=True)
            with open(args.save_result, 'w') as f:
                json.dump(results, f, indent=2)
            logger.info(f"Results saved to: {args.save_result}")
        
    else:
        logger.error("Evaluation failed")
        return 1
    
    return 0


if __name__ == '__main__':
    exit(main())
