#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
ç»„ä»¶çº§åˆ«è°ƒè¯•å·¥å…·
é€ä¸ªæ£€æŸ¥æ¨¡å‹ç»„ä»¶ï¼Œæ‰¾å‡ºé—®é¢˜æ ¹æº
"""

import os
import sys
import torch
import jittor as jt
import numpy as np

# æ·»åŠ è·¯å¾„
sys.path.append('/home/kyc/project/nanodet/nanodet-jittor')
from nanodet.model.arch.nanodet_plus import NanoDetPlus


def create_test_input():
    """åˆ›å»ºå›ºå®šçš„æµ‹è¯•è¾“å…¥"""
    np.random.seed(42)
    torch.manual_seed(42)
    jt.set_global_seed(42)
    
    # ä½¿ç”¨å›ºå®šçš„æµ‹è¯•æ•°æ®
    if os.path.exists("fixed_input_data.npy"):
        input_data = np.load("fixed_input_data.npy")
    else:
        input_data = np.random.randn(1, 3, 320, 320).astype(np.float32)
        np.save("fixed_input_data.npy", input_data)
    
    return input_data


def create_jittor_model():
    """åˆ›å»ºJittoræ¨¡å‹"""
    print("ğŸ” åˆ›å»ºJittoræ¨¡å‹...")
    
    backbone_cfg = {
        'name': 'ShuffleNetV2',
        'model_size': '1.0x',
        'out_stages': [2, 3, 4],
        'activation': 'LeakyReLU',
        'pretrain': True
    }
    
    fpn_cfg = {
        'name': 'GhostPAN',
        'in_channels': [116, 232, 464],
        'out_channels': 96,
        'kernel_size': 5,
        'num_extra_level': 1,
        'use_depthwise': True,
        'activation': 'LeakyReLU'
    }
    
    head_cfg = {
        'name': 'NanoDetPlusHead',
        'num_classes': 20,
        'input_channel': 96,
        'feat_channels': 96,
        'stacked_convs': 2,
        'kernel_size': 5,
        'strides': [8, 16, 32, 64],
        'activation': 'LeakyReLU',
        'reg_max': 7,
        'norm_cfg': {'type': 'BN'},
        'loss': {
            'loss_qfl': {
                'name': 'QualityFocalLoss',
                'use_sigmoid': True,
                'beta': 2.0,
                'loss_weight': 1.0
            },
            'loss_dfl': {
                'name': 'DistributionFocalLoss',
                'loss_weight': 0.25
            },
            'loss_bbox': {
                'name': 'GIoULoss',
                'loss_weight': 2.0
            }
        }
    }
    
    aux_head_cfg = {
        'name': 'SimpleConvHead',
        'num_classes': 20,
        'input_channel': 192,
        'feat_channels': 192,
        'stacked_convs': 4,
        'strides': [8, 16, 32, 64],
        'activation': 'LeakyReLU',
        'reg_max': 7
    }
    
    model = NanoDetPlus(backbone_cfg, fpn_cfg, aux_head_cfg, head_cfg)
    
    # åŠ è½½æƒé‡
    print("åŠ è½½PyTorchæƒé‡...")
    checkpoint_path = "/home/kyc/project/nanodet/nanodet-pytorch/workspace/nanodet-plus-m_320_voc_bs64/model_best/model_best.ckpt"
    ckpt = torch.load(checkpoint_path, map_location='cpu')
    state_dict = ckpt.get('state_dict', ckpt)
    
    jittor_state_dict = {}
    for name, param in model.named_parameters():
        jittor_state_dict[name] = param
    
    # æƒé‡åŠ è½½
    for pytorch_name, pytorch_param in state_dict.items():
        jittor_name = pytorch_name
        if jittor_name.startswith("model."):
            jittor_name = jittor_name[6:]
        
        if "num_batches_tracked" in jittor_name or jittor_name.startswith("avg_"):
            continue
        
        if "distribution_project.project" in jittor_name:
            continue
        
        if jittor_name in jittor_state_dict:
            jittor_param = jittor_state_dict[jittor_name]
            
            if list(pytorch_param.shape) == list(jittor_param.shape):
                jittor_param.assign(jt.array(pytorch_param.detach().numpy()))
            elif "scale" in jittor_name and len(pytorch_param.shape) == 0 and list(jittor_param.shape) == [1]:
                jittor_param.assign(jt.array([pytorch_param.detach().numpy()]))
    
    model.eval()
    return model


def test_backbone_only():
    """æµ‹è¯•ä»…Backbone"""
    print("ğŸ” æµ‹è¯•ä»…Backbone")
    print("=" * 60)
    
    input_data = create_test_input()
    jittor_input = jt.array(input_data)
    
    model = create_jittor_model()
    
    with jt.no_grad():
        backbone_features = model.backbone(jittor_input)
        
        print(f"Backboneè¾“å‡º:")
        for i, feat in enumerate(backbone_features):
            print(f"  ç‰¹å¾{i}: {feat.shape}, èŒƒå›´[{feat.min():.6f}, {feat.max():.6f}]")
            
            # æ£€æŸ¥æ˜¯å¦æœ‰å¼‚å¸¸å€¼
            if feat.max() > 100 or feat.min() < -100:
                print(f"    âš ï¸ ç‰¹å¾{i}æ•°å€¼èŒƒå›´å¼‚å¸¸")
            elif feat.max() > 10 or feat.min() < -10:
                print(f"    âš ï¸ ç‰¹å¾{i}æ•°å€¼èŒƒå›´åå¤§")
            else:
                print(f"    âœ… ç‰¹å¾{i}æ•°å€¼èŒƒå›´æ­£å¸¸")
    
    return backbone_features


def test_fpn_only(backbone_features):
    """æµ‹è¯•ä»…FPN"""
    print(f"\nğŸ” æµ‹è¯•ä»…FPN")
    print("=" * 60)
    
    model = create_jittor_model()
    
    with jt.no_grad():
        fpn_features = model.fpn(backbone_features)
        
        print(f"FPNè¾“å‡º:")
        for i, feat in enumerate(fpn_features):
            print(f"  FPNç‰¹å¾{i}: {feat.shape}, èŒƒå›´[{feat.min():.6f}, {feat.max():.6f}]")
            
            # æ£€æŸ¥æ˜¯å¦æœ‰å¼‚å¸¸å€¼
            if feat.max() > 100 or feat.min() < -100:
                print(f"    âš ï¸ FPNç‰¹å¾{i}æ•°å€¼èŒƒå›´å¼‚å¸¸")
            elif feat.max() > 10 or feat.min() < -10:
                print(f"    âš ï¸ FPNç‰¹å¾{i}æ•°å€¼èŒƒå›´åå¤§")
            else:
                print(f"    âœ… FPNç‰¹å¾{i}æ•°å€¼èŒƒå›´æ­£å¸¸")
    
    return fpn_features


def test_head_only(fpn_features):
    """æµ‹è¯•ä»…Head"""
    print(f"\nğŸ” æµ‹è¯•ä»…Head")
    print("=" * 60)
    
    model = create_jittor_model()
    
    with jt.no_grad():
        head_output = model.head(fpn_features)
        
        print(f"Headè¾“å‡º:")
        print(f"  è¾“å‡ºå½¢çŠ¶: {head_output.shape}")
        print(f"  è¾“å‡ºèŒƒå›´: [{head_output.min():.6f}, {head_output.max():.6f}]")
        
        # åˆ†æHeadè¾“å‡º
        cls_preds = head_output[:, :, :20]
        reg_preds = head_output[:, :, 20:]
        cls_scores = jt.sigmoid(cls_preds)
        
        print(f"  åˆ†ç±»é¢„æµ‹: èŒƒå›´[{cls_preds.min():.6f}, {cls_preds.max():.6f}]")
        print(f"  å›å½’é¢„æµ‹: èŒƒå›´[{reg_preds.min():.6f}, {reg_preds.max():.6f}]")
        print(f"  åˆ†ç±»ç½®ä¿¡åº¦: èŒƒå›´[{cls_scores.min():.6f}, {cls_scores.max():.6f}]")
        print(f"  æœ€é«˜ç½®ä¿¡åº¦: {cls_scores.max():.6f}")
        
        # æ£€æŸ¥Headè¾“å‡ºæ˜¯å¦æ­£å¸¸
        if cls_preds.max() > 0:
            print(f"    âš ï¸ åˆ†ç±»é¢„æµ‹æœ‰æ­£å€¼ï¼Œå¯èƒ½æœ‰é—®é¢˜")
        else:
            print(f"    âœ… åˆ†ç±»é¢„æµ‹å…¨ä¸ºè´Ÿå€¼ï¼Œç¬¦åˆé¢„æœŸ")
        
        if cls_scores.max() < 0.1:
            print(f"    âŒ æœ€é«˜ç½®ä¿¡åº¦è¿‡ä½ï¼ŒHeadæœ‰é—®é¢˜")
        elif cls_scores.max() < 0.5:
            print(f"    âš ï¸ æœ€é«˜ç½®ä¿¡åº¦åä½")
        else:
            print(f"    âœ… æœ€é«˜ç½®ä¿¡åº¦æ­£å¸¸")
    
    return head_output


def test_individual_head_layers(fpn_features):
    """æµ‹è¯•Headçš„å„ä¸ªå±‚"""
    print(f"\nğŸ” æµ‹è¯•Headçš„å„ä¸ªå±‚")
    print("=" * 60)
    
    model = create_jittor_model()
    head = model.head
    
    with jt.no_grad():
        # æµ‹è¯•cls_convs
        print(f"æµ‹è¯•åˆ†ç±»å·ç§¯å±‚:")
        cls_feat = fpn_features[0]  # ä½¿ç”¨ç¬¬ä¸€ä¸ªç‰¹å¾
        for i, conv in enumerate(head.cls_convs):
            cls_feat = conv(cls_feat)
            print(f"  cls_conv{i}: {cls_feat.shape}, èŒƒå›´[{cls_feat.min():.6f}, {cls_feat.max():.6f}]")
        
        # æµ‹è¯•reg_convs
        print(f"\næµ‹è¯•å›å½’å·ç§¯å±‚:")
        reg_feat = fpn_features[0]  # ä½¿ç”¨ç¬¬ä¸€ä¸ªç‰¹å¾
        for i, conv in enumerate(head.reg_convs):
            reg_feat = conv(reg_feat)
            print(f"  reg_conv{i}: {reg_feat.shape}, èŒƒå›´[{reg_feat.min():.6f}, {reg_feat.max():.6f}]")
        
        # æµ‹è¯•æœ€ç»ˆé¢„æµ‹å±‚
        print(f"\næµ‹è¯•æœ€ç»ˆé¢„æµ‹å±‚:")
        cls_pred = head.gfl_cls(cls_feat)
        reg_pred = head.gfl_reg(reg_feat)
        
        print(f"  cls_pred: {cls_pred.shape}, èŒƒå›´[{cls_pred.min():.6f}, {cls_pred.max():.6f}]")
        print(f"  reg_pred: {reg_pred.shape}, èŒƒå›´[{reg_pred.min():.6f}, {reg_pred.max():.6f}]")
        
        # æ£€æŸ¥é¢„æµ‹å±‚è¾“å‡º
        if cls_pred.max() > 10 or cls_pred.min() < -10:
            print(f"    âŒ åˆ†ç±»é¢„æµ‹æ•°å€¼èŒƒå›´å¼‚å¸¸")
        else:
            print(f"    âœ… åˆ†ç±»é¢„æµ‹æ•°å€¼èŒƒå›´æ­£å¸¸")
        
        if reg_pred.max() > 100 or reg_pred.min() < -100:
            print(f"    âŒ å›å½’é¢„æµ‹æ•°å€¼èŒƒå›´å¼‚å¸¸")
        else:
            print(f"    âœ… å›å½’é¢„æµ‹æ•°å€¼èŒƒå›´æ­£å¸¸")


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ å¼€å§‹ç»„ä»¶çº§åˆ«è°ƒè¯•")
    
    # 1. æµ‹è¯•Backbone
    backbone_features = test_backbone_only()
    
    # 2. æµ‹è¯•FPN
    fpn_features = test_fpn_only(backbone_features)
    
    # 3. æµ‹è¯•Head
    head_output = test_head_only(fpn_features)
    
    # 4. æµ‹è¯•Headå„ä¸ªå±‚
    test_individual_head_layers(fpn_features)
    
    print(f"\nâœ… ç»„ä»¶çº§åˆ«è°ƒè¯•å®Œæˆ")
    
    # æ€»ç»“
    print(f"\nğŸ“Š è°ƒè¯•æ€»ç»“:")
    print(f"  å¦‚æœBackboneæ­£å¸¸ï¼ŒFPNå¼‚å¸¸ -> FPNå®ç°æœ‰é—®é¢˜")
    print(f"  å¦‚æœFPNæ­£å¸¸ï¼ŒHeadå¼‚å¸¸ -> Headå®ç°æœ‰é—®é¢˜")
    print(f"  å¦‚æœæ‰€æœ‰ç»„ä»¶éƒ½æ­£å¸¸ï¼Œä½†æ•´ä½“å¼‚å¸¸ -> ç»„åˆæˆ–æƒé‡åŠ è½½æœ‰é—®é¢˜")


if __name__ == '__main__':
    main()
