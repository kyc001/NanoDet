#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
ç»Ÿä¸€çš„FPNéªŒè¯å·¥å…·
ç¡®ä¿ä¸¤æ¬¡æµ‹è¯•ä½¿ç”¨ç›¸åŒçš„è¾“å…¥å’Œæ¨¡å‹çŠ¶æ€
"""

import os
import sys
import torch
import jittor as jt
import numpy as np

# æ·»åŠ è·¯å¾„
sys.path.append('/home/kyc/project/nanodet/nanodet-jittor')
from nanodet.model.arch.nanodet_plus import NanoDetPlus


def create_test_input():
    """åˆ›å»ºå›ºå®šçš„æµ‹è¯•è¾“å…¥"""
    np.random.seed(42)
    torch.manual_seed(42)
    jt.set_global_seed(42)
    
    # ä½¿ç”¨å›ºå®šçš„æµ‹è¯•æ•°æ®
    if os.path.exists("fixed_input_data.npy"):
        input_data = np.load("fixed_input_data.npy")
    else:
        input_data = np.random.randn(1, 3, 320, 320).astype(np.float32)
        np.save("fixed_input_data.npy", input_data)
    
    return input_data


def create_jittor_model():
    """åˆ›å»ºJittoræ¨¡å‹"""
    print("ğŸ” åˆ›å»ºJittoræ¨¡å‹...")
    
    backbone_cfg = {
        'name': 'ShuffleNetV2',
        'model_size': '1.0x',
        'out_stages': [2, 3, 4],
        'activation': 'LeakyReLU',
        'pretrain': True
    }
    
    fpn_cfg = {
        'name': 'GhostPAN',
        'in_channels': [116, 232, 464],
        'out_channels': 96,
        'kernel_size': 5,
        'num_extra_level': 1,
        'use_depthwise': True,
        'activation': 'LeakyReLU'
    }
    
    head_cfg = {
        'name': 'NanoDetPlusHead',
        'num_classes': 20,
        'input_channel': 96,
        'feat_channels': 96,
        'stacked_convs': 2,
        'kernel_size': 5,
        'strides': [8, 16, 32, 64],
        'activation': 'LeakyReLU',
        'reg_max': 7,
        'norm_cfg': {'type': 'BN'},
        'loss': {
            'loss_qfl': {
                'name': 'QualityFocalLoss',
                'use_sigmoid': True,
                'beta': 2.0,
                'loss_weight': 1.0
            },
            'loss_dfl': {
                'name': 'DistributionFocalLoss',
                'loss_weight': 0.25
            },
            'loss_bbox': {
                'name': 'GIoULoss',
                'loss_weight': 2.0
            }
        }
    }
    
    aux_head_cfg = {
        'name': 'SimpleConvHead',
        'num_classes': 20,
        'input_channel': 192,
        'feat_channels': 192,
        'stacked_convs': 4,
        'strides': [8, 16, 32, 64],
        'activation': 'LeakyReLU',
        'reg_max': 7
    }
    
    model = NanoDetPlus(backbone_cfg, fpn_cfg, aux_head_cfg, head_cfg)
    
    # åŠ è½½æƒé‡
    print("åŠ è½½PyTorchæƒé‡...")
    checkpoint_path = "/home/kyc/project/nanodet/nanodet-pytorch/workspace/nanodet-plus-m_320_voc_bs64/model_best/model_best.ckpt"
    ckpt = torch.load(checkpoint_path, map_location='cpu')
    state_dict = ckpt.get('state_dict', ckpt)
    
    jittor_state_dict = {}
    for name, param in model.named_parameters():
        jittor_state_dict[name] = param
    
    # æƒé‡åŠ è½½
    for pytorch_name, pytorch_param in state_dict.items():
        jittor_name = pytorch_name
        if jittor_name.startswith("model."):
            jittor_name = jittor_name[6:]
        
        if "num_batches_tracked" in jittor_name or jittor_name.startswith("avg_"):
            continue
        
        if "distribution_project.project" in jittor_name:
            continue
        
        if jittor_name in jittor_state_dict:
            jittor_param = jittor_state_dict[jittor_name]
            
            if list(pytorch_param.shape) == list(jittor_param.shape):
                jittor_param.assign(jt.array(pytorch_param.detach().numpy()))
            elif "scale" in jittor_name and len(pytorch_param.shape) == 0 and list(jittor_param.shape) == [1]:
                jittor_param.assign(jt.array([pytorch_param.detach().numpy()]))
    
    model.eval()
    return model


def unified_fpn_verification():
    """ç»Ÿä¸€çš„FPNéªŒè¯"""
    print(f"ğŸ” ç»Ÿä¸€çš„FPNéªŒè¯")
    print("=" * 60)
    
    # åˆ›å»ºæ¨¡å‹
    model = create_jittor_model()
    
    # åˆ›å»ºæµ‹è¯•è¾“å…¥
    input_data = create_test_input()
    jittor_input = jt.array(input_data)
    
    print(f"è¾“å…¥æ•°æ®: {input_data.shape}, èŒƒå›´[{input_data.min():.6f}, {input_data.max():.6f}]")
    
    with jt.no_grad():
        # è·å–backboneç‰¹å¾
        backbone_features = model.backbone(jittor_input)
        print(f"\nBackboneç‰¹å¾:")
        for i, feat in enumerate(backbone_features):
            print(f"  ç‰¹å¾{i}: {feat.shape}, èŒƒå›´[{feat.min():.6f}, {feat.max():.6f}]")
        
        # æ–¹æ³•1: ä½¿ç”¨å®Œæ•´FPN
        print(f"\næ–¹æ³•1: ä½¿ç”¨å®Œæ•´FPN")
        fpn_features_full = model.fpn(backbone_features)
        for i, feat in enumerate(fpn_features_full):
            print(f"  FPNç‰¹å¾{i}: {feat.shape}, èŒƒå›´[{feat.min():.6f}, {feat.max():.6f}]")
        
        # æ–¹æ³•2: æ‰‹åŠ¨æ‰§è¡ŒFPNçš„æ¯ä¸€æ­¥ï¼ˆç®€åŒ–ç‰ˆï¼‰
        print(f"\næ–¹æ³•2: æ‰‹åŠ¨æ‰§è¡ŒFPNï¼ˆç®€åŒ–ç‰ˆï¼‰")
        fpn = model.fpn
        
        # 1x1 conv to reduce channels
        reduced_inputs = []
        for i, (input_x, reduce) in enumerate(zip(backbone_features, fpn.reduce_layers)):
            reduced = reduce(input_x)
            reduced_inputs.append(reduced)
        
        # top-down path
        inner_outs = [reduced_inputs[-1]]
        
        for idx in range(len(fpn.in_channels) - 1, 0, -1):
            feat_high = inner_outs[0]
            feat_low = reduced_inputs[idx - 1]
            
            inner_outs[0] = feat_high
            
            # upsample
            upsample_feat = fpn.upsample(feat_high)
            
            # concat
            concat_feat = jt.concat([upsample_feat, feat_low], dim=1)
            
            # top_down_blockå¤„ç†
            block_idx = len(fpn.in_channels) - 1 - idx
            inner_out = fpn.top_down_blocks[block_idx](concat_feat)
            
            # ç‰¹åˆ«å…³æ³¨top_down_block1
            if block_idx == 1:
                print(f"    ğŸ” top_down_block1è¯¦ç»†:")
                print(f"      è¾“å…¥concat: {concat_feat.shape}, èŒƒå›´[{concat_feat.min():.6f}, {concat_feat.max():.6f}]")
                print(f"      è¾“å‡º: {inner_out.shape}, èŒƒå›´[{inner_out.min():.6f}, {inner_out.max():.6f}]")
                
                # æ‰‹åŠ¨æ‰§è¡Œtop_down_block1
                top_down_block1 = fpn.top_down_blocks[1]
                manual_output = top_down_block1(concat_feat)
                diff = jt.abs(inner_out - manual_output).max()
                print(f"      æ‰‹åŠ¨vsè‡ªåŠ¨å·®å¼‚: {diff:.10f}")
            
            inner_outs.insert(0, inner_out)
        
        # bottom-up path
        outs = [inner_outs[0]]
        
        for idx in range(len(fpn.in_channels) - 1):
            feat_low = outs[-1]
            feat_high = inner_outs[idx + 1]
            
            # downsample
            downsample_feat = fpn.downsamples[idx](feat_low)
            
            # concat
            concat_feat = jt.concat([downsample_feat, feat_high], dim=1)
            
            # bottom_up_blockå¤„ç†
            out = fpn.bottom_up_blocks[idx](concat_feat)
            
            outs.append(out)
        
        # extra layers
        for i, (extra_in_layer, extra_out_layer) in enumerate(zip(fpn.extra_lvl_in_conv, fpn.extra_lvl_out_conv)):
            extra_in = extra_in_layer(reduced_inputs[-1])
            extra_out = extra_out_layer(outs[-1])
            extra_final = extra_in + extra_out
            
            outs.append(extra_final)
        
        print(f"\næ‰‹åŠ¨FPNè¾“å‡º:")
        for i, out in enumerate(outs):
            print(f"  æ‰‹åŠ¨FPNç‰¹å¾{i}: {out.shape}, èŒƒå›´[{out.min():.6f}, {out.max():.6f}]")
        
        # å¯¹æ¯”ä¸¤ç§æ–¹æ³•
        print(f"\næ–¹æ³•å¯¹æ¯”:")
        for i, (full_out, manual_out) in enumerate(zip(fpn_features_full, outs)):
            diff = jt.abs(full_out - manual_out).max()
            print(f"  ç‰¹å¾{i}å·®å¼‚: {diff:.10f}")
            
            if diff < 1e-6:
                print(f"    âœ… ä¸¤ç§æ–¹æ³•ä¸€è‡´")
            else:
                print(f"    âŒ ä¸¤ç§æ–¹æ³•ä¸ä¸€è‡´")
                print(f"      å®Œæ•´FPN: èŒƒå›´[{full_out.min():.6f}, {full_out.max():.6f}]")
                print(f"      æ‰‹åŠ¨FPN: èŒƒå›´[{manual_out.min():.6f}, {manual_out.max():.6f}]")
        
        # æ–¹æ³•3: ç›´æ¥æµ‹è¯•top_down_block1
        print(f"\næ–¹æ³•3: ç›´æ¥æµ‹è¯•top_down_block1")
        
        # é‡æ–°è·å–top_down_block1çš„è¾“å…¥
        reduced_feat_high = reduced_inputs[-1]  # [1,96,10,10]
        reduced_feat_low = reduced_inputs[1]    # [1,96,20,20]
        
        # upsample
        upsample_feat = fpn.upsample(reduced_feat_high)
        
        # concat
        test_concat_feat = jt.concat([upsample_feat, reduced_feat_low], dim=1)
        
        # ç›´æ¥è°ƒç”¨top_down_block1
        top_down_block1 = fpn.top_down_blocks[1]
        direct_output = top_down_block1(test_concat_feat)
        
        print(f"  ç›´æ¥æµ‹è¯•è¾“å…¥: {test_concat_feat.shape}, èŒƒå›´[{test_concat_feat.min():.6f}, {test_concat_feat.max():.6f}]")
        print(f"  ç›´æ¥æµ‹è¯•è¾“å‡º: {direct_output.shape}, èŒƒå›´[{direct_output.min():.6f}, {direct_output.max():.6f}]")
        
        # ä¸ä¹‹å‰çš„ç»“æœå¯¹æ¯”
        # è¿™é‡Œæˆ‘ä»¬éœ€è¦æ‰¾åˆ°å¯¹åº”çš„è¾“å‡ºè¿›è¡Œå¯¹æ¯”
        # åœ¨å®Œæ•´FPNä¸­ï¼Œtop_down_block1çš„è¾“å‡ºåº”è¯¥å¯¹åº”æŸä¸ªä¸­é—´ç»“æœ
        
        print(f"\nâœ… ç»Ÿä¸€éªŒè¯å®Œæˆ")


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ å¼€å§‹ç»Ÿä¸€FPNéªŒè¯")
    
    # ç»Ÿä¸€FPNéªŒè¯
    unified_fpn_verification()
    
    print(f"\nâœ… éªŒè¯å®Œæˆ")


if __name__ == '__main__':
    main()
