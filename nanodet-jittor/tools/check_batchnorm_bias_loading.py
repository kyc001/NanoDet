#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
æ£€æŸ¥BatchNorm biasçš„æƒé‡åŠ è½½æƒ…å†µ
ç¡®è®¤biasæ˜¯å¦è¢«æ­£ç¡®è¦†ç›–
"""

import os
import sys
import torch
import jittor as jt
import numpy as np

# æ·»åŠ è·¯å¾„
sys.path.append('/home/kyc/project/nanodet/nanodet-jittor')
from nanodet.model.arch.nanodet_plus import NanoDetPlus


def check_batchnorm_bias_loading():
    """æ£€æŸ¥BatchNorm biasçš„æƒé‡åŠ è½½"""
    print("ğŸ” æ£€æŸ¥BatchNorm biasçš„æƒé‡åŠ è½½")
    print("=" * 60)
    
    # åˆ›å»ºæ¨¡å‹é…ç½®
    backbone_cfg = {
        'name': 'ShuffleNetV2',
        'model_size': '1.0x',
        'out_stages': [2, 3, 4],
        'activation': 'LeakyReLU',
        'pretrain': True
    }
    
    fpn_cfg = {
        'name': 'GhostPAN',
        'in_channels': [116, 232, 464],
        'out_channels': 96,
        'kernel_size': 5,
        'num_extra_level': 1,
        'use_depthwise': True,
        'activation': 'LeakyReLU'
    }
    
    head_cfg = {
        'name': 'NanoDetPlusHead',
        'num_classes': 20,
        'input_channel': 96,
        'feat_channels': 96,
        'stacked_convs': 2,
        'kernel_size': 5,
        'strides': [8, 16, 32, 64],
        'activation': 'LeakyReLU',
        'reg_max': 7,
        'norm_cfg': {'type': 'BN'},
        'loss': {
            'loss_qfl': {
                'name': 'QualityFocalLoss',
                'use_sigmoid': True,
                'beta': 2.0,
                'loss_weight': 1.0
            },
            'loss_dfl': {
                'name': 'DistributionFocalLoss',
                'loss_weight': 0.25
            },
            'loss_bbox': {
                'name': 'GIoULoss',
                'loss_weight': 2.0
            }
        }
    }
    
    aux_head_cfg = {
        'name': 'SimpleConvHead',
        'num_classes': 20,
        'input_channel': 192,
        'feat_channels': 192,
        'stacked_convs': 4,
        'strides': [8, 16, 32, 64],
        'activation': 'LeakyReLU',
        'reg_max': 7
    }
    
    print("1ï¸âƒ£ åˆ›å»ºæ¨¡å‹...")
    model = NanoDetPlus(backbone_cfg, fpn_cfg, aux_head_cfg, head_cfg)
    
    # æ£€æŸ¥åˆå§‹åŒ–åçš„BatchNorm bias
    print("\n2ï¸âƒ£ æ£€æŸ¥åˆå§‹åŒ–åçš„BatchNorm bias...")
    initial_bias_values = {}
    for name, module in model.named_modules():
        if isinstance(module, jt.nn.BatchNorm2d) and hasattr(module, 'bias') and module.bias is not None:
            bias_value = module.bias.numpy().copy()
            initial_bias_values[name] = bias_value
            if len(initial_bias_values) <= 5:  # åªæ˜¾ç¤ºå‰5ä¸ª
                print(f"  {name}: biasèŒƒå›´[{bias_value.min():.6f}, {bias_value.max():.6f}]")
    
    print(f"âœ“ è®°å½•äº† {len(initial_bias_values)} ä¸ªBatchNormå±‚çš„åˆå§‹bias")
    
    # åŠ è½½PyTorchæƒé‡
    print("\n3ï¸âƒ£ åŠ è½½PyTorchæƒé‡...")
    checkpoint_path = "/home/kyc/project/nanodet/nanodet-pytorch/workspace/nanodet-plus-m_320_voc_bs64/model_best/model_best.ckpt"
    ckpt = torch.load(checkpoint_path, map_location='cpu')
    state_dict = ckpt.get('state_dict', ckpt)
    
    # æ£€æŸ¥PyTorchæƒé‡ä¸­çš„BatchNorm bias
    print("\n4ï¸âƒ£ æ£€æŸ¥PyTorchæƒé‡ä¸­çš„BatchNorm bias...")
    pytorch_bias_values = {}
    for pytorch_name, pytorch_param in state_dict.items():
        if 'bias' in pytorch_name and ('bn' in pytorch_name.lower() or 'norm' in pytorch_name.lower() or '.1.' in pytorch_name):
            jittor_name = pytorch_name
            if jittor_name.startswith("model."):
                jittor_name = jittor_name[6:]
            
            pytorch_bias_values[jittor_name] = pytorch_param.detach().numpy()
            if len(pytorch_bias_values) <= 5:  # åªæ˜¾ç¤ºå‰5ä¸ª
                bias_data = pytorch_param.detach().numpy()
                print(f"  {pytorch_name} -> {jittor_name}: èŒƒå›´[{bias_data.min():.6f}, {bias_data.max():.6f}]")
    
    print(f"âœ“ æ‰¾åˆ° {len(pytorch_bias_values)} ä¸ªPyTorch BatchNorm bias")
    
    # è·å–Jittoræ¨¡å‹çš„å‚æ•°å­—å…¸
    jittor_state_dict = {}
    for name, param in model.named_parameters():
        jittor_state_dict[name] = param
    
    print("\n5ï¸âƒ£ æ‰§è¡Œæƒé‡åŠ è½½...")
    
    # è®°å½•biasåŠ è½½è¿‡ç¨‹
    bias_loaded_count = 0
    bias_changes = {}
    
    for pytorch_name, pytorch_param in state_dict.items():
        jittor_name = pytorch_name
        if jittor_name.startswith("model."):
            jittor_name = jittor_name[6:]
        
        if "num_batches_tracked" in jittor_name or jittor_name.startswith("avg_"):
            continue
        
        if "distribution_project.project" in jittor_name:
            continue
        
        if jittor_name in jittor_state_dict:
            jittor_param = jittor_state_dict[jittor_name]
            
            # è®°å½•biaså‚æ•°çš„åŠ è½½
            if 'bias' in jittor_name and ('bn' in jittor_name.lower() or 'norm' in jittor_name.lower() or '.1.' in jittor_name):
                before_data = jittor_param.numpy().copy()
                
                if list(pytorch_param.shape) == list(jittor_param.shape):
                    jittor_param.assign(jt.array(pytorch_param.detach().numpy()))
                    bias_loaded_count += 1
                    
                    after_data = jittor_param.numpy().copy()
                    pytorch_data = pytorch_param.detach().numpy()
                    
                    bias_changes[jittor_name] = {
                        'before': before_data,
                        'after': after_data,
                        'pytorch': pytorch_data,
                        'diff': np.abs(after_data - pytorch_data).max()
                    }
                    
                    if bias_loaded_count <= 5:  # åªæ˜¾ç¤ºå‰5ä¸ª
                        print(f"\n  åŠ è½½ {jittor_name}:")
                        print(f"    åŠ è½½å‰: èŒƒå›´[{before_data.min():.6f}, {before_data.max():.6f}]")
                        print(f"    PyTorch: èŒƒå›´[{pytorch_data.min():.6f}, {pytorch_data.max():.6f}]")
                        print(f"    åŠ è½½å: èŒƒå›´[{after_data.min():.6f}, {after_data.max():.6f}]")
                        print(f"    å·®å¼‚: {np.abs(after_data - pytorch_data).max():.10f}")
            
            elif list(pytorch_param.shape) == list(jittor_param.shape):
                jittor_param.assign(jt.array(pytorch_param.detach().numpy()))
            elif "scale" in jittor_name and len(pytorch_param.shape) == 0 and list(jittor_param.shape) == [1]:
                jittor_param.assign(jt.array([pytorch_param.detach().numpy()]))
    
    print(f"\nâœ“ åŠ è½½äº† {bias_loaded_count} ä¸ªBatchNorm biaså‚æ•°")
    
    # éªŒè¯æœ€ç»ˆçš„biasçŠ¶æ€
    print("\n6ï¸âƒ£ éªŒè¯æœ€ç»ˆçš„biasçŠ¶æ€...")
    
    final_bias_check = {}
    for name, module in model.named_modules():
        if isinstance(module, jt.nn.BatchNorm2d) and hasattr(module, 'bias') and module.bias is not None:
            final_bias = module.bias.numpy().copy()
            final_bias_check[name] = final_bias
            
            if len(final_bias_check) <= 5:  # åªæ˜¾ç¤ºå‰5ä¸ª
                print(f"  {name}: æœ€ç»ˆbiasèŒƒå›´[{final_bias.min():.6f}, {final_bias.max():.6f}]")
                
                # æ£€æŸ¥æ˜¯å¦è¿˜æ˜¯åˆå§‹å€¼0.0001
                if np.allclose(final_bias, 0.0001):
                    print(f"    âŒ ä»ç„¶æ˜¯åˆå§‹å€¼0.0001ï¼Œæƒé‡åŠ è½½å¤±è´¥")
                else:
                    print(f"    âœ… å·²è¢«æ­£ç¡®è¦†ç›–")
    
    # ç»Ÿè®¡åˆ†æ
    print(f"\nğŸ“Š ç»Ÿè®¡åˆ†æ:")
    
    still_initial = 0
    properly_loaded = 0
    
    for name, final_bias in final_bias_check.items():
        if np.allclose(final_bias, 0.0001):
            still_initial += 1
        else:
            properly_loaded += 1
    
    print(f"  ä»ä¸ºåˆå§‹å€¼(0.0001): {still_initial}")
    print(f"  æ­£ç¡®åŠ è½½: {properly_loaded}")
    print(f"  æ€»BatchNormå±‚: {len(final_bias_check)}")
    
    if still_initial > 0:
        print(f"  âŒ æœ‰ {still_initial} ä¸ªBatchNorm biasæœªè¢«æ­£ç¡®åŠ è½½")
        return False
    else:
        print(f"  âœ… æ‰€æœ‰BatchNorm biaséƒ½è¢«æ­£ç¡®åŠ è½½")
        return True


def check_specific_bias_parameters():
    """æ£€æŸ¥ç‰¹å®šçš„biaså‚æ•°"""
    print(f"\nğŸ” æ£€æŸ¥ç‰¹å®šçš„biaså‚æ•°")
    print("=" * 60)
    
    # åŠ è½½PyTorchæƒé‡
    checkpoint_path = "/home/kyc/project/nanodet/nanodet-pytorch/workspace/nanodet-plus-m_320_voc_bs64/model_best/model_best.ckpt"
    ckpt = torch.load(checkpoint_path, map_location='cpu')
    state_dict = ckpt.get('state_dict', ckpt)
    
    # æŸ¥æ‰¾æ‰€æœ‰biaså‚æ•°
    bias_params = {}
    for name, param in state_dict.items():
        if 'bias' in name:
            bias_params[name] = param.detach().numpy()
    
    print(f"PyTorchæƒé‡ä¸­åŒ…å« {len(bias_params)} ä¸ªbiaså‚æ•°")
    
    # åˆ†æbiaså‚æ•°çš„å€¼åˆ†å¸ƒ
    all_bias_values = []
    for name, bias_data in bias_params.items():
        all_bias_values.extend(bias_data.flatten())
    
    all_bias_values = np.array(all_bias_values)
    
    print(f"\næ‰€æœ‰biaså‚æ•°ç»Ÿè®¡:")
    print(f"  æ€»æ•°: {len(all_bias_values)}")
    print(f"  èŒƒå›´: [{all_bias_values.min():.6f}, {all_bias_values.max():.6f}]")
    print(f"  å‡å€¼: {all_bias_values.mean():.6f}")
    print(f"  æ ‡å‡†å·®: {all_bias_values.std():.6f}")
    
    # æ£€æŸ¥æœ‰å¤šå°‘biasæ˜¯0.0001
    close_to_0001 = np.isclose(all_bias_values, 0.0001, atol=1e-6)
    print(f"  æ¥è¿‘0.0001çš„biasæ•°é‡: {close_to_0001.sum()}")
    print(f"  æ¥è¿‘0.0001çš„æ¯”ä¾‹: {close_to_0001.sum() / len(all_bias_values) * 100:.2f}%")
    
    # æ˜¾ç¤ºä¸€äº›å…·ä½“çš„biaså‚æ•°
    print(f"\nå…·ä½“çš„biaså‚æ•°ç¤ºä¾‹:")
    count = 0
    for name, bias_data in bias_params.items():
        if count < 10:
            print(f"  {name}: èŒƒå›´[{bias_data.min():.6f}, {bias_data.max():.6f}], å½¢çŠ¶{bias_data.shape}")
            if np.allclose(bias_data, 0.0001):
                print(f"    âš ï¸ å…¨éƒ¨ä¸º0.0001")
            count += 1


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ å¼€å§‹æ£€æŸ¥BatchNorm biasåŠ è½½")
    
    # æ£€æŸ¥biasåŠ è½½è¿‡ç¨‹
    bias_ok = check_batchnorm_bias_loading()
    
    # æ£€æŸ¥ç‰¹å®šbiaså‚æ•°
    check_specific_bias_parameters()
    
    print(f"\nâœ… æ£€æŸ¥å®Œæˆ")
    
    if bias_ok:
        print(f"BatchNorm biasåŠ è½½æ­£ç¡®")
    else:
        print(f"BatchNorm biasåŠ è½½æœ‰é—®é¢˜")


if __name__ == '__main__':
    main()
