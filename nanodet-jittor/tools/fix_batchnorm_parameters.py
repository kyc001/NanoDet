#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
ä¿®å¤BatchNormå‚æ•°é—®é¢˜
å°†running_meanå’Œrunning_varä»å‚æ•°ä¸­æ’é™¤
"""

import os
import sys
import re

# æ·»åŠ è·¯å¾„
sys.path.append('/home/kyc/project/nanodet/nanodet-jittor')


def find_batchnorm_files():
    """æ‰¾åˆ°æ‰€æœ‰åŒ…å«BatchNormçš„æ–‡ä»¶"""
    batchnorm_files = []
    
    # æœç´¢ç›®å½•
    search_dirs = [
        'nanodet/model/backbone',
        'nanodet/model/fpn', 
        'nanodet/model/head',
        'nanodet/model/module'
    ]
    
    for search_dir in search_dirs:
        if os.path.exists(search_dir):
            for root, dirs, files in os.walk(search_dir):
                for file in files:
                    if file.endswith('.py'):
                        file_path = os.path.join(root, file)
                        
                        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦åŒ…å«BatchNorm
                        try:
                            with open(file_path, 'r', encoding='utf-8') as f:
                                content = f.read()
                                if 'BatchNorm' in content or 'nn.BatchNorm' in content:
                                    batchnorm_files.append(file_path)
                        except:
                            pass
    
    return batchnorm_files


def create_custom_batchnorm():
    """åˆ›å»ºè‡ªå®šä¹‰BatchNormï¼Œæ’é™¤ç»Ÿè®¡å‚æ•°"""
    custom_bn_code = '''#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
è‡ªå®šä¹‰BatchNormï¼Œä¸PyTorchå‚æ•°æœºåˆ¶å¯¹é½
æ’é™¤running_meanå’Œrunning_varä»named_parameters()
"""

import jittor as jt
from jittor import nn


class BatchNormAligned(nn.Module):
    """
    ä¸PyTorchå¯¹é½çš„BatchNorm
    running_meanå’Œrunning_varä¸è¢«è®¡å…¥named_parameters()
    """
    
    def __init__(self, num_features, eps=1e-5, momentum=0.1, affine=True, track_running_stats=True):
        super(BatchNormAligned, self).__init__()
        
        self.num_features = num_features
        self.eps = eps
        self.momentum = momentum
        self.affine = affine
        self.track_running_stats = track_running_stats
        
        if self.affine:
            # è¿™äº›æ˜¯å¯è®­ç»ƒå‚æ•°ï¼Œä¼šè¢«è®¡å…¥named_parameters()
            self.weight = jt.ones(num_features)
            self.bias = jt.zeros(num_features)
        else:
            self.weight = None
            self.bias = None
        
        if self.track_running_stats:
            # ä½¿ç”¨ä¸‹åˆ’çº¿å‰ç¼€ï¼Œä¸è¢«è®¡å…¥named_parameters()
            self._running_mean = jt.zeros(num_features)
            self._running_var = jt.ones(num_features)
            self._running_mean.requires_grad = False
            self._running_var.requires_grad = False
            # å…¼å®¹æ€§å±æ€§
            self._num_batches_tracked = 0
        else:
            self._running_mean = None
            self._running_var = None
            self._num_batches_tracked = None
    
    @property
    def running_mean(self):
        """å…¼å®¹æ€§å±æ€§è®¿é—®"""
        return self._running_mean
    
    @property 
    def running_var(self):
        """å…¼å®¹æ€§å±æ€§è®¿é—®"""
        return self._running_var
    
    @property
    def num_batches_tracked(self):
        """å…¼å®¹æ€§å±æ€§è®¿é—®"""
        return self._num_batches_tracked
    
    def execute(self, x):
        """å‰å‘ä¼ æ’­"""
        if self.training and self.track_running_stats:
            # è®­ç»ƒæ¨¡å¼ï¼šæ›´æ–°ç»Ÿè®¡ä¿¡æ¯
            if self._running_mean is not None:
                # è®¡ç®—å½“å‰æ‰¹æ¬¡ç»Ÿè®¡
                mean = x.mean(dim=[0, 2, 3], keepdims=False)
                var = x.var(dim=[0, 2, 3], keepdims=False)
                
                # æ›´æ–°ç§»åŠ¨å¹³å‡
                self._running_mean = (1 - self.momentum) * self._running_mean + self.momentum * mean
                self._running_var = (1 - self.momentum) * self._running_var + self.momentum * var
                self._num_batches_tracked += 1
        
        # ä½¿ç”¨BatchNorm
        if self.track_running_stats and not self.training:
            # æ¨ç†æ¨¡å¼ï¼šä½¿ç”¨ç»Ÿè®¡ä¿¡æ¯
            mean = self._running_mean
            var = self._running_var
        else:
            # è®­ç»ƒæ¨¡å¼ï¼šä½¿ç”¨å½“å‰æ‰¹æ¬¡ç»Ÿè®¡
            mean = x.mean(dim=[0, 2, 3], keepdims=True)
            var = x.var(dim=[0, 2, 3], keepdims=True)
        
        # æ ‡å‡†åŒ–
        x_norm = (x - mean) / jt.sqrt(var + self.eps)
        
        # ä»¿å°„å˜æ¢
        if self.affine:
            if len(x.shape) == 4:  # [N, C, H, W]
                weight = self.weight.view(1, -1, 1, 1)
                bias = self.bias.view(1, -1, 1, 1)
            else:  # [N, C]
                weight = self.weight
                bias = self.bias
            
            x_norm = x_norm * weight + bias
        
        return x_norm


# å…¼å®¹æ€§åˆ«å
BatchNorm = BatchNormAligned
BatchNorm1d = BatchNormAligned
BatchNorm2d = BatchNormAligned
BatchNorm3d = BatchNormAligned
'''
    
    # ä¿å­˜è‡ªå®šä¹‰BatchNorm
    with open('nanodet/model/module/batchnorm_aligned.py', 'w', encoding='utf-8') as f:
        f.write(custom_bn_code)
    
    print("âœ“ åˆ›å»ºäº†è‡ªå®šä¹‰BatchNorm: nanodet/model/module/batchnorm_aligned.py")


def create_custom_scale():
    """åˆ›å»ºè‡ªå®šä¹‰Scaleï¼Œä½¿ç”¨æ ‡é‡å½¢çŠ¶"""
    custom_scale_code = '''#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
è‡ªå®šä¹‰Scaleæ¨¡å—ï¼Œä¸PyTorchæ ‡é‡å‚æ•°å¯¹é½
"""

import jittor as jt
from jittor import nn
import numpy as np


class ScaleAligned(nn.Module):
    """
    ä¸PyTorchå¯¹é½çš„Scaleæ¨¡å—
    å°è¯•åˆ›å»ºæ ‡é‡å‚æ•°
    """
    
    def __init__(self, scale=1.0):
        super(ScaleAligned, self).__init__()
        
        # å°è¯•å¤šç§æ–¹å¼åˆ›å»ºæ ‡é‡å‚æ•°
        # æ–¹æ³•1: ä½¿ç”¨ä¸‹åˆ’çº¿å‰ç¼€å­˜å‚¨æ•°æ®ï¼Œç”¨å±æ€§è®¿é—®
        self._scale_value = float(scale)
    
    @property
    def scale(self):
        """åŠ¨æ€åˆ›å»ºæ ‡é‡å¼ é‡"""
        # æ¯æ¬¡è®¿é—®æ—¶åˆ›å»ºæ–°çš„æ ‡é‡å¼ é‡
        # è¿™æ ·å¯ä»¥é¿å…è¢«è®¡å…¥named_parameters()
        return jt.array(self._scale_value)
    
    def execute(self, x):
        """å‰å‘ä¼ æ’­"""
        return x * self.scale


# å¦‚æœä¸Šé¢çš„æ–¹æ³•ä¸è¡Œï¼Œå°è¯•è¿™ä¸ª
class ScaleParameter(nn.Module):
    """
    Scaleå‚æ•°æ¨¡å— - å¤‡é€‰æ–¹æ¡ˆ
    """
    
    def __init__(self, scale=1.0):
        super(ScaleParameter, self).__init__()
        
        # åˆ›å»º1ç»´å¼ é‡ï¼Œåœ¨æƒé‡åŠ è½½æ—¶ç‰¹æ®Šå¤„ç†
        self.scale = jt.array([scale])
    
    def execute(self, x):
        """å‰å‘ä¼ æ’­"""
        return x * self.scale


# é»˜è®¤ä½¿ç”¨å¯¹é½ç‰ˆæœ¬
Scale = ScaleAligned
'''
    
    # ä¿å­˜è‡ªå®šä¹‰Scale
    with open('nanodet/model/module/scale_aligned.py', 'w', encoding='utf-8') as f:
        f.write(custom_scale_code)
    
    print("âœ“ åˆ›å»ºäº†è‡ªå®šä¹‰Scale: nanodet/model/module/scale_aligned.py")


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ å¼€å§‹ä¿®å¤BatchNormå’ŒScaleå‚æ•°é—®é¢˜")
    
    # æ‰¾åˆ°BatchNormæ–‡ä»¶
    batchnorm_files = find_batchnorm_files()
    print(f"æ‰¾åˆ° {len(batchnorm_files)} ä¸ªåŒ…å«BatchNormçš„æ–‡ä»¶:")
    for file in batchnorm_files:
        print(f"  - {file}")
    
    # åˆ›å»ºè‡ªå®šä¹‰æ¨¡å—
    create_custom_batchnorm()
    create_custom_scale()
    
    print(f"\\nâœ… ä¿®å¤å®Œæˆï¼")
    print(f"ä¸‹ä¸€æ­¥ï¼š")
    print(f"1. å°†ç°æœ‰ä»£ç ä¸­çš„ nn.BatchNorm æ›¿æ¢ä¸º BatchNormAligned")
    print(f"2. å°†ç°æœ‰ä»£ç ä¸­çš„ Scale æ›¿æ¢ä¸º ScaleAligned")
    print(f"3. é‡æ–°æµ‹è¯•å‚æ•°å¯¹é½")


if __name__ == '__main__':
    main()
