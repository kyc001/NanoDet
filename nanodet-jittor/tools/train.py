# JITTOR MIGRATION by Google LLC.
import sys
import os


# å°†é¡¹ç›®æ ¹ç›®å½•ï¼ˆnanodet-jittorï¼‰æ·»åŠ åˆ° Python è·¯å¾„
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

import argparse
import random
import warnings
from types import SimpleNamespace
import traceback

# JITTOR MIGRATION: å¯¼å…¥ jittor å’Œ numpy
import jittor as jt
import numpy as np

# JITTOR MIGRATION: å¯¼å…¥å·²è¿ç§»çš„å·¥å…·å‡½æ•°å’Œç±»
from nanodet.data.collate import naive_collate
from nanodet.data.dataset import build_dataset
from nanodet.evaluator import build_evaluator
from nanodet.trainer.task import TrainingTask
# JITTOR MIGRATION FIX: ç›´æ¥ä» logger æ¨¡å—å¯¼å…¥æ­£ç¡®çš„ç±»åï¼Œå¹¶è°ƒæ•´å…¶ä»–å¯¼å…¥
from nanodet.util.logger import NanoDetLightningLogger
from nanodet.util import (
    cfg,
    convert_old_model,
    env_utils,
    load_config,
    load_model_weight,
    mkdir,
)

def parse_args():
    """è§£æå‘½ä»¤è¡Œå‚æ•°ã€‚"""
    parser = argparse.ArgumentParser()
    parser.add_argument("config", help="è®­ç»ƒé…ç½®æ–‡ä»¶çš„è·¯å¾„")
    parser.add_argument(
        "--local_rank", default=-1, type=int, help="ç”¨äºåˆ†å¸ƒå¼è®­ç»ƒçš„èŠ‚ç‚¹æ’å"
    )
    parser.add_argument("--seed", type=int, default=None, help="éšæœºç§å­")
    # ä¾¿äºå¿«é€Ÿè°ƒè¯•çš„å¯é€‰å‚æ•°
    parser.add_argument("--max_epochs", type=int, default=None, help="æœ€å¤šè®­ç»ƒçš„ epoch æ•°ï¼ˆå¯é€‰ï¼‰")
    parser.add_argument("--max_train_batches", type=int, default=None, help="æ¯ä¸ª epoch è®­ç»ƒçš„æœ€å¤§ batch æ•°ï¼ˆå¯é€‰ï¼‰")
    parser.add_argument("--max_val_batches", type=int, default=None, help="æ¯æ¬¡éªŒè¯çš„æœ€å¤§ batch æ•°ï¼ˆå¯é€‰ï¼‰")
    parser.add_argument("--warmup_steps", type=int, default=0, help="é¢„çƒ­æ­¥æ•°ï¼Œä»…ç”¨äºè§¦å‘JITç¼–è¯‘ç¼“å­˜ï¼ˆä¸åšéªŒè¯ä¸ä¿å­˜ï¼‰")
    args = parser.parse_args()
    return args

def set_seed(seed):
    """è®¾ç½®éšæœºç§å­ä»¥ç¡®ä¿å¯å¤ç°æ€§ã€‚"""
    random.seed(seed)
    np.random.seed(seed)
    jt.seed(seed)

def main(args):
    # åŠ è½½é…ç½®
    load_config(cfg, args.config)
    if cfg.model.arch.head.num_classes != len(cfg.class_names):
        raise ValueError(
            "cfg.model.arch.head.num_classes å¿…é¡»ç­‰äº len(cfg.class_names), "
            f"ä½†å¾—åˆ°äº† {cfg.model.arch.head.num_classes} å’Œ {len(cfg.class_names)}"
        )
    
    # JITTOR MIGRATION: ä½¿ç”¨ jt.rank è·å–æ’åï¼Œå¹¶è®¾ç½® GPU
    local_rank = jt.rank if jt.world_size > 1 else 0
    if jt.has_cuda:
        jt.flags.use_cuda = 1

    # åˆ›å»ºä¿å­˜ç›®å½•å’Œæ—¥å¿—è®°å½•å™¨
    # JITTOR MIGRATION FIX: ä¿®å¤ mkdir çš„è°ƒç”¨ï¼Œä¼ å…¥ local_rank
    mkdir(local_rank, cfg.save_dir)
    # JITTOR MIGRATION FIX: ä½¿ç”¨æ­£ç¡®çš„ç±»åå®ä¾‹åŒ– Logger
    logger = NanoDetLightningLogger(cfg.save_dir)
    logger.dump_cfg(cfg)

    # è®¾ç½®éšæœºç§å­
    if args.seed is not None:
        logger.info(f"è®¾ç½®éšæœºç§å­ä¸º {args.seed}")
        set_seed(args.seed)

    # JITTOR MIGRATION: ç§»é™¤ PyTorch-Lightning å’Œ jt.backends.cudnn çš„è®¾ç½®
    
    logger.info("æ­£åœ¨è®¾ç½®æ•°æ®...")
    train_dataset = build_dataset(cfg.data.train, "train")
    val_dataset = build_dataset(cfg.data.val, "val") # æ¨¡å¼åº”ä¸º 'val'

    # JITTOR MIGRATION: Jittor çš„ DataLoader ç›´æ¥åœ¨ Dataset å¯¹è±¡ä¸Šé…ç½®
    train_dataloader = train_dataset.set_attrs(
        batch_size=cfg.device.batchsize_per_gpu,
        shuffle=True,
        num_workers=cfg.device.workers_per_gpu,
        collate_batch=naive_collate,
        drop_last=True,
    )
    val_dataloader = val_dataset.set_attrs(
        batch_size=cfg.device.batchsize_per_gpu,
        shuffle=False,
        num_workers=cfg.device.workers_per_gpu,
        collate_batch=naive_collate,
        drop_last=False,
    )

    evaluator = build_evaluator(cfg.evaluator, val_dataset)

    logger.info("æ­£åœ¨åˆ›å»ºæ¨¡å‹...")
    task = TrainingTask(cfg, evaluator, logger)
    
    # åŠ è½½é¢„è®­ç»ƒæ¨¡å‹æƒé‡
    if "load_model" in cfg.schedule:
        ckpt = jt.load(cfg.schedule.load_model)
        if "pytorch-lightning_version" in ckpt:
             warnings.warn(
                "è­¦å‘Šï¼æ‚¨æ­£åœ¨åŠ è½½ä¸€ä¸ª PyTorch Lightning æ£€æŸ¥ç‚¹ã€‚è¯·ç¡®ä¿å…¶ä¸å½“å‰æ¨¡å‹å…¼å®¹ã€‚"
            )
        elif "state_dict" not in ckpt:
            # å‡è®¾æ˜¯æ—§æ ¼å¼
            warnings.warn(
                "è­¦å‘Šï¼æ—§çš„ .pth æ£€æŸ¥ç‚¹æ ¼å¼å·²å¼ƒç”¨ã€‚è¯·ä½¿ç”¨ tools/convert_old_checkpoint.py è¿›è¡Œè½¬æ¢ã€‚"
            )
            ckpt = convert_old_model(ckpt)
        load_model_weight(task.model, ckpt, logger)
        logger.info(f"ä» {cfg.schedule.load_model} åŠ è½½äº†æ¨¡å‹æƒé‡")

    # JITTOR MIGRATION: æ›¿æ¢ PyTorch Lightning Trainer ä¸ºæ‰‹åŠ¨è®­ç»ƒå¾ªç¯
    
    # é…ç½®ä¼˜åŒ–å™¨å’Œå­¦ä¹ ç‡è°ƒåº¦å™¨
    optimizer, scheduler = task.configure_optimizers()
    
    # å¦‚æœæ˜¯å¤š GPU è®­ç»ƒï¼Œä½¿ç”¨ DataParallel åŒ…è£…æ¨¡å‹
    if jt.world_size > 1:
        task.model = jt.DataParallel(task.model)
        env_utils.set_multi_processing(distributed=True)

    logger.info("å¼€å§‹è®­ç»ƒ...")
    logger.info(f"æ€»å…± {cfg.schedule.total_epochs} ä¸ª epochï¼Œæ¯ä¸ª epoch {len(train_dataloader)} ä¸ªæ‰¹æ¬¡")
    logger.info("=" * 80)

    # é¢„çƒ­é˜¶æ®µï¼šä»…ç”¨äºè§¦å‘ JIT ç¼–è¯‘ç¼“å­˜
    if args.warmup_steps and args.warmup_steps > 0:
        logger.info(f"ğŸš€ å¼€å§‹é¢„çƒ­ warmupï¼Œå…± {args.warmup_steps} æ­¥ï¼ˆä¸ä¿å­˜æ¨¡å‹/ä¸éªŒè¯ï¼‰...")
        task.model.train()
        warmup_losses = []
        warmup_batches = iter(train_dataloader)
        for wi in range(args.warmup_steps):
            try:
                batch = next(warmup_batches)
            except StopIteration:
                warmup_batches = iter(train_dataloader)
                batch = next(warmup_batches)
            try:
                res = task.training_step(batch, wi, SimpleNamespace(current_epoch=0, global_step=wi, optimizer=None, num_training_batches=len(train_dataloader), num_val_batches=len(val_dataloader)))
                warmup_loss = res['loss'] if isinstance(res, dict) else res
                # åªåšä¸€æ¬¡ optimizer.step ä»¥ç¡®ä¿åå‘è·¯å¾„è¢«ç¼–è¯‘
                optimizer.step(warmup_loss)
                warmup_losses.append(float(warmup_loss))
                if (wi+1) % max(1, args.warmup_steps//5) == 0:
                    logger.info(f"Warmup {wi+1}/{args.warmup_steps} | loss:{np.mean(warmup_losses):.4f}")
            except Exception as e:
                logger.warning(f"Warmup ç¬¬ {wi} æ­¥å¤±è´¥: {e}")
                traceback.print_exc()
                continue
        logger.info("âœ… é¢„çƒ­å®Œæˆï¼Œå¼€å§‹æ­£å¼è®­ç»ƒ")

    global_step = 0
    start_epoch = 0
    best_ap = 0.0

    # å¯¼å…¥æ—¶é—´æ¨¡å—
    import time

    total_epochs = cfg.schedule.total_epochs if args.max_epochs is None else min(cfg.schedule.total_epochs, args.max_epochs)
    for epoch in range(start_epoch, total_epochs):
        epoch_start_time = time.time()
        task.on_train_epoch_start(epoch)

        # æ¨¡æ‹Ÿä¸€ä¸ª trainer å¯¹è±¡
        trainer_mock = SimpleNamespace(
            current_epoch=epoch,
            global_step=global_step,
            num_training_batches=len(train_dataloader),
            num_val_batches=len(val_dataloader),
            optimizer=optimizer
        )

        # ğŸ¯ PyTorch é£æ ¼çš„è®­ç»ƒå¾ªç¯ - å¸¦å®æ—¶è¿›åº¦æ˜¾ç¤º
        task.model.train()
        epoch_losses = []

        # è·å–å½“å‰å­¦ä¹ ç‡ - Jittor ä¼˜åŒ–å™¨å…¼å®¹æ€§ä¿®å¤
        current_lr = 0.001  # é»˜è®¤å€¼
        try:
            if hasattr(optimizer, 'param_groups') and len(optimizer.param_groups) > 0:
                current_lr = optimizer.param_groups[0]['lr']
            elif hasattr(optimizer, 'lr'):
                current_lr = optimizer.lr
            elif hasattr(optimizer, 'learning_rate'):
                current_lr = optimizer.learning_rate
        except:
            current_lr = 0.001  # ä¿æŒé»˜è®¤å€¼

        logger.info(f"å¼€å§‹ Epoch {epoch+1}/{cfg.schedule.total_epochs} | LR: {current_lr:.6f}")

        # ä½¿ç”¨ç®€å•çš„è¿›åº¦æ˜¾ç¤ºï¼Œé¿å… tqdm å…¼å®¹æ€§é—®é¢˜
        batch_count = len(train_dataloader)
        print_interval = max(1, batch_count // 20)  # æ¯5%æ˜¾ç¤ºä¸€æ¬¡è¿›åº¦

        for i, batch in enumerate(train_dataloader):
            trainer_mock.global_step = global_step

            try:
                # è°ƒè¯•ï¼šæ£€æŸ¥å¹¶æ‰“å° batch çš„å…³é”®å­—æ®µå½¢çŠ¶ï¼Œé¿å…åŠ¨æ€å½¢çŠ¶å¯¼è‡´çš„ç¼“å­˜çˆ†ç‚¸
                if i < 3:  # ä»…å‰å‡ ä¸ªbatchæ‰“å°
                    try:
                        img = batch.get("img")
                        if isinstance(img, jt.Var):
                            logger.info(f"Batch {i} img shape: {tuple(img.shape)}")
                        else:
                            logger.info(f"Batch {i} img type: {type(img)}")
                        for k in ["gt_bboxes", "gt_labels", "gt_bboxes_ignore"]:
                            v = batch.get(k)
                            if v is None:
                                continue
                            if isinstance(v, jt.Var):
                                logger.info(f"  {k} jt.Var shape: {tuple(v.shape)}")
                            elif isinstance(v, np.ndarray):
                                logger.info(f"  {k} np shape: {v.shape} dtype:{v.dtype}")
                            else:
                                try:
                                    logger.info(f"  {k} type:{type(v)} len:{len(v)}")
                                except Exception:
                                    logger.info(f"  {k} type:{type(v)}")
                    except Exception as e_dbg:
                        logger.warning(f"Batch {i} debug inspect failed: {e_dbg}")

                # å‰å‘ä¼ æ’­å’ŒæŸå¤±è®¡ç®—
                training_result = task.training_step(batch, i, trainer_mock)
                # ğŸ”§ ä¿®å¤ï¼šæ­£ç¡®å¤„ç†è¿”å›å€¼
                if isinstance(training_result, dict):
                    training_loss = training_result['loss']
                else:
                    training_loss = training_result
                # ğŸ”§ ä¿®å¤ï¼šé¿å…è®¡ç®—å›¾æ–­è£‚ï¼Œå»¶è¿Ÿè·å–æŸå¤±å€¼ç”¨äºè®°å½•
                loss_value = float(training_loss)  # Jittor ä¼šè‡ªåŠ¨å¤„ç†
                epoch_losses.append(loss_value)

                # åå‘ä¼ æ’­
                optimizer.step(training_loss)
                task.on_train_batch_end(global_step)
                global_step += 1

                # ğŸ¯ å®æ—¶è¿›åº¦æ˜¾ç¤º
                if (i + 1) % print_interval == 0 or i == 0:
                    progress = (i + 1) / batch_count * 100
                    avg_loss = np.mean(epoch_losses)
                    # éµå¾ªç”¨æˆ·è¦æ±‚çš„æ—¥å¿—æ ¼å¼
                    # [NanoDet][MM-DD HH:MM:SS]INFO: Train|Epoch1/50|Iter0(1/108)| mem:5.06G| lr:1.00e-06| loss_qfl:...|
                    # Jittor å½“å‰æ—  used_cuda_memï¼Œå¯å®‰å…¨ç½®0é¿å… AttributeError
                    mem_gb = 0.0
                    iter_str = f"Iter{trainer_mock.global_step}({i+1}/{batch_count})"
                    # å½“ loss_states å¯ç”¨æ—¶é€é¡¹æ‰“å°ï¼›å¦åˆ™å›é€€ä¸º Loss/Avg
                    base = f"Train|Epoch{epoch+1}/{cfg.schedule.total_epochs}|{iter_str}| mem:{mem_gb:.2f}G| lr:{current_lr:.2e}| "
                    try:
                        if isinstance(training_result, dict) and 'loss_states' in training_result:
                            loss_states = training_result['loss_states']
                            for k, v in loss_states.items():
                                try:
                                    v_mean = v.mean() if hasattr(v, 'numel') and v.numel()>1 else v
                                    base += f"{k}:{float(v_mean):.4f}| "
                                except Exception:
                                    pass
                        else:
                            base += f"loss:{loss_value:.4f}| avg:{avg_loss:.4f}| "
                    except Exception:
                        base += f"loss:{loss_value:.4f}| avg:{avg_loss:.4f}| "
                    logger.info(base)

            except Exception as e:
                # è¾“å‡ºæ›´è¯¦ç»†çš„æ‰¹æ¬¡å…³é”®ä¿¡æ¯ï¼Œå¸®åŠ©å®šä½é—®é¢˜
                try:
                    shapes = {}
                    if isinstance(batch, dict):
                        for k, v in batch.items():
                            if isinstance(v, jt.Var):
                                shapes[k] = tuple(v.shape)
                            elif isinstance(v, np.ndarray):
                                shapes[k] = v.shape
                            else:
                                shapes[k] = type(v).__name__
                    logger.error(f"è®­ç»ƒæ‰¹æ¬¡ {i} å¤±è´¥: {e}. batch keys: {list(batch.keys()) if isinstance(batch, dict) else type(batch)} shapes: {shapes}")
                except Exception:
                    logger.error(f"è®­ç»ƒæ‰¹æ¬¡ {i} å¤±è´¥: {e}")
                # æ— è®ºå¦‚ä½•éƒ½æ‰“å°å®Œæ•´å †æ ˆï¼Œä¾¿äºå®šä½
                traceback.print_exc()
                continue

            # æ—©åœï¼šé™åˆ¶æ¯ä¸ª epoch çš„è®­ç»ƒ batch æ•°
            if args.max_train_batches is not None and (i + 1) >= args.max_train_batches:
                break

        # æ›´æ–°å­¦ä¹ ç‡
        if scheduler:
            scheduler.step()

        # è®¡ç®— epoch ç»Ÿè®¡ä¿¡æ¯
        epoch_time = time.time() - epoch_start_time
        avg_loss = np.mean(epoch_losses)

        # ğŸ¯ PyTorch é£æ ¼çš„ epoch æ€»ç»“
        logger.info(f"Epoch {epoch+1:3d}/{cfg.schedule.total_epochs} | "
                   f"Loss: {avg_loss:.4f} | "
                   f"Time: {epoch_time:.1f}s | "
                   f"LR: {current_lr:.6f}")

        # éªŒè¯å’Œæµ‹è¯„
        if (epoch + 1) % cfg.schedule.val_intervals == 0:
            logger.info(f"ğŸ” å¼€å§‹éªŒè¯ Epoch {epoch + 1}...")

            task.model.eval()
            task.on_validation_epoch_start()
            val_outputs = []

            # éªŒè¯å¾ªç¯ - ç®€å•è¿›åº¦æ˜¾ç¤º
            val_batch_count = len(val_dataloader)
            val_print_interval = max(1, val_batch_count // 10)

            for i, batch in enumerate(val_dataloader):
                trainer_mock.global_step = global_step
                try:
                    with jt.no_grad():
                        dets = task.validation_step(batch, i, trainer_mock)
                        val_outputs.append(dets)

                    # éªŒè¯è¿›åº¦æ˜¾ç¤º
                    if (i + 1) % val_print_interval == 0 or i == 0:
                        val_progress = (i + 1) / val_batch_count * 100
                        print(f"  éªŒè¯è¿›åº¦: [{i+1:4d}/{val_batch_count}] ({val_progress:5.1f}%)", flush=True)

                except Exception as e:
                    logger.error(f"éªŒè¯æ‰¹æ¬¡ {i} å¤±è´¥: {e}")
                    continue

                # æ—©åœï¼šé™åˆ¶éªŒè¯ batch æ•°
                if args.max_val_batches is not None and (i + 1) >= args.max_val_batches:
                    break

            # ğŸ¯ è‡ªåŠ¨è°ƒç”¨æµ‹è¯„å·¥å…·
            try:
                metrics = task.validation_epoch_end(val_outputs, epoch)

                # æå–å…³é”®æŒ‡æ ‡
                if metrics and 'mAP' in metrics:
                    current_ap = metrics['mAP']
                    logger.info(f"ğŸ“Š éªŒè¯ç»“æœ | mAP: {current_ap:.4f}")

                    # ä¿å­˜æœ€ä½³æ¨¡å‹
                    if current_ap > best_ap:
                        best_ap = current_ap
                        if jt.rank == 0:
                            best_model_path = os.path.join(cfg.save_dir, "model_best.ckpt")
                            task.model.save(best_model_path)
                            logger.info(f"ğŸ† æ–°çš„æœ€ä½³æ¨¡å‹ï¼mAP: {best_ap:.4f} -> {best_model_path}")
                else:
                    logger.info("ğŸ“Š éªŒè¯å®Œæˆï¼Œä½†æœªè·å–åˆ° mAP æŒ‡æ ‡")

            except Exception as e:
                logger.error(f"éªŒè¯è¯„ä¼°å¤±è´¥: {e}")

            logger.info("-" * 80)

        # ä¿å­˜æœ€æ–°æ¨¡å‹
        if jt.rank == 0:
            task.model.save(os.path.join(cfg.save_dir, "model_last.ckpt"))

    # ğŸ¯ è®­ç»ƒå®Œæˆæ€»ç»“
    logger.info("=" * 80)
    logger.info("ğŸ‰ è®­ç»ƒå®Œæˆï¼")
    logger.info(f"ğŸ“Š æœ€ä½³ mAP: {best_ap:.4f}")
    logger.info(f"ğŸ’¾ æ¨¡å‹ä¿å­˜åœ¨: {cfg.save_dir}")
    logger.info("=" * 80)


if __name__ == "__main__":
    args = parse_args()
    main(args)
