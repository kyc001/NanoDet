# JITTOR MIGRATION by Google LLC.
import sys
import os


# Limit CPU threading for dataloader & OpenCV to improve throughput
try:
    import os as _os
    _os.environ.setdefault('OMP_NUM_THREADS','1')
    _os.environ.setdefault('MKL_NUM_THREADS','1')
    _os.environ.setdefault('OPENBLAS_NUM_THREADS','1')
    _os.environ.setdefault('NUMEXPR_NUM_THREADS','1')
    import cv2 as _cv2
    try:
        _cv2.setNumThreads(0)
    except Exception:
        pass
except Exception:
    pass

# å°†é¡¹ç›®æ ¹ç›®å½•ï¼ˆnanodet-jittorï¼‰æ·»åŠ åˆ° Python è·¯å¾„
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

import argparse
import random
import warnings
from types import SimpleNamespace
import copy
import traceback

# JITTOR MIGRATION: å¯¼å…¥ jittor å’Œ numpy
import jittor as jt
import numpy as np

# JITTOR MIGRATION: å¯¼å…¥å·²è¿ç§»çš„å·¥å…·å‡½æ•°å’Œç±»
from nanodet.data.collate import naive_collate
from nanodet.data.dataset import build_dataset
from nanodet.evaluator import build_evaluator
from nanodet.trainer.task import TrainingTask
# JITTOR MIGRATION FIX: ç›´æ¥ä» logger æ¨¡å—å¯¼å…¥æ­£ç¡®çš„ç±»åï¼Œå¹¶è°ƒæ•´å…¶ä»–å¯¼å…¥
from nanodet.util.logger import NanoDetLightningLogger
from nanodet.util import (
    cfg,
    convert_old_model,
    env_utils,
    load_config,
    load_model_weight,
    mkdir,
)

def parse_args():
    """è§£æå‘½ä»¤è¡Œå‚æ•°ã€‚"""
    parser = argparse.ArgumentParser()
    parser.add_argument("config", help="è®­ç»ƒé…ç½®æ–‡ä»¶çš„è·¯å¾„")
    parser.add_argument(
        "--local_rank", default=-1, type=int, help="ç”¨äºåˆ†å¸ƒå¼è®­ç»ƒçš„èŠ‚ç‚¹æ’å"
    )
    parser.add_argument("--seed", type=int, default=None, help="éšæœºç§å­")
    # ä¾¿äºå¿«é€Ÿè°ƒè¯•çš„å¯é€‰å‚æ•°
    parser.add_argument("--max_epochs", type=int, default=None, help="æœ€å¤šè®­ç»ƒçš„ epoch æ•°ï¼ˆå¯é€‰ï¼‰")
    parser.add_argument("--max_train_batches", type=int, default=None, help="æ¯ä¸ª epoch è®­ç»ƒçš„æœ€å¤§ batch æ•°ï¼ˆå¯é€‰ï¼‰")
    parser.add_argument("--max_val_batches", type=int, default=None, help="æ¯æ¬¡éªŒè¯çš„æœ€å¤§ batch æ•°ï¼ˆå¯é€‰ï¼‰")
    parser.add_argument("--warmup_steps", type=int, default=0, help="é¢„çƒ­æ­¥æ•°ï¼Œä»…ç”¨äºè§¦å‘JITç¼–è¯‘ç¼“å­˜ï¼ˆä¸åšéªŒè¯ä¸ä¿å­˜ï¼‰")
    args = parser.parse_args()
    return args

def set_seed(seed):
    """è®¾ç½®éšæœºç§å­ä»¥ç¡®ä¿å¯å¤ç°æ€§ã€‚"""
    random.seed(seed)
    np.random.seed(seed)
    jt.seed(seed)

def main(args):
    # åŠ è½½é…ç½®
    load_config(cfg, args.config)
    # å…è®¸å‘½ä»¤è¡Œè¦†ç›–æ€» epochs æ•°ï¼Œä¾¿äºå¿«é€Ÿå¾®è°ƒ
    if args.max_epochs is not None and args.max_epochs > 0:
        # yacs CfgNode é»˜è®¤ immutableï¼Œå…ˆè§£é”å†è®¾ç½®
        cfg.defrost()
        cfg.schedule.total_epochs = args.max_epochs
        cfg.freeze()
    if cfg.model.arch.head.num_classes != len(cfg.class_names):
        raise ValueError(
            "cfg.model.arch.head.num_classes å¿…é¡»ç­‰äº len(cfg.class_names), "
            f"ä½†å¾—åˆ°äº† {cfg.model.arch.head.num_classes} å’Œ {len(cfg.class_names)}"
        )

    # JITTOR MIGRATION: ä½¿ç”¨ jt.rank è·å–æ’åï¼Œå¹¶è®¾ç½® GPU
    local_rank = jt.rank if jt.world_size > 1 else 0
    if jt.has_cuda:
        jt.flags.use_cuda = 1

    # åˆ›å»ºä¿å­˜ç›®å½•å’Œæ—¥å¿—è®°å½•å™¨
    mkdir(cfg.save_dir)
    # JITTOR MIGRATION FIX: ä½¿ç”¨æ­£ç¡®çš„ç±»åå®ä¾‹åŒ– Logger
    logger = NanoDetLightningLogger(cfg.save_dir)
    logger.dump_cfg(cfg)

    # è®¾ç½®éšæœºç§å­
    if args.seed is not None:
        logger.info(f"è®¾ç½®éšæœºç§å­ä¸º {args.seed}")
        set_seed(args.seed)

    # JITTOR MIGRATION: ç§»é™¤ PyTorch-Lightning å’Œ jt.backends.cudnn çš„è®¾ç½®

    logger.info("æ­£åœ¨è®¾ç½®æ•°æ®...")
    train_dataset = build_dataset(cfg.data.train, "train")
    val_dataset = build_dataset(cfg.data.val, "val") # æ¨¡å¼åº”ä¸º 'val'
    try:
        logger.info(f"æ•°æ®é›†å·²æ„å»ºå®Œæˆ | è®­ç»ƒæ ·æœ¬æ•°: {len(train_dataset)} | éªŒè¯æ ·æœ¬æ•°: {len(val_dataset)}")
        import sys as _sys
        print(f"[Heartbeat] Datasets ready. train={len(train_dataset)} val={len(val_dataset)}", flush=True)
    except Exception:
        pass

    # JITTOR MIGRATION: Jittor çš„ DataLoader ç›´æ¥åœ¨ Dataset å¯¹è±¡ä¸Šé…ç½®
    train_dataloader = train_dataset.set_attrs(
        batch_size=cfg.device.batchsize_per_gpu,
        shuffle=True,
        num_workers=cfg.device.workers_per_gpu,
        collate_batch=naive_collate,
        drop_last=True,
    )
    val_dataloader = val_dataset.set_attrs(
        batch_size=cfg.device.batchsize_per_gpu,
        shuffle=False,
        num_workers=cfg.device.workers_per_gpu,
        collate_batch=naive_collate,
        drop_last=False,
    )
    try:
        logger.info(f"DataLoader å·²å°±ç»ª | train_batches_per_epoch: {len(train_dataloader)} | val_batches: {len(val_dataloader)}")
        print(f"[Heartbeat] Dataloaders ready. train_batches={len(train_dataloader)} val_batches={len(val_dataloader)}", flush=True)
    except Exception:
        pass

    evaluator = build_evaluator(cfg.evaluator, val_dataset)

    logger.info("æ­£åœ¨åˆ›å»ºæ¨¡å‹...")
    task = TrainingTask(cfg, evaluator, logger)

    # åŠ è½½é¢„è®­ç»ƒæ¨¡å‹æƒé‡
    if hasattr(cfg.schedule, 'load_model') and cfg.schedule.load_model:
        lm = cfg.schedule.load_model
        logger.info(f"ğŸ”„ è®¡åˆ’åŠ è½½é¢„è®­ç»ƒæƒé‡: {lm}")
        # æ—¢æ”¯æŒ Jittor .pklï¼Œä¹Ÿæ”¯æŒç›´æ¥æŒ‡å‘ PyTorch .ckpt/.pth
        if isinstance(lm, str) and (lm.endswith('.ckpt') or lm.endswith('.pth')):
            from nanodet.util.check_point import pt_to_jt_checkpoint
            try:
                import torch
            except Exception as e:
                raise RuntimeError('éœ€è¦å®‰è£… PyTorch æ‰èƒ½ä» .ckpt/.pth åŠ è½½æƒé‡') from e
            pt_ckpt = torch.load(lm, map_location='cpu')
            ckpt = pt_to_jt_checkpoint(pt_ckpt, task.model)
        else:
            ckpt = jt.load(lm)
        # ç»Ÿè®¡è½¬æ¢å state_dict å…³é”®ä¿¡æ¯
        if isinstance(ckpt, dict) and 'state_dict' in ckpt:
            sd_keys = list(ckpt['state_dict'].keys())
            n_keys = len(sd_keys)
            has_head_cls = any(k.startswith('head.gfl_cls.') for k in sd_keys)
            has_head_reg = any(k.startswith('head.gfl_reg.') for k in sd_keys)
            logger.info(f"âœ… é¢„è®­ç»ƒæƒé‡é”®æ•°: {n_keys}, å« head.gfl_cls: {has_head_cls}, å« head.gfl_reg: {has_head_reg}")
        # ä¼˜å…ˆä½¿ç”¨ avg_model.* (EMA) æƒé‡åˆ†æ”¯ï¼Œæé«˜è¯„ä¼°ç¨³å®šæ€§ï¼ˆå¦‚æœ‰çš„è¯ï¼‰
        if isinstance(ckpt, dict) and "state_dict" in ckpt:
            has_ema = any(k.startswith("avg_model.") for k in ckpt["state_dict"].keys())
            if has_ema:
                from nanodet.util.check_point import convert_avg_params
                ema_state = convert_avg_params(ckpt)
                ckpt = dict(state_dict=ema_state)
        if isinstance(ckpt, dict) and "pytorch-lightning_version" in ckpt:
            warnings.warn("è­¦å‘Šï¼æ‚¨æ­£åœ¨åŠ è½½ä¸€ä¸ª PyTorch Lightning æ£€æŸ¥ç‚¹ã€‚è¯·ç¡®ä¿å…¶ä¸å½“å‰æ¨¡å‹å…¼å®¹ã€‚")
        elif isinstance(ckpt, dict) and "state_dict" not in ckpt:
            # å‡è®¾æ˜¯æ—§æ ¼å¼
            warnings.warn("è­¦å‘Šï¼æ—§çš„ .pth æ£€æŸ¥ç‚¹æ ¼å¼å·²å¼ƒç”¨ã€‚è¯·ä½¿ç”¨ tools/convert_old_checkpoint.py è¿›è¡Œè½¬æ¢ã€‚")
            ckpt = convert_old_model(ckpt)
        # å¾®è°ƒæ—¶ï¼šä¿ç•™ backboneã€fpnï¼Œé‡ç½® head çš„æœ€åè¾“å‡ºå±‚ï¼ˆé˜²æ­¢åŠ è½½è¿‡æ‹Ÿåˆçš„åç½®ï¼‰
        if getattr(cfg.schedule, 'finetune_reset_head', False):
            model_sd = task.model.state_dict()
            head_prefixes = ['head.gfl_cls.', 'head.gfl_reg.']
            if isinstance(ckpt, dict) and 'state_dict' in ckpt:
                sd = ckpt['state_dict']
                for k in list(sd.keys()):
                    if any(k.startswith(p) for p in head_prefixes):
                        sd.pop(k)
        # å®é™…åŠ è½½
        load_model_weight(task.model, ckpt, logger)
        # è‹¥å­˜åœ¨ avg_modelï¼ˆEMAï¼‰ï¼Œå°†å…¶ä¸å½“å‰ model åŒæ­¥ï¼Œé¿å…éªŒè¯æ—¶ç”¨åˆ°æœªåˆå§‹åŒ–çš„ avg_model
        try:
            if getattr(task, 'avg_model', None) is not None:
                task.avg_model.load_state_dict(task.model.state_dict())
        except Exception as e:
            logger.warning(f"avg_model åŒæ­¥å¤±è´¥: {e}")
        # åŠ è½½åå¿«é€Ÿæ‰“å°è‹¥å¹²å…³é”®å±‚çš„èŒƒæ•°ï¼Œç¡®è®¤ééšæœºåˆå§‹åŒ–
        try:
            import numpy as _np
            msd = task.model.state_dict()
            def _norm(name):
                v = msd.get(name, None)
                return None if v is None else float(_np.linalg.norm(v.numpy()))
            logger.info(
                "ğŸ” å‚æ•°èŒƒæ•° | head.gfl_cls.0.weight: {:.4f} | head.gfl_reg.0.weight: {:.4f}".format(
                    _norm('head.gfl_cls.0.weight') or -1.0,
                    _norm('head.gfl_reg.0.weight') or -1.0,
                )
            )
        except Exception as e:
            logger.warning(f"å‚æ•°èŒƒæ•°æ£€æŸ¥å¤±è´¥: {e}")
        logger.info(f"ä» {cfg.schedule.load_model} åŠ è½½äº†æ¨¡å‹æƒé‡")

    # JITTOR MIGRATION: æ›¿æ¢ PyTorch Lightning Trainer ä¸ºæ‰‹åŠ¨è®­ç»ƒå¾ªç¯

    # é…ç½®ä¼˜åŒ–å™¨å’Œå­¦ä¹ ç‡è°ƒåº¦å™¨
    optimizer, scheduler = task.configure_optimizers()

    # å¦‚æœæ˜¯å¤š GPU è®­ç»ƒï¼Œä½¿ç”¨ DataParallel åŒ…è£…æ¨¡å‹
    if jt.world_size > 1:
        task.model = jt.DataParallel(task.model)
        env_utils.set_multi_processing(distributed=True)

    logger.info("å¼€å§‹è®­ç»ƒ...")
    logger.info(f"æ€»å…± {cfg.schedule.total_epochs} ä¸ª epochï¼Œæ¯ä¸ª epoch {len(train_dataloader)} ä¸ªæ‰¹æ¬¡")
    logger.info("=" * 80)

    # é¢„çƒ­é˜¶æ®µï¼šä»…ç”¨äºè§¦å‘ JIT ç¼–è¯‘ç¼“å­˜
    if args.warmup_steps and args.warmup_steps > 0:
        logger.info(f"ğŸš€ å¼€å§‹é¢„çƒ­ warmupï¼Œå…± {args.warmup_steps} æ­¥ï¼ˆä¸ä¿å­˜æ¨¡å‹/ä¸éªŒè¯ï¼‰...")
        task.model.train()
        warmup_losses = []
        warmup_batches = iter(train_dataloader)
        for wi in range(args.warmup_steps):
            try:
                batch = next(warmup_batches)
            except StopIteration:
                warmup_batches = iter(train_dataloader)
                batch = next(warmup_batches)
            try:
                res = task.training_step(batch, wi, SimpleNamespace(current_epoch=0, global_step=wi, optimizer=None, num_training_batches=len(train_dataloader), num_val_batches=len(val_dataloader)))
                warmup_loss = res['loss'] if isinstance(res, dict) else res
                # åªåšä¸€æ¬¡ optimizer.step ä»¥ç¡®ä¿åå‘è·¯å¾„è¢«ç¼–è¯‘
                optimizer.step(warmup_loss)
                warmup_losses.append(float(warmup_loss))
                if (wi+1) % max(1, args.warmup_steps//5) == 0:
                    logger.info(f"Warmup {wi+1}/{args.warmup_steps} | loss:{np.mean(warmup_losses):.4f}")
            except Exception as e:
                logger.warning(f"Warmup ç¬¬ {wi} æ­¥å¤±è´¥: {e}")
                traceback.print_exc()
                continue
        logger.info("âœ… é¢„çƒ­å®Œæˆï¼Œå¼€å§‹æ­£å¼è®­ç»ƒ")

    global_step = 0
    start_epoch = 0
    best_ap = 0.0
    mid_eval_done = False  # åœ¨å…¨å±€ç¬¬100ä¸ªiterè§¦å‘ä¸€æ¬¡å¿«é€Ÿè¯„ä¼°
    mid_eval_batches = 50  # è¯„ä¼°ä½¿ç”¨å‰50ä¸ªval batchï¼Œå¿«é€Ÿä¼°è®¡mAPæ˜¯å¦ä¿æŒåœ¨~35

    # å¯¼å…¥æ—¶é—´æ¨¡å—
    import time

    total_epochs = cfg.schedule.total_epochs if args.max_epochs is None else min(cfg.schedule.total_epochs, args.max_epochs)
    for epoch in range(start_epoch, total_epochs):
        epoch_start_time = time.time()
        task.on_train_epoch_start(epoch)

        # æ¨¡æ‹Ÿä¸€ä¸ª trainer å¯¹è±¡
        trainer_mock = SimpleNamespace(
            current_epoch=epoch,
            global_step=global_step,
            num_training_batches=len(train_dataloader),
            num_val_batches=len(val_dataloader),
            optimizer=optimizer
        )

        # ğŸ¯ PyTorch é£æ ¼çš„è®­ç»ƒå¾ªç¯ - å¸¦å®æ—¶è¿›åº¦æ˜¾ç¤º
        task.model.train()
        epoch_losses = []

        # è·å–å½“å‰å­¦ä¹ ç‡ - Jittor ä¼˜åŒ–å™¨å…¼å®¹æ€§ä¿®å¤
        current_lr = 0.001  # é»˜è®¤å€¼
        try:
            if hasattr(optimizer, 'param_groups') and len(optimizer.param_groups) > 0:
                current_lr = optimizer.param_groups[0]['lr']
            elif hasattr(optimizer, 'lr'):
                current_lr = optimizer.lr
            elif hasattr(optimizer, 'learning_rate'):
                current_lr = optimizer.learning_rate
        except:
            current_lr = 0.001  # ä¿æŒé»˜è®¤å€¼

        logger.info(f"å¼€å§‹ Epoch {epoch+1}/{cfg.schedule.total_epochs} | LR: {current_lr:.6f}")

        # ä½¿ç”¨ç®€å•çš„è¿›åº¦æ˜¾ç¤ºï¼Œé¿å… tqdm å…¼å®¹æ€§é—®é¢˜
        batch_count = len(train_dataloader)
        print_interval = max(1, batch_count // 20)  # æ¯5%æ˜¾ç¤ºä¸€æ¬¡è¿›åº¦

        for i, batch in enumerate(train_dataloader):
            trainer_mock.global_step = global_step

            try:
                # è°ƒè¯•ï¼šæ£€æŸ¥å¹¶æ‰“å° batch çš„å…³é”®å­—æ®µå½¢çŠ¶ï¼Œé¿å…åŠ¨æ€å½¢çŠ¶å¯¼è‡´çš„ç¼“å­˜çˆ†ç‚¸
                if i < 3:  # ä»…å‰å‡ ä¸ªbatchæ‰“å°
                    try:
                        img = batch.get("img")
                        if isinstance(img, jt.Var):
                            logger.info(f"Batch {i} img shape: {tuple(img.shape)}")
                        else:
                            logger.info(f"Batch {i} img type: {type(img)}")
                        for k in ["gt_bboxes", "gt_labels", "gt_bboxes_ignore"]:
                            v = batch.get(k)
                            if v is None:
                                continue
                            if isinstance(v, jt.Var):
                                logger.info(f"  {k} jt.Var shape: {tuple(v.shape)}")
                            elif isinstance(v, np.ndarray):
                                logger.info(f"  {k} np shape: {v.shape} dtype:{v.dtype}")
                            else:
                                try:
                                    logger.info(f"  {k} type:{type(v)} len:{len(v)}")
                                except Exception:
                                    logger.info(f"  {k} type:{type(v)}")
                    except Exception as e_dbg:
                        logger.warning(f"Batch {i} debug inspect failed: {e_dbg}")

                # å‰å‘ä¼ æ’­å’ŒæŸå¤±è®¡ç®—
                training_result = task.training_step(batch, i, trainer_mock)
                # ğŸ”§ ä¿®å¤ï¼šæ­£ç¡®å¤„ç†è¿”å›å€¼
                if isinstance(training_result, dict):
                    training_loss = training_result['loss']
                else:
                    training_loss = training_result
                # ğŸ”§ ä¿®å¤ï¼šé¿å…è®¡ç®—å›¾æ–­è£‚ï¼Œå»¶è¿Ÿè·å–æŸå¤±å€¼ç”¨äºè®°å½•
                loss_value = float(training_loss)  # Jittor ä¼šè‡ªåŠ¨å¤„ç†
                epoch_losses.append(loss_value)

                # åå‘ä¼ æ’­
                optimizer.step(training_loss)
                task.on_train_batch_end(global_step)
                global_step += 1

                # ğŸ¯ å®æ—¶è¿›åº¦æ˜¾ç¤º
                if (i + 1) % print_interval == 0 or i == 0:
                    progress = (i + 1) / batch_count * 100
                    avg_loss = np.mean(epoch_losses)
                    # éµå¾ªç”¨æˆ·è¦æ±‚çš„æ—¥å¿—æ ¼å¼
                    # [NanoDet][MM-DD HH:MM:SS]INFO: Train|Epoch1/50|Iter0(1/108)| mem:5.06G| lr:1.00e-06| loss_qfl:...|
                    # Jittor å½“å‰æ—  used_cuda_memï¼Œå¯å®‰å…¨ç½®0é¿å… AttributeError
                    mem_gb = 0.0
                    iter_str = f"Iter{trainer_mock.global_step}({i+1}/{batch_count})"
                    # å½“ loss_states å¯ç”¨æ—¶é€é¡¹æ‰“å°ï¼›å¦åˆ™å›é€€ä¸º Loss/Avg
                    base = f"Train|Epoch{epoch+1}/{cfg.schedule.total_epochs}|{iter_str}| mem:{mem_gb:.2f}G| lr:{current_lr:.2e}| "
                    try:
                        if isinstance(training_result, dict) and 'loss_states' in training_result:
                            loss_states = training_result['loss_states']
                            for k, v in loss_states.items():
                                try:
                                    v_mean = v.mean() if hasattr(v, 'numel') and v.numel()>1 else v
                                    base += f"{k}:{float(v_mean):.4f}| "
                                except Exception:
                                    pass
                        else:
                            base += f"loss:{loss_value:.4f}| avg:{avg_loss:.4f}| "
                    except Exception:
                        base += f"loss:{loss_value:.4f}| avg:{avg_loss:.4f}| "
                    logger.info(base)

            except Exception as e:
                # è¾“å‡ºæ›´è¯¦ç»†çš„æ‰¹æ¬¡å…³é”®ä¿¡æ¯ï¼Œå¸®åŠ©å®šä½é—®é¢˜
                try:
                    shapes = {}
                    if isinstance(batch, dict):
                        for k, v in batch.items():
                            if isinstance(v, jt.Var):
                                shapes[k] = tuple(v.shape)
                            elif isinstance(v, np.ndarray):
                                shapes[k] = v.shape
                            else:
                                shapes[k] = type(v).__name__
                    logger.error(f"è®­ç»ƒæ‰¹æ¬¡ {i} å¤±è´¥: {e}. batch keys: {list(batch.keys()) if isinstance(batch, dict) else type(batch)} shapes: {shapes}")
                except Exception:
                    logger.error(f"è®­ç»ƒæ‰¹æ¬¡ {i} å¤±è´¥: {e}")
                # æ— è®ºå¦‚ä½•éƒ½æ‰“å°å®Œæ•´å †æ ˆï¼Œä¾¿äºå®šä½
                traceback.print_exc()
                continue

            # æ—©åœï¼šé™åˆ¶æ¯ä¸ª epoch çš„è®­ç»ƒ batch æ•°
            if args.max_train_batches is not None and (i + 1) >= args.max_train_batches:
                break

        # åœ¨å…¨å±€ç¬¬100ä¸ªiterè§¦å‘ä¸€æ¬¡å¿«é€Ÿè¯„ä¼°ï¼ˆä»…ä¸€æ¬¡ï¼‰
        if (not mid_eval_done) and global_step >= 100:
            try:
                logger.info("ğŸ§ª è§¦å‘ä¸­é€”å¿«é€Ÿè¯„ä¼°ï¼šä½¿ç”¨å‰50ä¸ªval batch ä¼°è®¡ mAPï¼Œä»¥ç¡®è®¤æœªæš´è·Œâ€¦")
                task.model.eval()
                task.on_validation_epoch_start()
                val_outputs = []
                for vi, vbatch in enumerate(val_dataloader):
                    if vi >= mid_eval_batches:
                        break
                    with jt.no_grad():
                        dets = task.validation_step(vbatch, vi, trainer_mock)
                        val_outputs.append(dets)
                metrics = task.validation_epoch_end(val_outputs, epoch)
                if metrics and 'mAP' in metrics:
                    logger.info(f"ğŸ§ª ä¸­é€”è¯„ä¼° mAP: {metrics['mAP']:.4f} | AP50: {metrics.get('AP_50','')}")
                else:
                    logger.info("ğŸ§ª ä¸­é€”è¯„ä¼°å®Œæˆï¼Œä½†æœªè·å–åˆ° mAP æŒ‡æ ‡")
            except Exception as e:
                logger.warning(f"ä¸­é€”è¯„ä¼°å¤±è´¥: {e}")
            finally:
                mid_eval_done = True
                task.model.train()

        # æ›´æ–°å­¦ä¹ ç‡
        if scheduler:
            scheduler.step()

        # è®¡ç®— epoch ç»Ÿè®¡ä¿¡æ¯
        epoch_time = time.time() - epoch_start_time
        avg_loss = np.mean(epoch_losses)

        # ğŸ¯ PyTorch é£æ ¼çš„ epoch æ€»ç»“
        logger.info(f"Epoch {epoch+1:3d}/{cfg.schedule.total_epochs} | "
                   f"Loss: {avg_loss:.4f} | "
                   f"Time: {epoch_time:.1f}s | "
                   f"LR: {current_lr:.6f}")

        # è®°å½•åˆ° CSV ä¾¿äºç”»æ›²çº¿
        try:
            import csv, os
            metrics_path = os.path.join(cfg.save_dir, 'logs', 'metrics.csv')
            os.makedirs(os.path.dirname(metrics_path), exist_ok=True)
            header = ['epoch','avg_train_loss','mAP','AP50','AP75']
            # å…ˆå†™å…¥å ä½ï¼ŒmAP å°†åœ¨éªŒè¯åå†™å…¥
            file_exists = os.path.exists(metrics_path)
            with open(metrics_path, 'a', newline='') as f:
                w = csv.writer(f)
                if not file_exists:
                    w.writerow(header)
                w.writerow([epoch+1, float(avg_loss), '', '', ''])
        except Exception as e:
            logger.warning(f"ä¿å­˜ metrics.csv å¤±è´¥: {e}")

        # éªŒè¯å’Œæµ‹è¯„
        if (epoch + 1) % cfg.schedule.val_intervals == 0:
            logger.info(f"ğŸ” å¼€å§‹éªŒè¯ Epoch {epoch + 1}...")

            task.model.eval()
            task.on_validation_epoch_start()
            val_outputs = []

            # éªŒè¯å¾ªç¯ - ç®€å•è¿›åº¦æ˜¾ç¤º
            val_batch_count = len(val_dataloader)
            val_print_interval = max(1, val_batch_count // 10)

            for i, batch in enumerate(val_dataloader):
                trainer_mock.global_step = global_step
                try:
                    with jt.no_grad():
                        dets = task.validation_step(batch, i, trainer_mock)
                        val_outputs.append(dets)

                    # éªŒè¯è¿›åº¦æ˜¾ç¤º
                    if (i + 1) % val_print_interval == 0 or i == 0:
                        val_progress = (i + 1) / val_batch_count * 100
                        print(f"  éªŒè¯è¿›åº¦: [{i+1:4d}/{val_batch_count}] ({val_progress:5.1f}%)", flush=True)

                except Exception as e:
                    logger.error(f"éªŒè¯æ‰¹æ¬¡ {i} å¤±è´¥: {e}")
                    continue

                # æ—©åœï¼šé™åˆ¶éªŒè¯ batch æ•°
                if args.max_val_batches is not None and (i + 1) >= args.max_val_batches:
                    break

            # ğŸ¯ è‡ªåŠ¨è°ƒç”¨æµ‹è¯„å·¥å…·
            try:
                metrics = task.validation_epoch_end(val_outputs, epoch)

                # æå–å…³é”®æŒ‡æ ‡
                if metrics and 'mAP' in metrics:
                    current_ap = metrics['mAP']
                    ap50 = metrics.get('AP_50', '')
                    ap75 = metrics.get('AP_75', '')
                    logger.info(f"ğŸ“Š éªŒè¯ç»“æœ | mAP: {current_ap:.4f}")

                    # å°† mAP å†™å› CSVï¼ˆæ›´æ–°è¯¥ epoch è¡Œï¼‰
                    try:
                        import csv, os
                        metrics_path = os.path.join(cfg.save_dir, 'logs', 'metrics.csv')
                        # è¯»å‡ºæ‰€æœ‰è¡Œï¼Œæ›´æ–°æœ€åä¸€è¡Œçš„ mAP/AP50/AP75
                        rows = []
                        if os.path.exists(metrics_path):
                            with open(metrics_path, 'r') as f:
                                rows = list(csv.reader(f))
                        if rows:
                            last = rows[-1]
                            if last and last[0] == str(epoch+1):
                                last[2] = f"{float(current_ap):.6f}"
                                last[3] = f"{float(ap50):.6f}" if ap50 != '' else ''
                                last[4] = f"{float(ap75):.6f}" if ap75 != '' else ''
                                rows[-1] = last
                                with open(metrics_path, 'w', newline='') as f:
                                    csv.writer(f).writerows(rows)
                    except Exception as e:
                        logger.warning(f"æ›´æ–° metrics.csv å¤±è´¥: {e}")

                    # ä¿å­˜æœ€ä½³æ¨¡å‹
                    if current_ap > best_ap:
                        best_ap = current_ap
                        if jt.rank == 0:
                            best_model_path = os.path.join(cfg.save_dir, "model_best.ckpt")
                            task.model.save(best_model_path)
                            logger.info(f"ğŸ† æ–°çš„æœ€ä½³æ¨¡å‹ï¼mAP: {best_ap:.4f} -> {best_model_path}")
                else:
                    logger.info("ğŸ“Š éªŒè¯å®Œæˆï¼Œä½†æœªè·å–åˆ° mAP æŒ‡æ ‡")

            except Exception as e:
                logger.error(f"éªŒè¯è¯„ä¼°å¤±è´¥: {e}")

            logger.info("-" * 80)

        # ä¿å­˜æœ€æ–°æ¨¡å‹
        if jt.rank == 0:
            task.model.save(os.path.join(cfg.save_dir, "model_last.ckpt"))
            # ç»˜åˆ¶/æ›´æ–° loss & mAP æ›²çº¿
            try:
                import csv, os
                import matplotlib
                matplotlib.use('Agg')
                import matplotlib.pyplot as plt
                metrics_path = os.path.join(cfg.save_dir, 'logs', 'metrics.csv')
                if os.path.exists(metrics_path):
                    epochs, tloss, mapv = [], [], []
                    with open(metrics_path, 'r') as f:
                        for i, row in enumerate(csv.reader(f)):
                            if i == 0: continue
                            if not row: continue
                            epochs.append(int(row[0]))
                            tloss.append(float(row[1]) if row[1] else None)
                            mapv.append(float(row[2]) if row[2] else None)
                    # ç”»å›¾
                    plt.figure(figsize=(8,4))
                    if any(v is not None for v in tloss):
                        plt.plot(epochs, tloss, '-o', label='avg_train_loss')
                    if any(v is not None for v in mapv):
                        plt.plot(epochs, mapv, '-o', label='mAP')
                    plt.xlabel('epoch')
                    plt.grid(True, ls='--', alpha=0.4)
                    plt.legend()
                    plt.tight_layout()
                    plt.savefig(os.path.join(cfg.save_dir, 'logs', 'curves.png'), dpi=150)
            except Exception as e:
                logger.warning(f"ç»˜åˆ¶æ›²çº¿å¤±è´¥: {e}")

    # ğŸ¯ è®­ç»ƒå®Œæˆæ€»ç»“
    logger.info("=" * 80)
    logger.info("ğŸ‰ è®­ç»ƒå®Œæˆï¼")
    logger.info(f"ğŸ“Š æœ€ä½³ mAP: {best_ap:.4f}")
    logger.info(f"ğŸ’¾ æ¨¡å‹ä¿å­˜åœ¨: {cfg.save_dir}")
    logger.info("=" * 80)


if __name__ == "__main__":
    args = parse_args()
    main(args)
