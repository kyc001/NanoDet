#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
é€ä¸ªå¯¹æ¯”Jittorå’ŒPyTorchæ¨¡å‹å‚æ•°
ä½¿ç”¨è½¬æ¢è„šæœ¬è¿›è¡Œç²¾ç¡®æ£€æŸ¥
"""

import os
import sys
import json
import jittor as jt
import torch

# æ·»åŠ è·¯å¾„
sys.path.append('/home/kyc/project/nanodet/nanodet-jittor')
from nanodet.model.arch.nanodet_plus import NanoDetPlus


def load_jittor_model():
    """åŠ è½½Jittoræ¨¡å‹"""
    print("åˆ›å»ºJittoræ¨¡å‹...")

    # åˆ›å»ºé…ç½®å­—å…¸
    backbone_cfg = {
        'name': 'ShuffleNetV2',
        'model_size': '1.0x',
        'out_stages': [2, 3, 4],
        'activation': 'LeakyReLU',
        'pretrain': True
    }

    fpn_cfg = {
        'name': 'GhostPAN',
        'in_channels': [116, 232, 464],
        'out_channels': 96,
        'kernel_size': 5,
        'num_extra_level': 1,
        'use_depthwise': True,
        'activation': 'LeakyReLU'
    }

    head_cfg = {
        'name': 'NanoDetPlusHead',
        'num_classes': 20,
        'input_channel': 96,
        'feat_channels': 96,
        'stacked_convs': 2,
        'kernel_size': 5,
        'strides': [8, 16, 32, 64],
        'activation': 'LeakyReLU',
        'reg_max': 7,
        'norm_cfg': {'type': 'BN'},
        'loss': {
            'loss_qfl': {
                'name': 'QualityFocalLoss',
                'use_sigmoid': True,
                'beta': 2.0,
                'loss_weight': 1.0
            },
            'loss_dfl': {
                'name': 'DistributionFocalLoss',
                'loss_weight': 0.25
            },
            'loss_bbox': {
                'name': 'GIoULoss',
                'loss_weight': 2.0
            }
        }
    }

    # åˆ›å»ºaux_headé…ç½®
    aux_head_cfg = {
        'name': 'NanoDetPlusHead',
        'num_classes': 20,
        'input_channel': 192,  # 96 * 2
        'feat_channels': 96,
        'stacked_convs': 2,
        'kernel_size': 5,
        'strides': [8, 16, 32, 64],
        'activation': 'LeakyReLU',
        'reg_max': 7,
        'norm_cfg': {'type': 'BN'},
        'loss': {
            'loss_qfl': {
                'name': 'QualityFocalLoss',
                'use_sigmoid': True,
                'beta': 2.0,
                'loss_weight': 1.0
            },
            'loss_dfl': {
                'name': 'DistributionFocalLoss',
                'loss_weight': 0.25
            },
            'loss_bbox': {
                'name': 'GIoULoss',
                'loss_weight': 2.0
            }
        }
    }

    # åˆ›å»ºå®Œæ•´æ¨¡å‹
    model = NanoDetPlus(backbone_cfg, fpn_cfg, aux_head_cfg, head_cfg)

    # ç»Ÿè®¡å‚æ•°
    total_params = sum(p.numel() for p in model.parameters())
    print(f"âœ“ Jittoræ¨¡å‹å‚æ•°æ•°é‡: {total_params:,}")

    return model, None


def analyze_jittor_model_structure(model):
    """åˆ†æJittoræ¨¡å‹ç»“æ„"""
    print("\n" + "=" * 80)
    print("ğŸ” åˆ†æJittoræ¨¡å‹ç»“æ„")
    print("=" * 80)
    
    jittor_params = {}
    
    # è·å–æ‰€æœ‰å‚æ•°
    for name, param in model.named_parameters():
        jittor_params[name] = {
            "shape": list(param.shape),
            "numel": param.numel(),
            "dtype": str(param.dtype),
        }
        
        # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        if param.dtype in [jt.float32, jt.float64, jt.float16]:
            jittor_params[name].update({
                "mean": float(param.mean()),
                "std": float(param.std()),
                "min": float(param.min()),
                "max": float(param.max())
            })
    
    print(f"âœ“ Jittoræ¨¡å‹æ€»å‚æ•°æ•°: {len(jittor_params)}")
    
    # æŒ‰æ¨¡å—åˆ†ç±»
    modules = {"backbone": {}, "fpn": {}, "head": {}, "other": {}}
    
    for name, info in jittor_params.items():
        if "backbone" in name:
            modules["backbone"][name] = info
        elif "fpn" in name:
            modules["fpn"][name] = info
        elif "head" in name:
            modules["head"][name] = info
        else:
            modules["other"][name] = info
    
    # æ‰“å°æ¨¡å—ç»Ÿè®¡
    for module_name, module_params in modules.items():
        if module_params:
            total_params = sum(p["numel"] for p in module_params.values())
            print(f"\nğŸ”¹ {module_name.upper()}:")
            print(f"   å‚æ•°æ•°é‡: {len(module_params)}")
            print(f"   æ€»å‚æ•°é‡: {total_params:,}")
            
            # æ˜¾ç¤ºå…³é”®å‚æ•°
            for param_name, param_info in list(module_params.items())[:3]:
                print(f"   {param_name}: {param_info['shape']}")
    
    return jittor_params


def compare_key_parameters():
    """å¯¹æ¯”å…³é”®å‚æ•°"""
    print("\n" + "=" * 80)
    print("ğŸ” å¯¹æ¯”å…³é”®å‚æ•°")
    print("=" * 80)
    
    # åŠ è½½PyTorchåˆ†æç»“æœ
    pytorch_analysis_file = "pytorch_model_analysis.json"
    if not os.path.exists(pytorch_analysis_file):
        print("âŒ è¯·å…ˆè¿è¡Œ analyze_pytorch_model_detailed.py")
        return False
    
    with open(pytorch_analysis_file, 'r') as f:
        pytorch_analysis = json.load(f)
    
    # åŠ è½½Jittoræ¨¡å‹
    jittor_model, cfg = load_jittor_model()
    jittor_params = analyze_jittor_model_structure(jittor_model)
    
    print("\n" + "=" * 80)
    print("ğŸ” å…³é”®å‚æ•°å¯¹æ¯”")
    print("=" * 80)
    
    # å¯¹æ¯”è¾“å‡ºå±‚
    print("\nğŸ”¹ è¾“å‡ºå±‚å¯¹æ¯”:")
    
    # PyTorchè¾“å‡ºå±‚
    pytorch_head = pytorch_analysis.get("head", {})
    pytorch_output_layers = {}
    for name, info in pytorch_head.items():
        if "gfl_cls" in name and "weight" in name:
            pytorch_output_layers[name] = info["shape"]
    
    print("PyTorchè¾“å‡ºå±‚:")
    for name, shape in pytorch_output_layers.items():
        print(f"   {name}: {shape}")
    
    # Jittorè¾“å‡ºå±‚
    jittor_output_layers = {}
    for name, info in jittor_params.items():
        if "gfl_cls" in name and "weight" in name:
            jittor_output_layers[name] = info["shape"]
    
    print("\nJittorè¾“å‡ºå±‚:")
    for name, shape in jittor_output_layers.items():
        print(f"   {name}: {shape}")
    
    # å¯¹æ¯”backboneç¬¬ä¸€å±‚
    print("\nğŸ”¹ Backboneç¬¬ä¸€å±‚å¯¹æ¯”:")
    
    # PyTorch backbone
    pytorch_backbone = pytorch_analysis.get("backbone", {})
    pytorch_first_conv = None
    for name, info in pytorch_backbone.items():
        if "conv1.0.weight" in name:
            pytorch_first_conv = (name, info["shape"])
            break
    
    if pytorch_first_conv:
        print(f"PyTorch: {pytorch_first_conv[0]} -> {pytorch_first_conv[1]}")
    
    # Jittor backbone
    jittor_first_conv = None
    for name, info in jittor_params.items():
        if "backbone" in name and "conv" in name and "weight" in name:
            jittor_first_conv = (name, info["shape"])
            break
    
    if jittor_first_conv:
        print(f"Jittor: {jittor_first_conv[0]} -> {jittor_first_conv[1]}")
    
    # æ£€æŸ¥åŒ¹é…æƒ…å†µ
    print("\nğŸ”¹ åŒ¹é…æ£€æŸ¥:")
    
    if pytorch_first_conv and jittor_first_conv:
        if pytorch_first_conv[1] == jittor_first_conv[1]:
            print("âœ… Backboneç¬¬ä¸€å±‚å½¢çŠ¶åŒ¹é…")
        else:
            print("âŒ Backboneç¬¬ä¸€å±‚å½¢çŠ¶ä¸åŒ¹é…")
            print(f"   PyTorch: {pytorch_first_conv[1]}")
            print(f"   Jittor: {jittor_first_conv[1]}")
    
    # æ£€æŸ¥è¾“å‡ºå±‚åŒ¹é…
    if len(pytorch_output_layers) > 0 and len(jittor_output_layers) > 0:
        pytorch_shapes = list(pytorch_output_layers.values())
        jittor_shapes = list(jittor_output_layers.values())
        
        if len(pytorch_shapes) == len(jittor_shapes):
            all_match = all(p == j for p, j in zip(pytorch_shapes, jittor_shapes))
            if all_match:
                print("âœ… è¾“å‡ºå±‚å½¢çŠ¶å®Œå…¨åŒ¹é…")
            else:
                print("âŒ è¾“å‡ºå±‚å½¢çŠ¶ä¸åŒ¹é…")
                for i, (p, j) in enumerate(zip(pytorch_shapes, jittor_shapes)):
                    if p != j:
                        print(f"   å±‚{i}: PyTorch{p} vs Jittor{j}")
        else:
            print("âŒ è¾“å‡ºå±‚æ•°é‡ä¸åŒ¹é…")
            print(f"   PyTorch: {len(pytorch_shapes)} å±‚")
            print(f"   Jittor: {len(jittor_shapes)} å±‚")
    
    return True


def test_model_output():
    """æµ‹è¯•æ¨¡å‹è¾“å‡ºå½¢çŠ¶"""
    print("\n" + "=" * 80)
    print("ğŸ” æµ‹è¯•æ¨¡å‹è¾“å‡ºå½¢çŠ¶")
    print("=" * 80)
    
    # åŠ è½½Jittoræ¨¡å‹
    jittor_model, cfg = load_jittor_model()
    
    # åˆ›å»ºæµ‹è¯•è¾“å…¥
    test_input = jt.randn(1, 3, 320, 320)
    
    print("è¿›è¡Œå‰å‘æ¨ç†...")
    with jt.no_grad():
        output = jittor_model(test_input)
    
    print(f"âœ“ è¾“å‡ºå½¢çŠ¶: {output.shape}")
    print(f"âœ“ è¾“å‡ºæ•°æ®ç±»å‹: {output.dtype}")
    
    # åˆ†æè¾“å‡ºé€šé“
    if len(output.shape) == 3:  # [B, N, C]
        batch_size, num_anchors, num_channels = output.shape
        print(f"âœ“ æ‰¹æ¬¡å¤§å°: {batch_size}")
        print(f"âœ“ é”šç‚¹æ•°é‡: {num_anchors}")
        print(f"âœ“ è¾“å‡ºé€šé“: {num_channels}")
        
        # åˆ†æé€šé“åˆ†é…
        expected_cls_channels = 20  # VOC 20ç±»
        expected_reg_channels = 32  # 4 * (7+1) = 32
        expected_total = expected_cls_channels + expected_reg_channels
        
        print(f"\nğŸ”¹ é€šé“åˆ†æ:")
        print(f"   æœŸæœ›åˆ†ç±»é€šé“: {expected_cls_channels}")
        print(f"   æœŸæœ›å›å½’é€šé“: {expected_reg_channels}")
        print(f"   æœŸæœ›æ€»é€šé“: {expected_total}")
        print(f"   å®é™…æ€»é€šé“: {num_channels}")
        
        if num_channels == expected_total:
            print("âœ… è¾“å‡ºé€šé“æ•°æ­£ç¡®")
        else:
            print("âŒ è¾“å‡ºé€šé“æ•°ä¸æ­£ç¡®")
            print(f"   å·®å¼‚: {num_channels - expected_total}")
    
    return True


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ å¼€å§‹å¯¹æ¯”Jittorå’ŒPyTorchæ¨¡å‹å‚æ•°")
    
    # å¯¹æ¯”å…³é”®å‚æ•°
    success = compare_key_parameters()
    
    if success:
        # æµ‹è¯•æ¨¡å‹è¾“å‡º
        test_model_output()
        print("\nğŸ‰ å‚æ•°å¯¹æ¯”å®Œæˆ!")
    else:
        print("\nâŒ å‚æ•°å¯¹æ¯”å¤±è´¥")
        return False
    
    return True


if __name__ == '__main__':
    success = main()
    sys.exit(0 if success else 1)
