#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
æ·±å…¥ç ”ç©¶Jittorå‚æ•°æœºåˆ¶
æ‰¾å‡ºBatchNormç»Ÿè®¡å‚æ•°å’ŒScaleå‚æ•°çš„æ­£ç¡®å¤„ç†æ–¹å¼
"""

import jittor as jt
from jittor import nn
import torch
import torch.nn as torch_nn


def investigate_jittor_batchnorm():
    """ç ”ç©¶Jittor BatchNormçš„å‚æ•°æœºåˆ¶"""
    print("ğŸ” ç ”ç©¶Jittor BatchNormå‚æ•°æœºåˆ¶")
    print("=" * 60)
    
    # åˆ›å»ºJittor BatchNorm
    jittor_bn = nn.BatchNorm(64)
    
    print("Jittor BatchNormå±æ€§:")
    for name in dir(jittor_bn):
        if not name.startswith('_'):
            attr = getattr(jittor_bn, name)
            if hasattr(attr, 'shape'):
                print(f"  {name}: {attr.shape} - {type(attr)}")
    
    print(f"\nJittor BatchNorm named_parameters():")
    for name, param in jittor_bn.named_parameters():
        print(f"  {name}: {param.shape}")
    
    print(f"\nJittor BatchNorm parameters():")
    params = list(jittor_bn.parameters())
    print(f"  æ€»æ•°: {len(params)}")
    for i, param in enumerate(params):
        print(f"  å‚æ•°{i}: {param.shape}")
    
    # å¯¹æ¯”PyTorch BatchNorm
    print(f"\n" + "=" * 60)
    print("å¯¹æ¯”PyTorch BatchNormå‚æ•°æœºåˆ¶")
    
    torch_bn = torch_nn.BatchNorm2d(64)
    
    print(f"\nPyTorch BatchNorm named_parameters():")
    for name, param in torch_bn.named_parameters():
        print(f"  {name}: {param.shape}")
    
    print(f"\nPyTorch BatchNorm named_buffers():")
    for name, buffer in torch_bn.named_buffers():
        print(f"  {name}: {buffer.shape}")
    
    print(f"\nPyTorch BatchNorm parameters():")
    params = list(torch_bn.parameters())
    print(f"  æ€»æ•°: {len(params)}")
    for i, param in enumerate(params):
        print(f"  å‚æ•°{i}: {param.shape}")


def investigate_jittor_scalar():
    """ç ”ç©¶Jittoræ ‡é‡å‚æ•°çš„åˆ›å»ºæ–¹å¼"""
    print("\nğŸ” ç ”ç©¶Jittoræ ‡é‡å‚æ•°åˆ›å»ºæ–¹å¼")
    print("=" * 60)
    
    # æµ‹è¯•ä¸åŒçš„æ ‡é‡åˆ›å»ºæ–¹å¼
    methods = [
        ("jt.array(1.0)", lambda: jt.array(1.0)),
        ("jt.float32(1.0)", lambda: jt.float32(1.0)),
        ("jt.Var(1.0)", lambda: jt.Var(1.0)),
        ("jt.array([1.0])", lambda: jt.array([1.0])),
        ("jt.ones([])", lambda: jt.ones([])),
        ("jt.tensor(1.0)", lambda: jt.array(1.0)),
    ]
    
    for method_name, method_func in methods:
        try:
            result = method_func()
            print(f"{method_name:20}: shape={result.shape}, ndim={result.ndim}, value={result}")
        except Exception as e:
            print(f"{method_name:20}: ERROR - {e}")
    
    # æµ‹è¯•PyTorchæ ‡é‡
    print(f"\nå¯¹æ¯”PyTorchæ ‡é‡:")
    torch_scalar = torch.tensor(1.0)
    print(f"torch.tensor(1.0):   shape={torch_scalar.shape}, ndim={torch_scalar.ndim}, value={torch_scalar}")
    
    torch_param = torch_nn.Parameter(torch.tensor(1.0))
    print(f"nn.Parameter(1.0):   shape={torch_param.shape}, ndim={torch_param.ndim}, value={torch_param}")


def investigate_jittor_module_system():
    """ç ”ç©¶Jittoræ¨¡å—ç³»ç»Ÿ"""
    print("\nğŸ” ç ”ç©¶Jittoræ¨¡å—ç³»ç»Ÿ")
    print("=" * 60)
    
    class TestModule(nn.Module):
        def __init__(self):
            super().__init__()
            # æµ‹è¯•ä¸åŒçš„å‚æ•°æ³¨å†Œæ–¹å¼
            self.param1 = jt.array([1.0])  # æ™®é€šå‚æ•°
            self.param2 = nn.Parameter(jt.array([2.0]))  # æ˜¾å¼å‚æ•°
            
            # æµ‹è¯•bufferæ³¨å†Œ
            try:
                self.register_buffer('buffer1', jt.array([3.0]))
                print("âœ“ register_buffer å¯ç”¨")
            except:
                print("âŒ register_buffer ä¸å¯ç”¨")
                # æ‰‹åŠ¨è®¾ç½®buffer
                self.buffer1 = jt.array([3.0])
                self.buffer1.requires_grad = False
            
            # æµ‹è¯•BatchNorm
            self.bn = nn.BatchNorm(10)
    
    module = TestModule()
    
    print(f"\nTestModule named_parameters():")
    for name, param in module.named_parameters():
        print(f"  {name}: {param.shape} - requires_grad={param.requires_grad}")
    
    print(f"\nTestModuleæ‰€æœ‰å±æ€§:")
    for name in dir(module):
        if not name.startswith('_'):
            attr = getattr(module, name)
            if hasattr(attr, 'shape'):
                print(f"  {name}: {attr.shape} - {type(attr)} - requires_grad={getattr(attr, 'requires_grad', 'N/A')}")


def test_parameter_exclusion():
    """æµ‹è¯•å‚æ•°æ’é™¤æœºåˆ¶"""
    print("\nğŸ” æµ‹è¯•å‚æ•°æ’é™¤æœºåˆ¶")
    print("=" * 60)
    
    class CustomModule(nn.Module):
        def __init__(self):
            super().__init__()
            # åˆ›å»ºä¸åŒç±»å‹çš„å±æ€§
            self.trainable_param = jt.array([1.0])
            
            # å°è¯•åˆ›å»ºéå‚æ•°å±æ€§
            non_param = jt.array([2.0])
            non_param.requires_grad = False
            object.__setattr__(self, 'non_param', non_param)
            
            # å°è¯•ä½¿ç”¨_å¼€å¤´çš„å±æ€§
            self._private_param = jt.array([3.0])
            
            # BatchNorm
            self.bn = nn.BatchNorm(5)
    
    module = CustomModule()
    
    print(f"CustomModule named_parameters():")
    for name, param in module.named_parameters():
        print(f"  {name}: {param.shape}")
    
    print(f"\nCustomModuleæ‰€æœ‰Varå±æ€§:")
    for name in dir(module):
        if not name.startswith('__'):
            attr = getattr(module, name)
            if hasattr(attr, 'shape') and hasattr(attr, 'requires_grad'):
                print(f"  {name}: {attr.shape} - requires_grad={attr.requires_grad}")


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ å¼€å§‹æ·±å…¥ç ”ç©¶Jittorå‚æ•°æœºåˆ¶")
    
    # ç ”ç©¶BatchNorm
    investigate_jittor_batchnorm()
    
    # ç ”ç©¶æ ‡é‡å‚æ•°
    investigate_jittor_scalar()
    
    # ç ”ç©¶æ¨¡å—ç³»ç»Ÿ
    investigate_jittor_module_system()
    
    # æµ‹è¯•å‚æ•°æ’é™¤
    test_parameter_exclusion()
    
    print("\nâœ… ç ”ç©¶å®Œæˆ")


if __name__ == '__main__':
    main()
