#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
æ£€æŸ¥FPNå’ŒHeadçš„æƒé‡åŠ è½½æƒ…å†µ
æ‰¾å‡ºæƒé‡åŠ è½½çš„å…·ä½“é—®é¢˜
"""

import os
import sys
import torch
import jittor as jt
import numpy as np

# æ·»åŠ è·¯å¾„
sys.path.append('/home/kyc/project/nanodet/nanodet-jittor')
from nanodet.model.arch.nanodet_plus import NanoDetPlus


def check_weight_loading_details():
    """æ£€æŸ¥æƒé‡åŠ è½½çš„è¯¦ç»†æƒ…å†µ"""
    print("ğŸ” æ£€æŸ¥FPNå’ŒHeadæƒé‡åŠ è½½è¯¦ç»†æƒ…å†µ")
    print("=" * 60)
    
    # åˆ›å»ºæ¨¡å‹
    backbone_cfg = {
        'name': 'ShuffleNetV2',
        'model_size': '1.0x',
        'out_stages': [2, 3, 4],
        'activation': 'LeakyReLU',
        'pretrain': True
    }
    
    fpn_cfg = {
        'name': 'GhostPAN',
        'in_channels': [116, 232, 464],
        'out_channels': 96,
        'kernel_size': 5,
        'num_extra_level': 1,
        'use_depthwise': True,
        'activation': 'LeakyReLU'
    }
    
    head_cfg = {
        'name': 'NanoDetPlusHead',
        'num_classes': 20,
        'input_channel': 96,
        'feat_channels': 96,
        'stacked_convs': 2,
        'kernel_size': 5,
        'strides': [8, 16, 32, 64],
        'activation': 'LeakyReLU',
        'reg_max': 7,
        'norm_cfg': {'type': 'BN'},
        'loss': {
            'loss_qfl': {
                'name': 'QualityFocalLoss',
                'use_sigmoid': True,
                'beta': 2.0,
                'loss_weight': 1.0
            },
            'loss_dfl': {
                'name': 'DistributionFocalLoss',
                'loss_weight': 0.25
            },
            'loss_bbox': {
                'name': 'GIoULoss',
                'loss_weight': 2.0
            }
        }
    }
    
    aux_head_cfg = {
        'name': 'SimpleConvHead',
        'num_classes': 20,
        'input_channel': 192,
        'feat_channels': 192,
        'stacked_convs': 4,
        'strides': [8, 16, 32, 64],
        'activation': 'LeakyReLU',
        'reg_max': 7
    }
    
    print("1ï¸âƒ£ åˆ›å»ºJittoræ¨¡å‹...")
    model = NanoDetPlus(backbone_cfg, fpn_cfg, aux_head_cfg, head_cfg)
    
    # åŠ è½½PyTorchæƒé‡
    print("\n2ï¸âƒ£ åŠ è½½PyTorchæƒé‡...")
    checkpoint_path = "/home/kyc/project/nanodet/nanodet-pytorch/workspace/nanodet-plus-m_320_voc_bs64/model_best/model_best.ckpt"
    ckpt = torch.load(checkpoint_path, map_location='cpu')
    state_dict = ckpt.get('state_dict', ckpt)
    
    print(f"PyTorch checkpointåŒ…å« {len(state_dict)} ä¸ªå‚æ•°")
    
    # åˆ†æPyTorchæƒé‡
    backbone_params = {}
    fpn_params = {}
    aux_fpn_params = {}
    head_params = {}
    aux_head_params = {}
    other_params = {}
    
    for name, param in state_dict.items():
        clean_name = name.replace('model.', '') if name.startswith('model.') else name
        
        if clean_name.startswith('backbone.'):
            backbone_params[clean_name] = param
        elif clean_name.startswith('fpn.'):
            fpn_params[clean_name] = param
        elif clean_name.startswith('aux_fpn.'):
            aux_fpn_params[clean_name] = param
        elif clean_name.startswith('head.'):
            head_params[clean_name] = param
        elif clean_name.startswith('aux_head.'):
            aux_head_params[clean_name] = param
        else:
            other_params[clean_name] = param
    
    print(f"\nPyTorchæƒé‡åˆ†å¸ƒ:")
    print(f"  backbone: {len(backbone_params)} ä¸ªå‚æ•°")
    print(f"  fpn: {len(fpn_params)} ä¸ªå‚æ•°")
    print(f"  aux_fpn: {len(aux_fpn_params)} ä¸ªå‚æ•°")
    print(f"  head: {len(head_params)} ä¸ªå‚æ•°")
    print(f"  aux_head: {len(aux_head_params)} ä¸ªå‚æ•°")
    print(f"  other: {len(other_params)} ä¸ªå‚æ•°")
    
    # è·å–Jittoræ¨¡å‹çš„å‚æ•°
    print("\n3ï¸âƒ£ åˆ†æJittoræ¨¡å‹å‚æ•°...")
    jittor_backbone_params = {}
    jittor_fpn_params = {}
    jittor_aux_fpn_params = {}
    jittor_head_params = {}
    jittor_aux_head_params = {}
    
    for name, param in model.named_parameters():
        if name.startswith('backbone.'):
            jittor_backbone_params[name] = param
        elif name.startswith('fpn.'):
            jittor_fpn_params[name] = param
        elif name.startswith('aux_fpn.'):
            jittor_aux_fpn_params[name] = param
        elif name.startswith('head.'):
            jittor_head_params[name] = param
        elif name.startswith('aux_head.'):
            jittor_aux_head_params[name] = param
    
    print(f"Jittoræ¨¡å‹å‚æ•°åˆ†å¸ƒ:")
    print(f"  backbone: {len(jittor_backbone_params)} ä¸ªå‚æ•°")
    print(f"  fpn: {len(jittor_fpn_params)} ä¸ªå‚æ•°")
    print(f"  aux_fpn: {len(jittor_aux_fpn_params)} ä¸ªå‚æ•°")
    print(f"  head: {len(jittor_head_params)} ä¸ªå‚æ•°")
    print(f"  aux_head: {len(jittor_aux_head_params)} ä¸ªå‚æ•°")
    
    # è¯¦ç»†æ£€æŸ¥æƒé‡åŠ è½½
    print("\n4ï¸âƒ£ è¯¦ç»†æ£€æŸ¥æƒé‡åŠ è½½...")
    
    def check_module_weights(pytorch_params, jittor_params, module_name):
        print(f"\nğŸ” æ£€æŸ¥{module_name}æƒé‡åŠ è½½:")
        
        loaded_count = 0
        failed_count = 0
        missing_count = 0
        
        # æ£€æŸ¥PyTorch -> Jittorçš„æ˜ å°„
        for pytorch_name, pytorch_param in pytorch_params.items():
            if pytorch_name in jittor_params:
                jittor_param = jittor_params[pytorch_name]
                
                if list(pytorch_param.shape) == list(jittor_param.shape):
                    loaded_count += 1
                    # æ£€æŸ¥æƒé‡æ•°å€¼
                    pytorch_np = pytorch_param.detach().numpy()
                    jittor_np = jittor_param.numpy()
                    diff = np.abs(pytorch_np - jittor_np).max()
                    
                    if diff > 1e-6:
                        print(f"    âš ï¸ {pytorch_name}: æƒé‡æ•°å€¼ä¸ä¸€è‡´ (å·®å¼‚{diff:.8f})")
                else:
                    print(f"    âŒ {pytorch_name}: å½¢çŠ¶ä¸åŒ¹é… PyTorch{pytorch_param.shape} vs Jittor{jittor_param.shape}")
                    failed_count += 1
            else:
                print(f"    âŒ {pytorch_name}: åœ¨Jittorä¸­ä¸å­˜åœ¨")
                missing_count += 1
        
        # æ£€æŸ¥Jittorä¸­å¤šä½™çš„å‚æ•°
        extra_count = 0
        for jittor_name in jittor_params.keys():
            if jittor_name not in pytorch_params:
                print(f"    âš ï¸ {jittor_name}: Jittorä¸­å¤šä½™çš„å‚æ•°")
                extra_count += 1
        
        print(f"  ğŸ“Š {module_name}æƒé‡åŠ è½½ç»Ÿè®¡:")
        print(f"    æˆåŠŸåŠ è½½: {loaded_count}")
        print(f"    åŠ è½½å¤±è´¥: {failed_count}")
        print(f"    ç¼ºå¤±å‚æ•°: {missing_count}")
        print(f"    å¤šä½™å‚æ•°: {extra_count}")
        
        return loaded_count, failed_count, missing_count, extra_count
    
    # æ£€æŸ¥å„ä¸ªæ¨¡å—
    backbone_stats = check_module_weights(backbone_params, jittor_backbone_params, "Backbone")
    fpn_stats = check_module_weights(fpn_params, jittor_fpn_params, "FPN")
    aux_fpn_stats = check_module_weights(aux_fpn_params, jittor_aux_fpn_params, "Aux_FPN")
    head_stats = check_module_weights(head_params, jittor_head_params, "Head")
    aux_head_stats = check_module_weights(aux_head_params, jittor_aux_head_params, "Aux_Head")
    
    # æ€»ç»“
    print(f"\nğŸ“Š æƒé‡åŠ è½½æ€»ç»“:")
    total_loaded = sum(stats[0] for stats in [backbone_stats, fpn_stats, aux_fpn_stats, head_stats, aux_head_stats])
    total_failed = sum(stats[1] for stats in [backbone_stats, fpn_stats, aux_fpn_stats, head_stats, aux_head_stats])
    total_missing = sum(stats[2] for stats in [backbone_stats, fpn_stats, aux_fpn_stats, head_stats, aux_head_stats])
    total_extra = sum(stats[3] for stats in [backbone_stats, fpn_stats, aux_fpn_stats, head_stats, aux_head_stats])
    
    print(f"  æ€»æˆåŠŸåŠ è½½: {total_loaded}")
    print(f"  æ€»åŠ è½½å¤±è´¥: {total_failed}")
    print(f"  æ€»ç¼ºå¤±å‚æ•°: {total_missing}")
    print(f"  æ€»å¤šä½™å‚æ•°: {total_extra}")
    
    if total_failed > 0 or total_missing > 0:
        print(f"  âŒ æƒé‡åŠ è½½å­˜åœ¨é—®é¢˜")
        return False
    else:
        print(f"  âœ… æƒé‡åŠ è½½å®Œå…¨æ­£ç¡®")
        return True


def test_segmented_model():
    """æµ‹è¯•åˆ†æ®µæ¨¡å‹"""
    print("\n5ï¸âƒ£ æµ‹è¯•åˆ†æ®µæ¨¡å‹...")
    
    # è®¾ç½®éšæœºç§å­
    np.random.seed(42)
    torch.manual_seed(42)
    jt.set_global_seed(42)
    
    # åˆ›å»ºæ¨¡å‹
    backbone_cfg = {
        'name': 'ShuffleNetV2',
        'model_size': '1.0x',
        'out_stages': [2, 3, 4],
        'activation': 'LeakyReLU',
        'pretrain': True
    }
    
    fpn_cfg = {
        'name': 'GhostPAN',
        'in_channels': [116, 232, 464],
        'out_channels': 96,
        'kernel_size': 5,
        'num_extra_level': 1,
        'use_depthwise': True,
        'activation': 'LeakyReLU'
    }
    
    head_cfg = {
        'name': 'NanoDetPlusHead',
        'num_classes': 20,
        'input_channel': 96,
        'feat_channels': 96,
        'stacked_convs': 2,
        'kernel_size': 5,
        'strides': [8, 16, 32, 64],
        'activation': 'LeakyReLU',
        'reg_max': 7,
        'norm_cfg': {'type': 'BN'},
        'loss': {
            'loss_qfl': {
                'name': 'QualityFocalLoss',
                'use_sigmoid': True,
                'beta': 2.0,
                'loss_weight': 1.0
            },
            'loss_dfl': {
                'name': 'DistributionFocalLoss',
                'loss_weight': 0.25
            },
            'loss_bbox': {
                'name': 'GIoULoss',
                'loss_weight': 2.0
            }
        }
    }
    
    aux_head_cfg = {
        'name': 'SimpleConvHead',
        'num_classes': 20,
        'input_channel': 192,
        'feat_channels': 192,
        'stacked_convs': 4,
        'strides': [8, 16, 32, 64],
        'activation': 'LeakyReLU',
        'reg_max': 7
    }
    
    model = NanoDetPlus(backbone_cfg, fpn_cfg, aux_head_cfg, head_cfg)
    
    # åŠ è½½æƒé‡ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰
    checkpoint_path = "/home/kyc/project/nanodet/nanodet-pytorch/workspace/nanodet-plus-m_320_voc_bs64/model_best/model_best.ckpt"
    ckpt = torch.load(checkpoint_path, map_location='cpu')
    state_dict = ckpt.get('state_dict', ckpt)
    
    jittor_state_dict = {}
    for name, param in model.named_parameters():
        jittor_state_dict[name] = param
    
    # æƒé‡åŠ è½½
    for pytorch_name, pytorch_param in state_dict.items():
        jittor_name = pytorch_name
        if jittor_name.startswith("model."):
            jittor_name = jittor_name[6:]
        
        if "num_batches_tracked" in jittor_name or jittor_name.startswith("avg_"):
            continue
        
        if "distribution_project.project" in jittor_name:
            continue
        
        if jittor_name in jittor_state_dict:
            jittor_param = jittor_state_dict[jittor_name]
            
            if list(pytorch_param.shape) == list(jittor_param.shape):
                jittor_param.assign(jt.array(pytorch_param.detach().numpy()))
            elif "scale" in jittor_name and len(pytorch_param.shape) == 0 and list(jittor_param.shape) == [1]:
                jittor_param.assign(jt.array([pytorch_param.detach().numpy()]))
    
    model.eval()
    
    # ä½¿ç”¨å›ºå®šè¾“å…¥
    if os.path.exists("fixed_input_data.npy"):
        input_data = np.load("fixed_input_data.npy")
    else:
        input_data = np.random.randn(1, 3, 320, 320).astype(np.float32)
    
    jittor_input = jt.array(input_data)
    
    print(f"\nğŸ” åˆ†æ®µæµ‹è¯•:")
    
    with jt.no_grad():
        # 1. Backboneè¾“å‡º
        backbone_features = model.backbone(jittor_input)
        print(f"  Backboneè¾“å‡º:")
        for i, feat in enumerate(backbone_features):
            print(f"    ç‰¹å¾{i}: {feat.shape}, èŒƒå›´[{feat.min():.6f}, {feat.max():.6f}]")
        
        # 2. FPNè¾“å‡º
        fpn_features = model.fpn(backbone_features)
        print(f"  FPNè¾“å‡º:")
        for i, feat in enumerate(fpn_features):
            print(f"    FPNç‰¹å¾{i}: {feat.shape}, èŒƒå›´[{feat.min():.6f}, {feat.max():.6f}]")
        
        # 3. Headè¾“å‡º
        head_output = model.head(fpn_features)
        print(f"  Headè¾“å‡º:")
        print(f"    Headè¾“å‡º: {head_output.shape}, èŒƒå›´[{head_output.min():.6f}, {head_output.max():.6f}]")
        
        # åˆ†æHeadè¾“å‡º
        cls_preds = head_output[:, :, :20]
        reg_preds = head_output[:, :, 20:]
        cls_scores = jt.sigmoid(cls_preds)
        
        print(f"    åˆ†ç±»é¢„æµ‹: èŒƒå›´[{cls_preds.min():.6f}, {cls_preds.max():.6f}]")
        print(f"    å›å½’é¢„æµ‹: èŒƒå›´[{reg_preds.min():.6f}, {reg_preds.max():.6f}]")
        print(f"    æœ€é«˜ç½®ä¿¡åº¦: {cls_scores.max():.6f}")


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ å¼€å§‹æ£€æŸ¥FPNå’ŒHeadæƒé‡åŠ è½½")
    
    # æ£€æŸ¥æƒé‡åŠ è½½è¯¦æƒ…
    weight_ok = check_weight_loading_details()
    
    # æµ‹è¯•åˆ†æ®µæ¨¡å‹
    test_segmented_model()
    
    print(f"\nâœ… æ£€æŸ¥å®Œæˆ")
    
    if weight_ok:
        print(f"æƒé‡åŠ è½½æ­£ç¡®ï¼Œé—®é¢˜å¯èƒ½åœ¨æ¨¡å‹å®ç°ç»†èŠ‚")
    else:
        print(f"å‘ç°æƒé‡åŠ è½½é—®é¢˜ï¼Œéœ€è¦ä¿®å¤")


if __name__ == '__main__':
    main()
