#!/usr/bin/env python3
"""
ğŸ” NanoDet æŸå¤±å‡½æ•°è°ƒè¯•è„šæœ¬
ä¸“é—¨è°ƒè¯• NanoDet è®­ç»ƒè¿‡ç¨‹ä¸­çš„æŸå¤±è®¡ç®—é—®é¢˜
"""

import sys
import os
sys.path.insert(0, '.')

import time
import jittor as jt
from nanodet.util import cfg, load_config
from nanodet.model.arch import build_model
from nanodet.data.dataset import build_dataset
from nanodet.data.collate import naive_collate
from nanodet.trainer.task import TrainingTask

def debug_single_batch():
    print("ğŸ” è°ƒè¯•å•ä¸ªæ‰¹æ¬¡çš„è®­ç»ƒè¿‡ç¨‹")
    print("=" * 50)
    
    # è®¾ç½®ç¯å¢ƒ
    os.environ['DISABLE_MULTIPROCESSING'] = '1'
    jt.flags.use_cuda = 1 if jt.has_cuda else 0
    print(f"âœ… CUDA: {'å¯ç”¨' if jt.flags.use_cuda else 'ç¦ç”¨'}")
    
    # åŠ è½½é…ç½®
    print("\nğŸ“‹ åŠ è½½é…ç½®...")
    config_path = 'config/nanodet-plus-m_320_voc_bs64_50epochs.yml'
    load_config(cfg, config_path)
    print(f"âœ… é…ç½®åŠ è½½æˆåŠŸ")
    
    # åˆ›å»ºæ•°æ®é›†
    print("\nğŸ“Š åˆ›å»ºæ•°æ®é›†...")
    train_dataset = build_dataset(cfg.data.train, 'train')
    print(f"âœ… è®­ç»ƒæ•°æ®é›†: {len(train_dataset)} æ ·æœ¬")
    
    # åˆ›å»ºæ¨¡å‹
    print("\nğŸ—ï¸ åˆ›å»ºæ¨¡å‹...")
    model = build_model(cfg.model)
    print(f"âœ… æ¨¡å‹åˆ›å»ºæˆåŠŸ")
    
    # åˆ›å»ºè®­ç»ƒä»»åŠ¡
    print("\nğŸ¯ åˆ›å»ºè®­ç»ƒä»»åŠ¡...")
    task = TrainingTask(cfg, model)
    optimizer, scheduler = task.configure_optimizers()
    print(f"âœ… è®­ç»ƒä»»åŠ¡åˆ›å»ºæˆåŠŸ")
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    print("\nğŸ“¦ åˆ›å»ºæ•°æ®åŠ è½½å™¨...")
    from jittor.dataset import DataLoader
    
    train_dataloader = DataLoader(
        train_dataset,
        batch_size=1,  # ä½¿ç”¨æœ€å°æ‰¹æ¬¡è¿›è¡Œè°ƒè¯•
        shuffle=False,  # ä¸æ‰“ä¹±ï¼Œä¾¿äºè°ƒè¯•
        num_workers=0,
        collate_batch=naive_collate,
        drop_last=False
    )
    
    print(f"âœ… æ•°æ®åŠ è½½å™¨åˆ›å»ºæˆåŠŸ")
    
    # è°ƒè¯•å•ä¸ªæ‰¹æ¬¡
    print("\nğŸ” å¼€å§‹è°ƒè¯•å•ä¸ªæ‰¹æ¬¡...")
    print("-" * 50)
    
    try:
        from types import SimpleNamespace
        
        # è®¾ç½®è®­ç»ƒæ¨¡å¼
        model.train()
        task.on_train_epoch_start(0)
        
        # åˆ›å»ºæ¨¡æ‹Ÿçš„ trainer å¯¹è±¡
        trainer_mock = SimpleNamespace(
            current_epoch=0,
            global_step=0,
            num_training_batches=1,
            num_val_batches=1,
            optimizer=optimizer
        )
        
        # è·å–ç¬¬ä¸€ä¸ªæ‰¹æ¬¡
        batch = next(iter(train_dataloader))
        print(f"âœ… è·å–æ‰¹æ¬¡æ•°æ®æˆåŠŸ")
        print(f"   æ‰¹æ¬¡ç±»å‹: {type(batch)}")
        
        # æ£€æŸ¥æ‰¹æ¬¡å†…å®¹
        if isinstance(batch, dict):
            print(f"   æ‰¹æ¬¡é”®: {list(batch.keys())}")
            for key, value in batch.items():
                if hasattr(value, 'shape'):
                    print(f"   {key}: {value.shape}")
                else:
                    print(f"   {key}: {type(value)}")
        
        print("\nğŸ” å¼€å§‹å‰å‘ä¼ æ’­...")
        
        # å‰å‘ä¼ æ’­
        try:
            total_loss = task.training_step(batch, 0, trainer_mock)
            print(f"âœ… å‰å‘ä¼ æ’­æˆåŠŸ")
            print(f"   è¿”å›å€¼ç±»å‹: {type(total_loss)}")

            # æ£€æŸ¥è¿”å›çš„æŸå¤±
            if hasattr(total_loss, 'shape'):
                print(f"   æŸå¤±å½¢çŠ¶: {total_loss.shape}")

            # éªŒè¯æ˜¯å¦ä¸º Jittor Var ç±»å‹
            if isinstance(total_loss, jt.Var):
                print(f"   âœ… è¿”å›å€¼æ˜¯æ­£ç¡®çš„ Jittor Var ç±»å‹")
            else:
                print(f"   âŒ è¿”å›å€¼ä¸æ˜¯ Jittor Var ç±»å‹: {type(total_loss)}")
                return
            print(f"\nğŸ¯ æ€»æŸå¤±åˆ†æ:")
            print(f"   ç±»å‹: {type(total_loss)}")
            print(f"   å½¢çŠ¶: {total_loss.shape}")
            
            # éªŒè¯æ€»æŸå¤±
            if isinstance(total_loss, jt.Var):
                print(f"   âœ… æ€»æŸå¤±æ˜¯æ­£ç¡®çš„ Jittor Var ç±»å‹")
                
                # å°è¯•åå‘ä¼ æ’­
                print(f"\nğŸ” å¼€å§‹åå‘ä¼ æ’­æµ‹è¯•...")
                
                try:
                    # æ¸…é›¶æ¢¯åº¦
                    optimizer.zero_grad()
                    print(f"   âœ… æ¢¯åº¦æ¸…é›¶æˆåŠŸ")
                    
                    # åå‘ä¼ æ’­
                    total_loss.backward()
                    print(f"   âœ… åå‘ä¼ æ’­æˆåŠŸ")
                    
                    # æ£€æŸ¥æ¢¯åº¦
                    grad_count = 0
                    for name, param in model.named_parameters():
                        if hasattr(param, 'grad') and param.grad is not None:
                            grad_count += 1
                    
                    print(f"   âœ… æ¢¯åº¦è®¡ç®—æˆåŠŸï¼Œ{grad_count} ä¸ªå‚æ•°æœ‰æ¢¯åº¦")
                    
                    # ğŸ”§ è¿™é‡Œæ˜¯å…³é”®ï¼šå°è¯•ä¼˜åŒ–å™¨æ­¥éª¤
                    print(f"\nğŸ”§ å°è¯•ä¼˜åŒ–å™¨æ­¥éª¤...")
                    
                    try:
                        # ä½¿ç”¨æ‰‹åŠ¨æ¢¯åº¦æ›´æ–°ï¼Œé¿å¼€ jt.sync é—®é¢˜
                        lr = cfg.optimizer.lr
                        updated_params = 0
                        
                        for param in model.parameters():
                            if hasattr(param, 'grad') and param.grad is not None:
                                param.data = param.data - lr * param.grad.data
                                updated_params += 1
                        
                        print(f"   âœ… æ‰‹åŠ¨å‚æ•°æ›´æ–°æˆåŠŸï¼Œæ›´æ–°äº† {updated_params} ä¸ªå‚æ•°")
                        
                    except Exception as opt_error:
                        print(f"   âŒ ä¼˜åŒ–å™¨æ­¥éª¤å¤±è´¥: {opt_error}")
                        
                        # å°è¯•ä½¿ç”¨åŸå§‹ä¼˜åŒ–å™¨
                        try:
                            print(f"   ğŸ”§ å°è¯•åŸå§‹ä¼˜åŒ–å™¨...")
                            optimizer.step(total_loss)
                            print(f"   âœ… åŸå§‹ä¼˜åŒ–å™¨æˆåŠŸ")
                        except Exception as orig_opt_error:
                            print(f"   âŒ åŸå§‹ä¼˜åŒ–å™¨ä¹Ÿå¤±è´¥: {orig_opt_error}")
                            print(f"   è¿™ç¡®è®¤äº† jt.sync çš„é—®é¢˜")
                    
                except Exception as backward_error:
                    print(f"   âŒ åå‘ä¼ æ’­å¤±è´¥: {backward_error}")
                    import traceback
                    traceback.print_exc()
                
            else:
                print(f"   âŒ æ€»æŸå¤±ä¸æ˜¯ Jittor Var ç±»å‹: {type(total_loss)}")
            
        except Exception as forward_error:
            print(f"âŒ å‰å‘ä¼ æ’­å¤±è´¥: {forward_error}")
            import traceback
            traceback.print_exc()
            
    except Exception as e:
        print(f"âŒ è°ƒè¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

def main():
    print("ğŸ” NanoDet æŸå¤±å‡½æ•°ä¸“é¡¹è°ƒè¯•")
    print("åŸºäºåŸºç¡€åŠŸèƒ½æµ‹è¯•é€šè¿‡ï¼Œä¸“é—¨è°ƒè¯• NanoDet å¤æ‚é€»è¾‘")
    print("=" * 60)
    
    debug_single_batch()

if __name__ == "__main__":
    main()
