import copy
import json
import os
import warnings
from typing import Any, Dict, List
import logging

# JITTOR HIGH-FIDELITY MOD: å¯¼å…¥ Jittor æ ¸å¿ƒåº“
import jittor as jt

# å‡è®¾è¿™äº›å¤–éƒ¨å‡½æ•°éƒ½å·²ç»è¢«æ­£ç¡®åœ°è¿ç§»åˆ°äº† Jittor
from nanodet.data.batch_process import stack_batch_img
from nanodet.optim import build_optimizer
from nanodet.util import convert_avg_params, gather_results_jittor as gather_results, mkdir

from ..model.arch import build_model
from ..model.weight_averager import build_weight_averager

# JITTOR HIGH-FIDELITY MOD: å®šä¹‰ä¸€ä¸ª rank_zero_only è£…é¥°å™¨ï¼Œæ¨¡æ‹Ÿ Lightning çš„åŠŸèƒ½ï¼Œä»£ç æ›´æ¸…æ™°
def rank_zero_only(fn):
    """ä¸€ä¸ªè£…é¥°å™¨ï¼Œç¡®ä¿å‡½æ•°åªåœ¨ rank 0 çš„è¿›ç¨‹ä¸Šæ‰§è¡Œã€‚"""
    def wrapper(*args, **kwargs):
        if not (jt.world_size > 1 and jt.rank != 0):
            return fn(*args, **kwargs)
    return wrapper

# JITTOR HIGH-FIDELITY MOD: ç»§æ‰¿è‡ª jt.Moduleï¼Œè¿™æ˜¯ Jittor ä¸­æ‰€æœ‰æ¨¡å‹çš„åŸºç±»
class TrainingTask(jt.Module):
    """
    ä¸€ä¸ªé€šç”¨çš„ Jittor è®­ç»ƒä»»åŠ¡æ¨¡å—ã€‚
    æ­¤ç‰ˆæœ¬æ—¨åœ¨é«˜åº¦å¤åŸ PyTorch Lightning çš„è®¾è®¡ï¼Œå°†è®­ç»ƒã€éªŒè¯ç­‰æ­¥éª¤å®šä¹‰ä¸ºç‹¬ç«‹æ–¹æ³•ï¼Œ
    ç”±å¤–éƒ¨çš„è®­ç»ƒå¾ªç¯ï¼ˆå³æ¨¡æ‹Ÿçš„ Trainerï¼‰æ¥è°ƒç”¨ã€‚

    Args:
        cfg: è®­ç»ƒè®¾å®šã€‚
        evaluator: ç”¨äºè¯„ä¼°æ¨¡å‹æ€§èƒ½çš„è¯„ä¼°å™¨ã€‚
        logger (optional): ä¸€ä¸ªé…ç½®å¥½çš„æ—¥å¿—è®°å½•å™¨ï¼Œç”¨äºè¾“å‡ºä¿¡æ¯å’ŒæŒ‡æ ‡ã€‚
    """

    def __init__(self, cfg, evaluator=None, logger=None):
        super(TrainingTask, self).__init__()
        self.cfg = cfg
        self.model = build_model(cfg.model)
        self.evaluator = evaluator
        self.save_flag = -10
        self.log_style = "NanoDet"
        
        # JITTOR HIGH-FIDELITY MOD: ä½¿ç”¨æ ‡å‡†æ—¥å¿—æ¨¡å—ï¼Œæˆ–ä»å¤–éƒ¨ä¼ å…¥ä¸€ä¸ªæ›´å¤æ‚çš„ logger (å¦‚ Tensorboard)
        if logger:
            self.logger = logger
        else:
            self.logger = logging.getLogger(__name__)
            self.logger.setLevel(logging.INFO)
        
        self.weight_averager = None
        if "weight_averager" in cfg.model:
            self.weight_averager = build_weight_averager(cfg.model.weight_averager)
            # JITTOR HIGH-FIDELITY MOD: ä½¿ç”¨ copy.deepcopy æ¥åˆ›å»º avg_modelï¼Œç¡®ä¿å®Œå…¨ç‹¬ç«‹ï¼Œè¿™æ¯” .clone() æ›´å¯é 
            self.avg_model = copy.deepcopy(self.model)

    def _preprocess_batch_input(self, batch):
        """é¢„å¤„ç†æ‰¹æ¬¡è¾“å…¥ã€‚åœ¨ Jittor ä¸­ï¼Œæ•°æ®å·²åœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Šï¼Œæ— éœ€æ‰‹åŠ¨ .to(device)ã€‚"""
        batch_imgs = batch["img"]
        if isinstance(batch_imgs, list):
            batch_img_tensor = stack_batch_img(batch_imgs, divisible=32)
            batch["img"] = batch_img_tensor
        return batch

    # JITTOR HIGH-FIDELITY MOD: Jittor çš„ forward å‡½æ•°æ ‡å‡†åç§°æ˜¯ execute
    def execute(self, x):
        """æ¨¡å‹çš„å‰å‘ä¼ æ’­ã€‚"""
        return self.model(x)

    # JITTOR HIGH-FIDELITY MOD: ä½¿ç”¨ @jt.no_grad() è£…é¥°å™¨æ¥å…³é—­æ¢¯åº¦è®¡ç®—
    @jt.no_grad()
    def predict(self, batch):
        """é¢„æµ‹å‡½æ•°ï¼Œåœ¨æ— æ¢¯åº¦çš„ä¸Šä¸‹æ–‡ä¸­æ‰§è¡Œã€‚"""
        batch = self._preprocess_batch_input(batch)
        preds = self.execute(batch["img"])
        results = self.model.head.post_process(preds, batch)
        return results

    def training_step(self, batch, batch_idx, trainer):
        """
        å•ä¸ªè®­ç»ƒæ­¥éª¤ï¼ŒèŒè´£ä¸ Lightning å®Œå…¨å¯¹é½ï¼šä»…è®¡ç®—å¹¶è¿”å›æŸå¤±ã€‚
        æ—¥å¿—è®°å½•ä¹Ÿåœ¨æ­¤å¤„å®Œæˆã€‚
        Args:
            trainer: ä¸€ä¸ªæ¨¡æ‹Ÿçš„ Trainer å¯¹è±¡ï¼Œç”¨äºè·å–å½“å‰çŠ¶æ€å¦‚ epoch, global_step, optimizer ç­‰ã€‚
        """
        batch = self._preprocess_batch_input(batch)
        preds, loss, loss_states = self.model.forward_train(batch)

        # JITTOR HIGH-FIDELITY MOD: æ—¥å¿—è®°å½•é€»è¾‘ä¿æŒä¸å˜ï¼Œä½†ä¾èµ–å¤–éƒ¨ trainer æä¾›çŠ¶æ€
        if trainer.global_step % self.cfg.log.interval == 0:
            # memory = jt.flags.used_cuda_mem / 1e9 if jt.flags.use_cuda else 0
            memory = 0
            lr = trainer.optimizer.lr
            log_msg = "Train|Epoch{}/{}|Iter{}({}/{})| mem:{:.3g}G| lr:{:.2e}| ".format(
                trainer.current_epoch + 1,
                self.cfg.schedule.total_epochs,
                trainer.global_step,
                batch_idx + 1,
                trainer.num_training_batches,
                memory,
                lr,
            )
            self.scalar_summary("Train_loss/lr", "Train", lr, trainer.global_step)
            for loss_name in loss_states:
                loss_value = loss_states[loss_name].mean().item()
                log_msg += "{}:{:.4f}| ".format(loss_name, loss_value)
                self.scalar_summary(
                    "Train_loss/" + loss_name, "Train", loss_value, trainer.global_step
                )
            self.info(log_msg)

            # ğŸ”§ å¢å¼ºæ—¥å¿—ï¼šæ·»åŠ è¯¦ç»†çš„è®­ç»ƒè¿›åº¦ä¿¡æ¯
            if trainer.global_step % (self.cfg.log.interval * 5) == 0:  # æ¯5ä¸ªæ—¥å¿—é—´éš”æ˜¾ç¤ºä¸€æ¬¡è¯¦ç»†ä¿¡æ¯
                total_steps = trainer.num_training_batches
                progress = (batch_idx + 1) / total_steps * 100
                eta_steps = total_steps - batch_idx - 1

                print(f"ğŸ“Š è¯¦ç»†è¿›åº¦ - Epoch {trainer.current_epoch + 1}/{self.cfg.schedule.total_epochs}")
                print(f"  æ­¥éª¤: {batch_idx + 1}/{total_steps} ({progress:.1f}%)")
                print(f"  å‰©ä½™æ­¥éª¤: {eta_steps}")
                print(f"  æŸå¤±è¯¦æƒ…:")
                for loss_name, loss_value in loss_states.items():
                    loss_val = loss_value.mean().item()
                    print(f"    {loss_name}: {loss_val:.6f}")
                print(f"  å­¦ä¹ ç‡: {lr:.2e}")
                print("â”€" * 50)

        return loss

    def validation_step(self, batch, batch_idx, trainer):
        """å•ä¸ªéªŒè¯æ­¥éª¤ã€‚åœ¨æ— æ¢¯åº¦ä¸‹æ‰§è¡Œï¼Œå¹¶è¿”å›æ£€æµ‹ç»“æœã€‚"""
        batch = self._preprocess_batch_input(batch)
        
        model_to_eval = self.avg_model if self.weight_averager else self.model
        
        with jt.no_grad():
            preds, loss, loss_states = model_to_eval.forward_train(batch)
            dets = model_to_eval.head.post_process(preds, batch)

        # æ—¥å¿—è®°å½•
        if batch_idx % self.cfg.log.interval == 0:
            memory = jt.flags.used_cuda_mem / 1e9 if jt.flags.use_cuda else 0
            lr = trainer.optimizer.param_groups[0]["lr"]
            log_msg = "Val|Epoch{}/{}|Iter{}({}/{})| mem:{:.3g}G| lr:{:.2e}| ".format(
                trainer.current_epoch + 1,
                self.cfg.schedule.total_epochs,
                trainer.global_step,
                batch_idx + 1,
                trainer.num_val_batches,
                memory,
                lr,
            )
            for loss_name in loss_states:
                log_msg += "{}:{:.4f}| ".format(loss_name, loss_states[loss_name].mean().item())
            self.info(log_msg)

        return dets

    def validation_epoch_end(self, validation_step_outputs, current_epoch):
        """éªŒè¯ epoch ç»“æŸåçš„æ“ä½œï¼Œæ±‡æ€»ç»“æœã€è¯„ä¼°å¹¶ä¿å­˜æœ€ä½³æ¨¡å‹ã€‚"""
        results = {}
        for res in validation_step_outputs:
            results.update(res)
        
        # JITTOR HIGH-FIDELITY MOD: ä½¿ç”¨ jt.world_size > 1 åˆ¤æ–­æ˜¯å¦ä¸ºåˆ†å¸ƒå¼ç¯å¢ƒ
        all_results = gather_results(results) if jt.world_size > 1 else results
        
        if jt.rank == 0 and all_results:
            eval_results = self.evaluator.evaluate(all_results, self.cfg.save_dir)
            self.log_metrics(eval_results, current_epoch + 1)
            
            metric = eval_results.get(self.cfg.evaluator.save_key)
            if metric is None:
                warnings.warn(f"Warning! Save_key '{self.cfg.evaluator.save_key}' is not in eval results! Only save model last!")
                return

            if metric > self.save_flag:
                self.save_flag = metric
                best_save_path = os.path.join(self.cfg.save_dir, "model_best")
                mkdir(best_save_path) # mkdir åªåœ¨ rank 0 æ‰§è¡Œ
                
                # JITTOR HIGH-FIDELITY MOD: ä¿å­˜æ¨¡å‹çŠ¶æ€å’Œæ£€æŸ¥ç‚¹
                self.save_model_state(os.path.join(best_save_path, "nanodet_model_best.pth"))
                self.model.save(os.path.join(best_save_path, "model_best.ckpt")) # ä¿å­˜Jittorçš„æ£€æŸ¥ç‚¹
                
                txt_path = os.path.join(best_save_path, "eval_results.txt")
                with open(txt_path, "a") as f:
                    f.write(f"Epoch:{current_epoch + 1}\n")
                    for k, v in eval_results.items():
                        f.write(f"{k}: {v}\n")
        elif not all_results:
            self.info(f"Skip val on rank {jt.rank}")

    def test_step(self, batch):
        """å•ä¸ªæµ‹è¯•æ­¥éª¤ã€‚"""
        return self.predict(batch)

    def test_epoch_end(self, test_step_outputs):
        """æµ‹è¯• epoch ç»“æŸåçš„æ“ä½œï¼Œä¿å­˜ç»“æœä¸º json å¹¶è¯„ä¼°ã€‚"""
        results = {}
        for res in test_step_outputs:
            results.update(res)
        
        all_results = gather_results(results) if jt.world_size > 1 else results
        
        if jt.rank == 0 and all_results:
            res_json = self.evaluator.results2json(all_results)
            json_path = os.path.join(self.cfg.save_dir, "results.json")
            with open(json_path, "w") as f:
                json.dump(res_json, f)

            if self.cfg.test_mode == "val":
                eval_results = self.evaluator.evaluate(all_results, self.cfg.save_dir)
                txt_path = os.path.join(self.cfg.save_dir, "eval_results.txt")
                with open(txt_path, "a") as f:
                    for k, v in eval_results.items():
                        f.write(f"{k}: {v}\n")
        elif not all_results:
            self.info(f"Skip test on rank {jt.rank}")

    # JITTOR HIGH-FIDELITY MOD: å°†ä¼˜åŒ–å™¨å’Œè°ƒåº¦å™¨çš„é…ç½®åˆ†å¼€ï¼Œä¸ Lightning çš„è®¾è®¡æ›´ä¸€è‡´
    def configure_optimizers(self):
        """é…ç½®ä¼˜åŒ–å™¨å’Œå­¦ä¹ ç‡è°ƒåº¦å™¨ã€‚"""
        optimizer_cfg = copy.deepcopy(self.cfg.schedule.optimizer)
        optimizer = build_optimizer(self.model, optimizer_cfg)

        schedule_cfg = copy.deepcopy(self.cfg.schedule.lr_schedule)
        name = schedule_cfg.pop("name")
        build_scheduler = getattr(jt.lr_scheduler, name)
        scheduler = build_scheduler(optimizer=optimizer, **schedule_cfg)
        
        return optimizer, scheduler

    def optimizer_step(self, optimizer, loss, global_step):
        """
        æ‰§è¡Œå•ä¸ªä¼˜åŒ–æ­¥éª¤ï¼Œå®Œæ•´å¤åˆ»äº† Lightning çš„ optimizer_step hook çš„åŠŸèƒ½ï¼Œ
        åŒ…æ‹¬å­¦ä¹ ç‡é¢„çƒ­å’Œå‚æ•°æ›´æ–°ã€‚
        """
        # å­¦ä¹ ç‡é¢„çƒ­ (Warm up)
        if global_step <= self.cfg.schedule.warmup.steps:
            warmup_cfg = self.cfg.schedule.warmup
            if warmup_cfg.name == "constant":
                k = warmup_cfg.ratio
            elif warmup_cfg.name == "linear":
                k = 1 - (1 - global_step / warmup_cfg.steps) * (1 - warmup_cfg.ratio)
            elif warmup_cfg.name == "exp":
                k = warmup_cfg.ratio ** (1 - global_step / warmup_cfg.steps)
            else:
                raise Exception("Unsupported warm up type!")
            
            for pg in optimizer.param_groups:
                # é¦–æ¬¡æ‰§è¡Œæ—¶ï¼Œä¿å­˜åˆå§‹å­¦ä¹ ç‡
                if "initial_lr" not in pg:
                    pg["initial_lr"] = pg["lr"]
                pg["lr"] = pg["initial_lr"] * k
        
        # JITTOR HIGH-FIDELITY MOD: Jittor çš„ optimizer.step(loss) ä¼šè‡ªåŠ¨å®Œæˆæ¢¯åº¦è®¡ç®—ã€å‚æ•°æ›´æ–°å’Œæ¢¯åº¦æ¸…é›¶
        optimizer.step(loss)

    @rank_zero_only
    def scalar_summary(self, tag, phase, value, step):
        """
        å†™å…¥ Tensorboard æ ‡é‡æ—¥å¿—ã€‚
        ä¾èµ–äºä¼ å…¥çš„ logger å¯¹è±¡æœ‰ä¸€ä¸ªåä¸º 'experiment' çš„ SummaryWriter å®ä¾‹ã€‚
        """
        if hasattr(self.logger, 'experiment') and hasattr(self.logger.experiment, 'add_scalars'):
            self.logger.experiment.add_scalars(tag, {phase: value}, step)

    @rank_zero_only
    def log_metrics(self, metrics, step):
        """è®°å½•è¯„ä¼°æŒ‡æ ‡ã€‚"""
        if hasattr(self.logger, 'experiment') and hasattr(self.logger.experiment, 'add_scalars'):
            self.logger.experiment.add_scalars("Val_metrics", metrics, step)

    @rank_zero_only
    def info(self, string):
        self.logger.info(string)

    @rank_zero_only
    def save_model_state(self, path):
        """
        åªåœ¨ rank 0 è¿›ç¨‹ä¸Šå‚¨å­˜æ¨¡å‹çŠ¶æ€ã€‚
        JITTOR HIGH-FIDELITY MOD: ä¸¥æ ¼æŒ‰ç…§ Lightning çš„æ ¼å¼ä¿å­˜ï¼Œä»¥å¤‡åç»­ä½¿ç”¨ã€‚
        """
        self.info(f"Saving model to {path}")
        state_dict = (
            self.weight_averager.state_dict()
            if self.weight_averager
            else self.model.state_dict()
        )
        # JITTOR HIGH-FIDELITY MOD: ä¿å­˜ä¸ºå­—å…¸æ ¼å¼ï¼Œä¸ PyTorch ä¿æŒä¸€è‡´
        jt.save({"state_dict": state_dict}, path)

    # ------------æ¨¡æ‹Ÿ Hooks (ç”±å¤–éƒ¨è®­ç»ƒå¾ªç¯è°ƒç”¨)-----------------
    # JITTOR HIGH-FIDELITY MOD: è¿™äº›æ–¹æ³•æ¨¡æ‹Ÿäº† Lightning çš„å›è°ƒé’©å­ï¼Œåº”ç”±å¤–éƒ¨å¾ªç¯åœ¨æ­£ç¡®æ—¶æœºè°ƒç”¨
    
    def on_fit_start(self):
        """è®­ç»ƒå¼€å§‹å‰è°ƒç”¨ã€‚"""
        if "weight_averager" in self.cfg.model:
            self.info("Weight Averaging is enabled")
            if self.weight_averager and self.weight_averager.has_inited():
                return
            self.weight_averager = build_weight_averager(self.cfg.model.weight_averager)
            self.weight_averager.load_from(self.model)

    def on_train_epoch_start(self, current_epoch):
        """è®­ç»ƒ epoch å¼€å§‹å‰è°ƒç”¨ã€‚"""
        if hasattr(self.model, 'set_epoch'):
            self.model.set_epoch(current_epoch)

    def on_train_batch_end(self, global_step):
        """è®­ç»ƒ batch ç»“æŸåè°ƒç”¨ã€‚"""
        if self.weight_averager:
            self.weight_averager.update(self.model, global_step)

    def on_validation_epoch_start(self):
        """éªŒè¯ epoch å¼€å§‹å‰è°ƒç”¨ã€‚"""
        if self.weight_averager:
            self.weight_averager.apply_to(self.avg_model)

    def on_test_epoch_start(self):
        """æµ‹è¯• epoch å¼€å§‹å‰è°ƒç”¨ã€‚"""
        if self.weight_averager:
            # JITTOR HIGH-FIDELITY MOD: ä»æ¨¡å‹å½“å‰çŠ¶æ€åŠ è½½ï¼Œè€Œä¸æ˜¯æ•´ä¸ªæ£€æŸ¥ç‚¹
            self.on_load_checkpoint({"state_dict": self.model.state_dict()})
            self.weight_averager.apply_to(self.model)

    def on_load_checkpoint(self, checkpoint: Dict[str, Any]):
        """åŠ è½½æ£€æŸ¥ç‚¹æ—¶è°ƒç”¨ã€‚"""
        if "weight_averager" in self.cfg.model:
            # JITTOR HIGH-FIDELITY MOD: ä¼ å…¥çš„æ˜¯æ•´ä¸ªæ£€æŸ¥ç‚¹å­—å…¸
            avg_params = convert_avg_params(checkpoint)
            if not avg_params:
                self.info("Weight averaging is enabled, but no average state found in checkpoint.")
                return

            if len(avg_params) != len(self.model.state_dict()):
                self.info("Weight averaging is enabled but average state does not match the model")
            else:
                if self.weight_averager is None:
                    self.weight_averager = build_weight_averager(self.cfg.model.weight_averager)
                self.weight_averager.load_state_dict(avg_params)
                self.info("Loaded average state from checkpoint.")
