import jittor as jt
from jittor import nn
from .utils import weighted_loss


@weighted_loss
def quality_focal_loss(pred, target, beta=2.0):
    """Quality Focal Loss (QFL) çš„ Jittor ç‰ˆæœ¬ã€‚"""
    assert len(target) == 2, "QFL çš„ target å¿…é ˆæ˜¯åŒ…å«é¡žåˆ¥å’Œå“è³ªæ¨™ç±¤çš„å…ƒçµ„"
    label, score = target

    pred_sigmoid = pred.sigmoid()
    scale_factor = pred_sigmoid
    # [é·ç§»] .new_zeros -> jt.zeros_like
    zerolabel = jt.zeros_like(pred)
    # [é·ç§»] F.binary_cross_entropy_with_logits -> jt.nn.binary_cross_entropy_with_logits
    # Jittor ä¸­æ­¤å‡½æ•¸ä¸æŽ¥å— reduction åƒæ•¸ï¼Œé è¨­è¡Œç‚ºå°±æ˜¯ 'none'
    loss = jt.nn.binary_cross_entropy_with_logits(
        pred, zerolabel
    ) * scale_factor.pow(beta)

    # [é·ç§»] .size(1) -> .shape[1]
    bg_class_ind = pred.shape[1]
    # [é·ç§»] torch.nonzero(...) -> jt.nonzero(...), ä¸¦è™•ç†è¿”å›žå½¢ç‹€
    # ðŸ”§ ä¿®å¤ï¼šJittor ä¸­éœ€è¦å…ˆ flatten å†æ‰¾æ­£æ ·æœ¬
    label_flat = label.flatten()
    score_flat = score.flatten()
    pred_flat = pred.view(-1, pred.shape[-1])  # [N, num_classes]
    pred_sigmoid_flat = pred_sigmoid.view(-1, pred_sigmoid.shape[-1])
    loss_flat = loss.view(-1, loss.shape[-1])

    pos_mask = (label_flat >= 0) & (label_flat < bg_class_ind)
    if not pos_mask.any():
        return loss.sum(dim=1, keepdims=False)

    pos_indices = pos_mask.nonzero().squeeze(-1)  # [num_pos]
    pos_labels = label_flat[pos_indices].int64()  # [num_pos]
    pos_scores = score_flat[pos_indices]  # [num_pos]

    # ðŸ”§ ä¿®å¤ï¼šä½¿ç”¨é«˜çº§ç´¢å¼•èŽ·å–å¯¹åº”ä½ç½®çš„é¢„æµ‹å€¼
    pos_pred_sigmoid = pred_sigmoid_flat[pos_indices, pos_labels]  # [num_pos]
    scale_factor = pos_scores - pos_pred_sigmoid

    # è®¡ç®—æ­£æ ·æœ¬çš„æŸå¤±
    pos_loss = jt.nn.binary_cross_entropy_with_logits(
        pred_flat[pos_indices, pos_labels], pos_scores
    ) * scale_factor.abs().pow(beta)

    # å°†æ­£æ ·æœ¬æŸå¤±å†™å›žåŽŸä½ç½®
    loss_flat[pos_indices, pos_labels] = pos_loss

    # ðŸ”§ ä¿®å¤ï¼šé‡æ–°æ•´å½¢å¹¶æ±‚å’Œ
    loss = loss_flat.view(pred.shape[:-1] + (pred.shape[-1],))  # æ¢å¤åŽŸå§‹å½¢çŠ¶
    loss = loss.sum(dim=-1)  # åœ¨ç±»åˆ«ç»´åº¦æ±‚å’Œ
    return loss


@weighted_loss
def distribution_focal_loss(pred, label):
    """Distribution Focal Loss (DFL) çš„ Jittor ç‰ˆæœ¬ã€‚"""
    # [é·ç§»] .long() -> .int64(), .float() -> .float32()
    dis_left = label.int64()
    dis_right = dis_left + 1
    weight_left = dis_right.float32() - label
    weight_right = label - dis_left.float32()
    
    # [é·ç§»] F.cross_entropy -> jt.nn.cross_entropy_loss
    loss = (
        jt.nn.cross_entropy_loss(pred, dis_left, reduction="none") * weight_left
        + jt.nn.cross_entropy_loss(pred, dis_right, reduction="none") * weight_right
    )
    return loss


class QualityFocalLoss(nn.Module):
    """Quality Focal Loss (QFL) é¡žåˆ¥çš„ Jittor ç‰ˆæœ¬ã€‚"""
    def __init__(self, use_sigmoid=True, beta=2.0, reduction="mean", loss_weight=1.0):
        super(QualityFocalLoss, self).__init__()
        assert use_sigmoid is True, "ç›®å‰ QFL åªæ”¯æŒ sigmoid"
        self.use_sigmoid = use_sigmoid
        self.beta = beta
        self.reduction = reduction
        self.loss_weight = loss_weight

    # [é·ç§»] forward -> execute
    def execute(self, pred, target, weight=None, avg_factor=None, reduction_override=None):
        assert reduction_override in (None, "none", "mean", "sum")
        reduction = reduction_override if reduction_override else self.reduction
        if self.use_sigmoid:
            loss_cls = self.loss_weight * quality_focal_loss(
                pred,
                target,
                weight,
                beta=self.beta,
                reduction=reduction,
                avg_factor=avg_factor,
            )
        else:
            raise NotImplementedError
        return loss_cls


class DistributionFocalLoss(nn.Module):
    """Distribution Focal Loss (DFL) é¡žåˆ¥çš„ Jittor ç‰ˆæœ¬ã€‚"""
    def __init__(self, reduction="mean", loss_weight=1.0):
        super(DistributionFocalLoss, self).__init__()
        self.reduction = reduction
        self.loss_weight = loss_weight

    # [é·ç§»] forward -> execute
    def execute(self, pred, target, weight=None, avg_factor=None, reduction_override=None):
        assert reduction_override in (None, "none", "mean", "sum")
        reduction = reduction_override if reduction_override else self.reduction
        loss_cls = self.loss_weight * distribution_focal_loss(
            pred, target, weight, reduction=reduction, avg_factor=avg_factor
        )
        return loss_cls
