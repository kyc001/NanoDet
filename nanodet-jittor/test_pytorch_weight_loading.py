#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
PyTorchæƒé‡åŠ è½½éªŒè¯è„šæœ¬
æµ‹è¯•Jittorç‰ˆæœ¬æ˜¯å¦èƒ½æ­£ç¡®åŠ è½½PyTorchè®­ç»ƒçš„æƒé‡
éªŒè¯æ¨¡å‹æ¶æ„çš„ä¸€è‡´æ€§
"""

import os
import sys
import jittor as jt
import numpy as np
import traceback


def load_pytorch_checkpoint(ckpt_path):
    """åŠ è½½PyTorchæ£€æŸ¥ç‚¹æ–‡ä»¶"""
    print(f"åŠ è½½PyTorchæ£€æŸ¥ç‚¹: {ckpt_path}")
    
    try:
        # ä½¿ç”¨CPUåŠ è½½ï¼Œé¿å…CUDAé—®é¢˜
        import torch
        checkpoint = torch.load(ckpt_path, map_location='cpu')
        
        print(f"âœ“ æ£€æŸ¥ç‚¹åŠ è½½æˆåŠŸ")
        print(f"  Keys: {list(checkpoint.keys())}")
        
        # è·å–æ¨¡å‹æƒé‡
        if 'state_dict' in checkpoint:
            state_dict = checkpoint['state_dict']
        elif 'model' in checkpoint:
            state_dict = checkpoint['model']
        else:
            state_dict = checkpoint
        
        # è¿‡æ»¤æ¨¡å‹æƒé‡ï¼ˆç§»é™¤'model.'å‰ç¼€ï¼‰
        model_weights = {}
        for key, value in state_dict.items():
            if key.startswith('model.'):
                new_key = key[6:]  # ç§»é™¤'model.'å‰ç¼€
                model_weights[new_key] = value.numpy()  # è½¬æ¢ä¸ºnumpy
            elif not key.startswith('avg_model.') and not key.startswith('optimizer'):
                model_weights[key] = value.numpy()
        
        print(f"âœ“ æå–æ¨¡å‹æƒé‡: {len(model_weights)} ä¸ªå‚æ•°")
        
        # æ˜¾ç¤ºä¸€äº›å…³é”®æƒé‡çš„ä¿¡æ¯
        key_weights = [
            'backbone.stage2.0.branch1.0.weight',
            'fpn.reduce_layers.0.conv.weight', 
            'head.gfl_cls.0.weight',
            'head.gfl_cls.0.bias'
        ]
        
        for key in key_weights:
            if key in model_weights:
                weight = model_weights[key]
                print(f"  {key}: {weight.shape}, mean={weight.mean():.6f}, std={weight.std():.6f}")
        
        return model_weights
        
    except Exception as e:
        print(f"âœ— åŠ è½½PyTorchæ£€æŸ¥ç‚¹å¤±è´¥: {e}")
        traceback.print_exc()
        return None


def create_jittor_model():
    """åˆ›å»ºJittorç‰ˆæœ¬çš„æ¨¡å‹"""
    print("\nåˆ›å»ºJittoræ¨¡å‹...")
    
    try:
        from nanodet.model.backbone import build_backbone
        from nanodet.model.fpn import build_fpn
        from nanodet.model.head import build_head
        
        # åˆ›å»ºbackbone
        backbone_cfg = {
            'name': 'ShuffleNetV2',
            'model_size': '1.0x',
            'out_stages': [2, 3, 4],
            'activation': 'LeakyReLU',
            'pretrain': False
        }
        backbone = build_backbone(backbone_cfg)
        
        # åˆ›å»ºFPN
        fpn_cfg = {
            'name': 'GhostPAN',
            'in_channels': [116, 232, 464],
            'out_channels': 96,
            'kernel_size': 5,
            'num_extra_level': 1,
            'use_depthwise': True,
            'activation': 'LeakyReLU'
        }
        fpn = build_fpn(fpn_cfg)
        
        # åˆ›å»ºæ£€æµ‹å¤´
        loss_cfg = type('LossCfg', (), {
            'loss_qfl': type('QFL', (), {'beta': 2.0, 'loss_weight': 1.0})(),
            'loss_dfl': type('DFL', (), {'loss_weight': 0.25})(),
            'loss_bbox': type('BBOX', (), {'loss_weight': 2.0})()
        })()
        
        head_cfg = {
            'name': 'NanoDetPlusHead',
            'num_classes': 20,
            'loss': loss_cfg,
            'input_channel': 96,
            'feat_channels': 96,
            'stacked_convs': 2,
            'kernel_size': 5,
            'strides': [8, 16, 32, 64],
            'conv_type': 'DWConv',
            'norm_cfg': dict(type='BN'),
            'reg_max': 7,
            'activation': 'LeakyReLU'
        }
        head = build_head(head_cfg)
        
        print(f"âœ“ Jittoræ¨¡å‹åˆ›å»ºæˆåŠŸ")
        print(f"  Backboneå‚æ•°: {sum(p.numel() for p in backbone.parameters())/1e6:.2f}M")
        print(f"  FPNå‚æ•°: {sum(p.numel() for p in fpn.parameters())/1e6:.2f}M")
        print(f"  Headå‚æ•°: {sum(p.numel() for p in head.parameters())/1e6:.2f}M")
        
        return backbone, fpn, head
        
    except Exception as e:
        print(f"âœ— åˆ›å»ºJittoræ¨¡å‹å¤±è´¥: {e}")
        traceback.print_exc()
        return None, None, None


def compare_model_outputs(backbone, fpn, head, pytorch_weights):
    """æ¯”è¾ƒæ¨¡å‹è¾“å‡ºï¼ŒéªŒè¯æ¶æ„ä¸€è‡´æ€§"""
    print("\næ¯”è¾ƒæ¨¡å‹è¾“å‡º...")
    
    try:
        # åˆ›å»ºæµ‹è¯•è¾“å…¥
        x = jt.randn(1, 3, 320, 320)
        
        # Jittoræ¨¡å‹å‰å‘ä¼ æ’­
        with jt.no_grad():
            backbone_out = backbone(x)
            fpn_out = fpn(backbone_out)
            head_out = head(fpn_out)
        
        print(f"âœ“ Jittoræ¨¡å‹å‰å‘ä¼ æ’­æˆåŠŸ")
        print(f"  è¾“å…¥: {x.shape}")
        print(f"  Backboneè¾“å‡º: {[o.shape for o in backbone_out]}")
        print(f"  FPNè¾“å‡º: {[o.shape for o in fpn_out]}")
        print(f"  Headè¾“å‡º: {head_out.shape}")
        
        # æ£€æŸ¥è¾“å‡ºå½¢çŠ¶æ˜¯å¦ç¬¦åˆé¢„æœŸ
        expected_shape = (1, 2125, 52)  # (batch, points, classes+reg)
        if head_out.shape != expected_shape:
            print(f"âš  è¾“å‡ºå½¢çŠ¶ä¸åŒ¹é…: å¾—åˆ° {head_out.shape}, æœŸæœ› {expected_shape}")
            return False
        
        print(f"âœ“ è¾“å‡ºå½¢çŠ¶æ­£ç¡®: {head_out.shape}")
        
        # åˆ†æè¾“å‡ºç»Ÿè®¡ä¿¡æ¯
        print(f"  è¾“å‡ºç»Ÿè®¡:")
        print(f"    å‡å€¼: {jt.mean(head_out).item():.6f}")
        print(f"    æ ‡å‡†å·®: {jt.std(head_out).item():.6f}")
        print(f"    æœ€å°å€¼: {jt.min(head_out).item():.6f}")
        print(f"    æœ€å¤§å€¼: {jt.max(head_out).item():.6f}")
        
        return True
        
    except Exception as e:
        print(f"âœ— æ¨¡å‹è¾“å‡ºæ¯”è¾ƒå¤±è´¥: {e}")
        traceback.print_exc()
        return False


def analyze_weight_compatibility(backbone, fpn, head, pytorch_weights):
    """åˆ†ææƒé‡å…¼å®¹æ€§"""
    print("\nåˆ†ææƒé‡å…¼å®¹æ€§...")
    
    try:
        # è·å–Jittoræ¨¡å‹çš„å‚æ•°åç§°
        jittor_params = {}
        
        # Backboneå‚æ•°
        for name, param in backbone.named_parameters():
            jittor_params[f'backbone.{name}'] = param.shape
        
        # FPNå‚æ•°
        for name, param in fpn.named_parameters():
            jittor_params[f'fpn.{name}'] = param.shape
        
        # Headå‚æ•°
        for name, param in head.named_parameters():
            jittor_params[f'head.{name}'] = param.shape
        
        print(f"âœ“ Jittoræ¨¡å‹å‚æ•°: {len(jittor_params)} ä¸ª")
        
        # æ¯”è¾ƒå‚æ•°åç§°å’Œå½¢çŠ¶
        matched_params = 0
        mismatched_params = 0
        missing_params = 0
        
        for jittor_name, jittor_shape in jittor_params.items():
            if jittor_name in pytorch_weights:
                pytorch_shape = pytorch_weights[jittor_name].shape
                if jittor_shape == pytorch_shape:
                    matched_params += 1
                else:
                    print(f"  âš  å½¢çŠ¶ä¸åŒ¹é…: {jittor_name}")
                    print(f"    Jittor: {jittor_shape}, PyTorch: {pytorch_shape}")
                    mismatched_params += 1
            else:
                print(f"  âœ— ç¼ºå¤±å‚æ•°: {jittor_name}")
                missing_params += 1
        
        # æ£€æŸ¥PyTorchä¸­å¤šä½™çš„å‚æ•°
        extra_params = 0
        for pytorch_name in pytorch_weights.keys():
            if pytorch_name not in jittor_params:
                print(f"  + é¢å¤–å‚æ•°: {pytorch_name}")
                extra_params += 1
        
        print(f"\næƒé‡å…¼å®¹æ€§åˆ†æ:")
        print(f"  âœ“ åŒ¹é…å‚æ•°: {matched_params}")
        print(f"  âš  å½¢çŠ¶ä¸åŒ¹é…: {mismatched_params}")
        print(f"  âœ— ç¼ºå¤±å‚æ•°: {missing_params}")
        print(f"  + é¢å¤–å‚æ•°: {extra_params}")
        
        compatibility_rate = matched_params / len(jittor_params) * 100
        print(f"  å…¼å®¹æ€§: {compatibility_rate:.1f}%")
        
        return compatibility_rate > 90  # 90%ä»¥ä¸Šè®¤ä¸ºå…¼å®¹
        
    except Exception as e:
        print(f"âœ— æƒé‡å…¼å®¹æ€§åˆ†æå¤±è´¥: {e}")
        traceback.print_exc()
        return False


def main():
    """ä¸»å‡½æ•°"""
    print("=" * 60)
    print("PyTorchæƒé‡åŠ è½½éªŒè¯")
    print("=" * 60)
    
    # æ˜¾ç¤ºç³»ç»Ÿä¿¡æ¯
    print(f"Jittorç‰ˆæœ¬: {jt.__version__}")
    print(f"CUDAå¯ç”¨: {jt.has_cuda}")
    if jt.has_cuda:
        jt.flags.use_cuda = 1
        print(f"ä½¿ç”¨CUDA: {jt.flags.use_cuda}")
    
    # æ£€æŸ¥PyTorchæ£€æŸ¥ç‚¹æ–‡ä»¶
    ckpt_path = "../nanodet-pytorch/workspace/nanodet-plus-m_320_voc/model_last.ckpt"
    if not os.path.exists(ckpt_path):
        print(f"âœ— PyTorchæ£€æŸ¥ç‚¹æ–‡ä»¶ä¸å­˜åœ¨: {ckpt_path}")
        return False
    
    # åŠ è½½PyTorchæƒé‡
    pytorch_weights = load_pytorch_checkpoint(ckpt_path)
    if pytorch_weights is None:
        return False
    
    # åˆ›å»ºJittoræ¨¡å‹
    backbone, fpn, head = create_jittor_model()
    if backbone is None:
        return False
    
    # æ¯”è¾ƒæ¨¡å‹è¾“å‡º
    if not compare_model_outputs(backbone, fpn, head, pytorch_weights):
        return False
    
    # åˆ†ææƒé‡å…¼å®¹æ€§
    if not analyze_weight_compatibility(backbone, fpn, head, pytorch_weights):
        print("\nâš  æƒé‡å…¼å®¹æ€§è¾ƒä½ï¼Œå¯èƒ½éœ€è¦è°ƒæ•´æ¨¡å‹æ¶æ„")
        return False
    
    print("\n" + "=" * 60)
    print("éªŒè¯ç»“æœ")
    print("=" * 60)
    print("ğŸ‰ PyTorchæƒé‡åŠ è½½éªŒè¯æˆåŠŸï¼")
    print("âœ… Jittoræ¨¡å‹æ¶æ„ä¸PyTorchç‰ˆæœ¬å…¼å®¹")
    print("âœ… å¯ä»¥å¼€å§‹Jittorç‰ˆæœ¬çš„è®­ç»ƒ")
    print("âœ… å»ºè®®ä½¿ç”¨ç›¸åŒçš„åˆå§‹åŒ–å’Œè®­ç»ƒå‚æ•°")
    
    return True


if __name__ == '__main__':
    success = main()
    if not success:
        print("\nâŒ éªŒè¯å¤±è´¥ï¼è¯·æ£€æŸ¥æ¨¡å‹æ¶æ„ä¸€è‡´æ€§ã€‚")
        sys.exit(1)
    else:
        print("\nâœ… éªŒè¯æˆåŠŸï¼å¯ä»¥å¼€å§‹Jittorè®­ç»ƒã€‚")
        sys.exit(0)
