#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
NanoDet Jittor Test Script
ä½¿ç”¨PyTorchè¯„ä¼°å·¥å…·è¯„ä¼°Jittoræ¨¡å‹ï¼Œç¡®ä¿è¯„ä¼°æ–¹æ³•å®Œå…¨ä¸€è‡´
ä¸PyTorchç‰ˆæœ¬ä¿æŒå®Œå…¨ä¸€è‡´çš„æ¥å£
"""

import os
import sys
import argparse
import subprocess
import json
import re
from pathlib import Path

# æ·»åŠ è·¯å¾„
sys.path.append('/home/kyc/project/nanodet/nanodet-jittor')
from nanodet.util import get_logger


def parse_args():
    """è§£æå‘½ä»¤è¡Œå‚æ•° - ä¸PyTorchç‰ˆæœ¬ä¸€è‡´"""
    parser = argparse.ArgumentParser(description='NanoDet Jittor Test')
    parser.add_argument('--config', required=True, help='test config file path')
    parser.add_argument('--model', required=True, help='model checkpoint path (PyTorch format)')
    parser.add_argument('--task', default='val', choices=['val', 'test'], help='evaluation task')
    parser.add_argument('--save_result', help='save evaluation results to file')
    args = parser.parse_args()
    return args


def call_pytorch_evaluation(checkpoint_path, config_path, task='val'):
    """è°ƒç”¨PyTorchç‰ˆæœ¬çš„è¯„ä¼°å·¥å…·"""
    logger = get_logger('NanoDet')
    logger.info("Calling PyTorch evaluation tools...")
    
    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not os.path.exists(checkpoint_path):
        logger.error(f"Checkpoint file not found: {checkpoint_path}")
        return None
    
    if not os.path.exists(config_path):
        logger.error(f"Config file not found: {config_path}")
        return None
    
    # æ„å»ºè¯„ä¼°å‘½ä»¤
    pytorch_root = "/home/kyc/project/nanodet/nanodet-pytorch"
    
    cmd = [
        "/home/kyc/miniconda3/envs/nano/bin/python",
        f"{pytorch_root}/tools/test.py",
        "--config", config_path,
        "--model", checkpoint_path,
        "--task", task
    ]
    
    logger.info(f"Evaluation command: {' '.join(cmd)}")
    logger.info(f"Working directory: {pytorch_root}")
    
    try:
        # è¿è¡Œè¯„ä¼°
        result = subprocess.run(
            cmd,
            capture_output=True,
            text=True,
            timeout=600,  # 10åˆ†é’Ÿè¶…æ—¶
            cwd=pytorch_root
        )
        
        if result.returncode == 0:
            logger.info("PyTorch evaluation completed successfully")
            
            # è§£æè¯„ä¼°ç»“æœ
            map_results = parse_evaluation_output(result.stdout)
            
            # æ˜¾ç¤ºç»“æœ
            logger.info("Evaluation results:")
            for metric, value in map_results.items():
                logger.info(f"  {metric}: {value:.4f}")
            
            return map_results
            
        else:
            logger.error("PyTorch evaluation failed")
            logger.error(f"STDOUT: {result.stdout}")
            logger.error(f"STDERR: {result.stderr}")
            return None
            
    except subprocess.TimeoutExpired:
        logger.error("PyTorch evaluation timeout")
        return None
    except Exception as e:
        logger.error(f"PyTorch evaluation exception: {e}")
        return None


def parse_evaluation_output(output_text):
    """è§£æè¯„ä¼°è¾“å‡ºï¼Œæå–mAPç»“æœ"""
    logger = get_logger('NanoDet')
    
    map_results = {}
    
    # åˆ†å‰²è¾“å‡ºä¸ºè¡Œ
    lines = output_text.split('\n')
    
    for line in lines:
        line = line.strip()
        
        # æŸ¥æ‰¾mAPç›¸å…³çš„è¡Œ
        if 'Average Precision' in line:
            # ç¤ºä¾‹: Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.275
            if 'IoU=0.50:0.95' in line and 'area=   all' in line:
                match = re.search(r'= ([\d.]+)', line)
                if match:
                    map_results['mAP'] = float(match.group(1))
                    logger.info(f"Extracted mAP@0.5:0.95: {map_results['mAP']}")
            
            # ç¤ºä¾‹: Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.483
            elif 'IoU=0.50' in line and 'area=   all' in line and '0.50:0.95' not in line:
                match = re.search(r'= ([\d.]+)', line)
                if match:
                    map_results['mAP_50'] = float(match.group(1))
                    logger.info(f"Extracted mAP@0.5: {map_results['mAP_50']}")
            
            # ç¤ºä¾‹: Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.xxx
            elif 'IoU=0.75' in line and 'area=   all' in line:
                match = re.search(r'= ([\d.]+)', line)
                if match:
                    map_results['mAP_75'] = float(match.group(1))
                    logger.info(f"Extracted mAP@0.75: {map_results['mAP_75']}")
        
        elif 'Average Recall' in line:
            # ç¤ºä¾‹: Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.xxx
            if 'IoU=0.50:0.95' in line and 'maxDets=100' in line:
                match = re.search(r'= ([\d.]+)', line)
                if match:
                    map_results['AR_100'] = float(match.group(1))
                    logger.info(f"Extracted AR@100: {map_results['AR_100']}")
    
    # å¦‚æœæ²¡æœ‰æ‰¾åˆ°æ ‡å‡†æ ¼å¼ï¼Œå°è¯•å…¶ä»–æ ¼å¼
    if not map_results:
        logger.warning("Standard mAP format not found, trying alternative parsing...")
        
        for line in lines:
            # æŸ¥æ‰¾åŒ…å«æ•°å­—çš„è¡Œ
            if 'mAP' in line.lower() or 'map' in line.lower():
                # å°è¯•æå–æ•°å­—
                numbers = re.findall(r'[\d.]+', line)
                if numbers:
                    try:
                        value = float(numbers[-1])  # å–æœ€åä¸€ä¸ªæ•°å­—
                        if 0 <= value <= 1:  # mAPåº”è¯¥åœ¨0-1ä¹‹é—´
                            map_results['mAP'] = value
                            logger.info(f"Alternative method extracted mAP: {value}")
                            break
                    except:
                        continue
    
    if not map_results:
        logger.warning("Unable to parse evaluation results")
        # è¿”å›åŸå§‹è¾“å‡ºçš„æœ€åå‡ è¡Œç”¨äºè°ƒè¯•
        logger.info("Last 10 lines of evaluation output:")
        for line in lines[-10:]:
            if line.strip():
                logger.info(f"  {line}")
    
    return map_results


def save_results(results, save_path):
    """ä¿å­˜è¯„ä¼°ç»“æœ"""
    logger = get_logger('NanoDet')
    
    if results:
        os.makedirs(os.path.dirname(save_path), exist_ok=True)
        
        with open(save_path, 'w') as f:
            json.dump(results, f, indent=2)
        
        logger.info(f"Evaluation results saved to: {save_path}")
    else:
        logger.warning("No evaluation results to save")


def main():
    """ä¸»å‡½æ•°"""
    args = parse_args()
    
    logger = get_logger('NanoDet')
    logger.info("Starting NanoDet Jittor model evaluation using PyTorch tools")
    logger.info(f"Checkpoint: {args.model}")
    logger.info(f"Config: {args.config}")
    logger.info(f"Task: {args.task}")
    
    # è°ƒç”¨è¯„ä¼°
    results = call_pytorch_evaluation(
        args.model,
        args.config,
        args.task
    )
    
    if results:
        logger.info("Evaluation completed successfully")
        
        # ä¿å­˜ç»“æœ
        if args.save_result:
            save_results(results, args.save_result)
        
        # æ˜¾ç¤ºæœ€ç»ˆç»“æœ
        print("\n" + "="*60)
        print("ğŸ“Š Final Evaluation Results:")
        print("="*60)
        
        for metric, value in results.items():
            print(f"  {metric}: {value:.4f}")
        
        # ä¸PyTorchåŸºå‡†å¯¹æ¯”
        pytorch_map = 0.275  # PyTorchåŸºå‡†mAP
        if 'mAP' in results:
            jittor_map = results['mAP']
            relative_performance = jittor_map / pytorch_map * 100
            
            print(f"\nComparison with PyTorch baseline:")
            print(f"  PyTorch mAP: {pytorch_map:.4f}")
            print(f"  Jittor mAP:  {jittor_map:.4f}")
            print(f"  Relative:    {relative_performance:.1f}%")
            
            if relative_performance >= 95:
                print(f"  ğŸ¯ Excellent! Jittor achieves 95%+ of PyTorch performance")
            elif relative_performance >= 90:
                print(f"  âœ… Good! Jittor achieves 90%+ of PyTorch performance")
            elif relative_performance >= 80:
                print(f"  âš ï¸ Acceptable! Jittor achieves 80%+ of PyTorch performance")
            else:
                print(f"  âŒ Needs optimization! Jittor performance < 80% of PyTorch")
        
        print("="*60)
        
    else:
        logger.error("Evaluation failed")
        return 1
    
    return 0


if __name__ == '__main__':
    exit(main())
