#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
Jittorè®­ç»ƒ + PyTorchè¯„ä¼°å·¥å…·
ä½¿ç”¨Jittorè¿›è¡Œè®­ç»ƒï¼Œä½†ä½¿ç”¨PyTorchç‰ˆæœ¬çš„è¯„ä¼°å·¥å…·è¿›è¡ŒmAPè®¡ç®—
ç¡®ä¿è¯„ä¼°æ–¹æ³•å®Œå…¨ä¸€è‡´
"""

import os
import sys
import json
import subprocess
import time
import torch
import jittor as jt
import numpy as np
from pathlib import Path

# æ·»åŠ è·¯å¾„
sys.path.append('/home/kyc/project/nanodet/nanodet-jittor')
from nanodet.util import get_logger, load_config, save_checkpoint, load_pytorch_checkpoint
from nanodet.model.arch.nanodet_plus import NanoDetPlus


def create_jittor_model(config):
    """åˆ›å»ºJittoræ¨¡å‹"""
    logger = get_logger('JittorTraining')
    logger.info("åˆ›å»ºJittoræ¨¡å‹...")
    
    model = NanoDetPlus(
        config.model.arch.backbone,
        config.model.arch.fpn,
        config.model.arch.aux_head,
        config.model.arch.head
    )
    
    logger.info("æ¨¡å‹åˆ›å»ºæˆåŠŸ")
    return model


def convert_jittor_to_pytorch_checkpoint(jittor_model, save_path):
    """å°†Jittoræ¨¡å‹è½¬æ¢ä¸ºPyTorchæ ¼å¼çš„æ£€æŸ¥ç‚¹"""
    logger = get_logger('JittorTraining')
    logger.info(f"è½¬æ¢Jittoræ¨¡å‹ä¸ºPyTorchæ ¼å¼: {save_path}")
    
    # è·å–Jittoræ¨¡å‹å‚æ•°
    jittor_state_dict = {}
    for name, param in jittor_model.named_parameters():
        jittor_state_dict[name] = param.numpy()
    
    # è½¬æ¢ä¸ºPyTorchæ ¼å¼
    pytorch_state_dict = {}
    for name, param_np in jittor_state_dict.items():
        pytorch_name = f"model.{name}"  # æ·»åŠ 'model.'å‰ç¼€
        pytorch_state_dict[pytorch_name] = torch.tensor(param_np)
    
    # ä¿å­˜ä¸ºPyTorchæ£€æŸ¥ç‚¹æ ¼å¼
    checkpoint = {
        'state_dict': pytorch_state_dict,
        'epoch': 0,
        'optimizer': None,
        'lr_scheduler': None,
        'best_map': 0.0
    }
    
    os.makedirs(os.path.dirname(save_path), exist_ok=True)
    torch.save(checkpoint, save_path)
    
    logger.info(f"PyTorchæ ¼å¼æ£€æŸ¥ç‚¹å·²ä¿å­˜: {save_path}")
    return save_path


def call_pytorch_evaluation(pytorch_checkpoint_path, config_path):
    """è°ƒç”¨PyTorchç‰ˆæœ¬çš„è¯„ä¼°å·¥å…·"""
    logger = get_logger('JittorTraining')
    logger.info("è°ƒç”¨PyTorchç‰ˆæœ¬çš„è¯„ä¼°å·¥å…·...")
    
    # æ„å»ºè¯„ä¼°å‘½ä»¤
    pytorch_root = "/home/kyc/project/nanodet/nanodet-pytorch"
    pytorch_config = f"{pytorch_root}/config/nanodet-plus-m_320_voc_bs64_50epochs.yml"
    
    cmd = [
        "/home/kyc/miniconda3/envs/nano/bin/python",
        f"{pytorch_root}/tools/test.py",
        "--config", pytorch_config,
        "--model", pytorch_checkpoint_path,
        "--task", "val"
    ]
    
    logger.info(f"è¯„ä¼°å‘½ä»¤: {' '.join(cmd)}")
    
    try:
        # è¿è¡Œè¯„ä¼°
        result = subprocess.run(
            cmd,
            capture_output=True,
            text=True,
            timeout=600,  # 10åˆ†é’Ÿè¶…æ—¶
            cwd=pytorch_root
        )
        
        if result.returncode == 0:
            logger.info("PyTorchè¯„ä¼°æˆåŠŸå®Œæˆ")
            
            # è§£æmAPç»“æœ
            output_lines = result.stdout.split('\n')
            map_results = {}
            
            for line in output_lines:
                if 'Average Precision' in line and 'IoU=0.50:0.95' in line:
                    # æå–mAP@0.5:0.95
                    parts = line.split('=')
                    if len(parts) >= 2:
                        try:
                            map_value = float(parts[-1].strip())
                            map_results['mAP'] = map_value
                        except:
                            pass
                elif 'Average Precision' in line and 'IoU=0.50' in line:
                    # æå–mAP@0.5
                    parts = line.split('=')
                    if len(parts) >= 2:
                        try:
                            map_value = float(parts[-1].strip())
                            map_results['mAP_50'] = map_value
                        except:
                            pass
            
            logger.info(f"è¯„ä¼°ç»“æœ: {map_results}")
            return map_results
            
        else:
            logger.error(f"PyTorchè¯„ä¼°å¤±è´¥: {result.stderr}")
            return None
            
    except subprocess.TimeoutExpired:
        logger.error("PyTorchè¯„ä¼°è¶…æ—¶")
        return None
    except Exception as e:
        logger.error(f"PyTorchè¯„ä¼°å¼‚å¸¸: {e}")
        return None


def train_jittor_model(config_path, num_epochs=10):
    """è®­ç»ƒJittoræ¨¡å‹"""
    logger = get_logger('JittorTraining')
    logger.info(f"å¼€å§‹Jittorè®­ç»ƒ: {num_epochs}è½®")
    
    # åŠ è½½é…ç½®
    config = load_config(config_path)
    
    # åˆ›å»ºæ¨¡å‹
    model = create_jittor_model(config)
    
    # è®¾ç½®Jittor
    jt.flags.use_cuda = 1 if jt.has_cuda else 0
    logger.info(f"ä½¿ç”¨è®¾å¤‡: {'CUDA' if jt.flags.use_cuda else 'CPU'}")
    
    # åˆ›å»ºä¼˜åŒ–å™¨
    optimizer = jt.optim.AdamW(
        model.parameters(),
        lr=config.schedule.optimizer.lr,
        weight_decay=config.schedule.optimizer.weight_decay
    )
    
    # åˆ›å»ºå·¥ä½œç›®å½•
    workspace = config.save_dir
    os.makedirs(workspace, exist_ok=True)
    
    # è®­ç»ƒå¾ªç¯
    model.train()
    
    for epoch in range(1, num_epochs + 1):
        logger.info(f"å¼€å§‹ç¬¬ {epoch}/{num_epochs} è½®è®­ç»ƒ")
        
        # æ¨¡æ‹Ÿè®­ç»ƒè¿‡ç¨‹ï¼ˆè¿™é‡Œéœ€è¦å®ç°çœŸå®çš„æ•°æ®åŠ è½½å’Œè®­ç»ƒå¾ªç¯ï¼‰
        # ä¸ºäº†æ¼”ç¤ºï¼Œæˆ‘ä»¬å…ˆåˆ›å»ºä¸€ä¸ªç®€åŒ–çš„è®­ç»ƒå¾ªç¯
        
        epoch_loss = 0.0
        num_batches = 108  # ä¸PyTorchç‰ˆæœ¬ä¸€è‡´
        
        for batch_idx in range(num_batches):
            # åˆ›å»ºæ¨¡æ‹Ÿæ•°æ®
            batch_size = 64
            input_data = np.random.randn(batch_size, 3, 320, 320).astype(np.float32)
            jittor_input = jt.array(input_data)
            
            # å‰å‘ä¼ æ’­
            output = model(jittor_input)
            
            # è®¡ç®—ç®€åŒ–æŸå¤±
            loss = jt.mean(output ** 2) * 0.001  # ç®€åŒ–çš„æŸå¤±å‡½æ•°
            
            # åå‘ä¼ æ’­
            optimizer.zero_grad()
            optimizer.backward(loss)
            optimizer.step()
            
            epoch_loss += float(loss.numpy())
            
            if (batch_idx + 1) % 20 == 0:
                logger.info(f"  Epoch {epoch}, Batch {batch_idx+1}/{num_batches}, Loss: {float(loss.numpy()):.6f}")
        
        avg_loss = epoch_loss / num_batches
        logger.info(f"ç¬¬ {epoch} è½®å®Œæˆï¼Œå¹³å‡æŸå¤±: {avg_loss:.6f}")
        
        # æ¯5è½®è¿›è¡Œä¸€æ¬¡è¯„ä¼°
        if epoch % 5 == 0:
            logger.info(f"ç¬¬ {epoch} è½®è¯„ä¼°...")
            
            # ä¿å­˜Jittoræ£€æŸ¥ç‚¹
            jittor_checkpoint_path = f"{workspace}/jittor_epoch_{epoch}.pkl"
            save_checkpoint(
                model,
                optimizer=optimizer,
                epoch=epoch,
                metrics={'loss': avg_loss},
                save_path=jittor_checkpoint_path
            )
            
            # è½¬æ¢ä¸ºPyTorchæ ¼å¼
            pytorch_checkpoint_path = f"{workspace}/pytorch_format_epoch_{epoch}.ckpt"
            convert_jittor_to_pytorch_checkpoint(model, pytorch_checkpoint_path)
            
            # è°ƒç”¨PyTorchè¯„ä¼°
            map_results = call_pytorch_evaluation(pytorch_checkpoint_path, config_path)
            
            if map_results:
                logger.info(f"ç¬¬ {epoch} è½® mAP ç»“æœ:")
                for metric, value in map_results.items():
                    logger.info(f"  {metric}: {value:.4f}")
                
                # ä¿å­˜è¯„ä¼°ç»“æœ
                results_file = f"{workspace}/eval_results_epoch_{epoch}.json"
                with open(results_file, 'w') as f:
                    json.dump({
                        'epoch': epoch,
                        'loss': avg_loss,
                        'map_results': map_results
                    }, f, indent=2)
            else:
                logger.warning(f"ç¬¬ {epoch} è½®è¯„ä¼°å¤±è´¥")
    
    logger.info("Jittorè®­ç»ƒå®Œæˆ")
    return workspace


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ å¼€å§‹Jittorè®­ç»ƒ + PyTorchè¯„ä¼°")
    print("=" * 60)
    
    try:
        # é…ç½®æ–‡ä»¶è·¯å¾„
        config_path = "config/nanodet-plus-m_320_voc_jittor.yml"
        
        if not os.path.exists(config_path):
            print(f"âŒ é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_path}")
            return
        
        # å¼€å§‹è®­ç»ƒ
        workspace = train_jittor_model(config_path, num_epochs=10)
        
        print(f"\nğŸ¯ è®­ç»ƒå®Œæˆï¼")
        print(f"å·¥ä½œç›®å½•: {workspace}")
        print(f"æ£€æŸ¥ç‚¹å’Œè¯„ä¼°ç»“æœå·²ä¿å­˜")
        
    except Exception as e:
        print(f"âŒ è®­ç»ƒå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()


if __name__ == '__main__':
    main()
