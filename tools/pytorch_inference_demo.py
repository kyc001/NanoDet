#!/usr/bin/env python3
"""
ä½¿ç”¨PyTorch NanoDetè¿›è¡ŒæŽ¨ç†çš„ç®€å•è„šæœ¬
"""

import sys
import os
import cv2
import torch
import argparse

# æ·»åŠ PyTorch NanoDetè·¯å¾„
sys.path.insert(0, 'nanodet-pytorch')

from nanodet.util.config import load_config, cfg
from nanodet.util.logger import Logger
from nanodet.data.transform import Pipeline
from nanodet.model.arch import build_model
from nanodet.util import load_model_weight

class PyTorchPredictor:
    def __init__(self, config_path, model_path, device="cuda:0"):
        # åŠ è½½é…ç½®
        load_config(cfg, config_path)
        
        # åˆ›å»ºlogger
        logger = Logger(-1, use_tensorboard=False)
        
        # æž„å»ºæ¨¡åž‹
        model = build_model(cfg.model)
        
        # åŠ è½½æƒé‡
        ckpt = torch.load(model_path, map_location=lambda storage, loc: storage)
        load_model_weight(model, ckpt, logger)
        
        self.model = model.to(device).eval()
        self.pipeline = Pipeline(cfg.data.val.pipeline, cfg.data.val.keep_ratio)
        self.device = device
        self.cfg = cfg
        
        # VOCç±»åˆ«å
        self.class_names = [
            'aeroplane', 'bicycle', 'bird', 'boat', 'bottle',
            'bus', 'car', 'cat', 'chair', 'cow', 'diningtable',
            'dog', 'horse', 'motorbike', 'person', 'pottedplant',
            'sheep', 'sofa', 'train', 'tvmonitor'
        ]
    
    def inference(self, img_path):
        """æŽ¨ç†å•å¼ å›¾ç‰‡"""
        # è¯»å–å›¾ç‰‡
        img = cv2.imread(img_path)
        if img is None:
            print(f"âŒ æ— æ³•è¯»å–å›¾ç‰‡: {img_path}")
            return None, None
        
        height, width = img.shape[:2]
        img_info = {
            "file_name": os.path.basename(img_path),
            "height": height,
            "width": width
        }
        
        # é¢„å¤„ç†
        meta = dict(img_info=img_info, raw_img=img, img=img)
        meta = self.pipeline(meta, self.cfg.data.val.input_size)
        meta["img"] = (
            torch.from_numpy(meta["img"].transpose(2, 0, 1))
            .unsqueeze(0)
            .to(self.device)
        )
        
        # æŽ¨ç†
        with torch.no_grad():
            results = self.model.inference(meta)
        
        return meta, results
    
    def visualize_and_save(self, img_path, output_path, score_thres=0.35):
        """æŽ¨ç†å¹¶ä¿å­˜å¯è§†åŒ–ç»“æžœ"""
        meta, results = self.inference(img_path)
        
        if meta is None or results is None:
            return False
        
        # èŽ·å–æ£€æµ‹ç»“æžœ
        dets = results[0]
        
        # åœ¨åŽŸå›¾ä¸Šç»˜åˆ¶æ£€æµ‹ç»“æžœ
        img = meta["raw_img"].copy()
        
        detection_count = 0
        for det in dets:
            if len(det) >= 6:
                bbox = det[:4]
                score = det[4]
                class_id = int(det[5])
                
                if score > score_thres and class_id < len(self.class_names):
                    detection_count += 1
                    
                    # ç»˜åˆ¶è¾¹ç•Œæ¡†
                    x1, y1, x2, y2 = map(int, bbox)
                    cv2.rectangle(img, (x1, y1), (x2, y2), (0, 255, 0), 2)
                    
                    # ç»˜åˆ¶æ ‡ç­¾
                    label = f"{self.class_names[class_id]}: {score:.3f}"
                    label_size = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.6, 2)[0]
                    
                    cv2.rectangle(img, (x1, y1 - label_size[1] - 10), 
                                 (x1 + label_size[0], y1), (0, 255, 0), -1)
                    cv2.putText(img, label, (x1, y1 - 5), 
                               cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)
        
        # ä¿å­˜ç»“æžœ
        cv2.imwrite(output_path, img)
        print(f"âœ… PyTorchæŽ¨ç†å®Œæˆï¼Œæ£€æµ‹åˆ° {detection_count} ä¸ªç›®æ ‡ï¼Œç»“æžœä¿å­˜åˆ°: {output_path}")
        
        return True

def main():
    parser = argparse.ArgumentParser(description='PyTorch NanoDet Inference')
    parser.add_argument('--config', required=True, help='é…ç½®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--model', required=True, help='æ¨¡åž‹æƒé‡è·¯å¾„')
    parser.add_argument('--img', required=True, help='è¾“å…¥å›¾ç‰‡è·¯å¾„')
    parser.add_argument('--output', required=True, help='è¾“å‡ºå›¾ç‰‡è·¯å¾„')
    parser.add_argument('--device', default='cuda:0', help='è®¾å¤‡')
    parser.add_argument('--score_thres', type=float, default=0.35, help='ç½®ä¿¡åº¦é˜ˆå€¼')
    
    args = parser.parse_args()
    
    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not os.path.exists(args.config):
        print(f"âŒ é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {args.config}")
        return
    
    if not os.path.exists(args.model):
        print(f"âŒ æ¨¡åž‹æ–‡ä»¶ä¸å­˜åœ¨: {args.model}")
        return
    
    if not os.path.exists(args.img):
        print(f"âŒ è¾“å…¥å›¾ç‰‡ä¸å­˜åœ¨: {args.img}")
        return
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(os.path.dirname(args.output), exist_ok=True)
    
    try:
        # åˆ›å»ºé¢„æµ‹å™¨
        predictor = PyTorchPredictor(args.config, args.model, args.device)
        
        # æŽ¨ç†å¹¶ä¿å­˜ç»“æžœ
        success = predictor.visualize_and_save(args.img, args.output, args.score_thres)
        
        if success:
            print("ðŸŽ‰ PyTorchæŽ¨ç†æˆåŠŸå®Œæˆï¼")
        else:
            print("âŒ PyTorchæŽ¨ç†å¤±è´¥")
            
    except Exception as e:
        print(f"âŒ PyTorchæŽ¨ç†å¼‚å¸¸: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
